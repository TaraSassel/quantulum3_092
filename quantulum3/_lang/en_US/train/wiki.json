[
    {
        "_id": "Second",
        "clean": "Second",
        "text": "The second (symbol: s) is a unit of time, historically defined as 1\u204486400 of a day \u2013 this factor derived from the division of the day first into 24 hours, then to 60 minutes and finally to 60 seconds each (24 \u00d7 60 \u00d7 60 = 86400). \n\nThe current and formal definition in the International System of Units (SI) is more precise:The second [...] is defined by taking the fixed numerical value of the caesium frequency, \u0394\u03bdCs, the unperturbed ground-state hyperfine transition frequency of the caesium 133 atom, to be 9192631770 when expressed in the unit Hz, which is equal to s\u22121.\nThis current definition was adopted in 1967 when it became feasible to define the second based on fundamental properties of nature with caesium clocks. Because the speed of Earth's rotation varies and is slowing ever so slightly, a leap second is added at irregular intervals to civil time to keep clocks in sync with Earth's rotation.\n\n\n== Etymology ==\n\"Minute\" comes from the Latin pars minuta prima, meaning \"first small part\" i.e. first division of the hour - dividing into sixty, and \"second\" comes from the pars minuta secunda, \"second small part\", dividing again into sixty.\n\n\n== Uses ==\nAnalog clocks and watches often have sixty tick marks on their faces, representing seconds (and minutes), and a \"second hand\" to mark the passage of time in seconds.  Digital clocks and watches often have a two-digit seconds counter.\nSI prefixes are frequently combined with the word second to denote subdivisions of the second: milliseconds (thousandths), microseconds (millionths), nanoseconds (billionths), and sometimes smaller units of a second. Multiples of seconds are usually counted in hours and minutes. Though SI prefixes may also be used to form multiples of the second such as kiloseconds (thousands of seconds), such units are rarely used in practice. An everyday experience with small fractions of a second is a 1-gigahertz microprocessor that has a cycle time of 1 nanosecond. Camera shutter speeds are often expressed in fractions of a second, such as 1\u204430 second or 1\u20441000 second.\nSexagesimal divisions of the day from a calendar based on astronomical observation have existed since the third millennium BC, though they were not seconds as we know them today. Small divisions of time could not be measured back then, so such divisions were mathematically derived. The first timekeepers that could count seconds accurately were pendulum clocks invented in the 17th century. Starting in the 1950s, atomic clocks became better timekeepers than Earth's rotation, and they continue to set the standard today.\n\n\n== Clocks and solar time ==\nA mechanical clock, which does not depend on measuring the relative rotational position of the Earth, keeps uniform time called mean time, within whatever accuracy is intrinsic to it. That means that every second, minute and every other division of time counted by the clock has the same duration as any other identical division of time. But a sundial, which measures the relative position of the Sun in the sky called apparent time, does not keep uniform time.  The time kept by a sundial varies by time of year, meaning that seconds, minutes and every other division of time is a different duration at different times of the year. The time of day measured with mean time versus apparent time may differ by as much as 15 minutes, but a single day differs from the next by only a small amount; 15 minutes is a cumulative difference over a part of the year. The effect is due chiefly to the obliqueness of Earth's axis with respect to its orbit around the Sun.\nThe difference between apparent solar time and mean time was recognized by astronomers since antiquity, but prior to the invention of accurate mechanical clocks in the mid-17th century, sundials were the only reliable timepieces, and apparent solar time was the only generally accepted standard.\n\n\n== Events and units of time in seconds ==\nFractions of a second are usually denoted in decimal notation, for example 2.01 seconds, or two and one hundredth seconds. Multiples of seconds are usually expressed as minutes and seconds, or hours, minutes and seconds of clock time, separated by colons, such as 11:23:24, or 45:23 (the latter notation can give rise to ambiguity, because the same notation is used to denote hours and minutes). It rarely makes sense to express longer periods of time like hours or days in seconds, because they are awkwardly large numbers. For the metric unit of second, there are decimal prefixes representing 10\u221230 to 1030 seconds.\nSome common units of time in seconds are: a minute is 60 seconds; an hour is 3,600 seconds; a day is 86,400 seconds; a week is 604,800 seconds; a year (other than leap years) is 31,536,000 seconds; and a (Gregorian) century averages 3,155,695,200 seconds; with all of the above excluding any possible leap seconds. In astronomy, a Julian year is precisely 31,557,600 seconds.\nSome common events in seconds are: a stone falls about 4.9 meters from rest in one second; a pendulum of length about one meter has a swing of one second, so pendulum clocks have pendulums about a meter long; the fastest human sprinters run 10 meters in a second; an ocean wave in deep water travels about 23 meters in one second; sound travels about 343 meters in one second in air; light takes 1.3 seconds to reach Earth from the surface of the Moon, a distance of 384,400 kilometers.\n\n\n== Other units incorporating seconds ==\nA second is directly part of other units, such as frequency measured in hertz (inverse seconds or s\u22121), speed in meters per second, and acceleration in meters per second squared.  The metric system unit becquerel, a measure of radioactive decay, is measured in inverse seconds and higher powers of second are involved in derivatives of acceleration such as jerk. Though many derivative units for everyday things are reported in terms of larger units of time, not seconds, they are ultimately defined in terms of the SI second; this includes time expressed in hours and minutes, velocity of a car in kilometers per hour or miles per hour, kilowatt hours of electricity usage, and speed of a turntable in rotations per minute.\nMoreover, most other SI base units are defined by their relationship to the second: the meter is defined by setting the speed of light (in vacuum) to be 299 792 458 m/s, exactly; definitions of the SI base units kilogram, ampere, kelvin, and candela also depend on the second. The only base unit whose definition does not depend on the second is the mole, and only two of the 22 named derived units, radian and steradian, do not depend on the second either.\n\n\n== Timekeeping standards ==\n\nA set of atomic clocks throughout the world keeps time by consensus: the clocks \"vote\" on the correct time, and all voting clocks are steered to agree with the consensus, which is called International Atomic Time (TAI). TAI \"ticks\" atomic seconds.:\u200a207\u2013218\u200a\nCivil time is defined to agree with the rotation of the Earth. The international standard for timekeeping is Coordinated Universal Time (UTC). This time scale \"ticks\" the same atomic seconds as TAI, but inserts or omits leap seconds as necessary to correct for variations in the rate of rotation of the Earth.:\u200a16\u201317,\u200a207\u200a\nA time scale in which the seconds are not exactly equal to atomic seconds is UT1, a form of universal time. UT1 is defined by the rotation of the Earth with respect to the Sun, and does not contain any leap seconds.:\u200a68,\u200a232\u200a UT1 always differs from UTC by less than a second.\n\n\n=== Optical lattice clock ===\n\nWhile they are not yet part of any timekeeping standard, optical lattice clocks with frequencies in the visible light spectrum now exist and are the most accurate timekeepers of all.  A strontium clock with frequency 430 THz, in the red range of visible light, during the 2010s held the accuracy record: it gains or loses less than a second in 15 billion years, which is longer than the estimated age of the universe. Such a clock can measure a change in its elevation of as little as 2 cm by the change in its rate due to gravitational time dilation.\n\n\n== History of definition ==\n\nThere have only ever been three definitions of the second: as a fraction of the day, as a fraction of an extrapolated year, and as the microwave frequency of a caesium atomic clock, which have each realized a sexagesimal division of the day from ancient astronomical calendars.\n\n\n=== Sexagesimal divisions of calendar time and day ===\nCivilizations in the classic period and earlier created divisions of the calendar as well as arcs using a sexagesimal system of counting, so at that time the second was a sexagesimal subdivision of the day (ancient second = \u2060day/60\u00d760\u2060), not of the hour like the modern second (= \u2060hour/60\u00d760\u2060).  Sundials and water clocks were among the earliest timekeeping devices, and units of time were measured in degrees of arc.  Conceptual units of time smaller than realisable on sundials were also used.\nThere are references to \"second\" as part of a lunar month in the writings of natural philosophers of the Middle Ages, which were mathematical subdivisions that could not be measured mechanically.\n\n\n=== Fraction of solar day ===\nThe earliest mechanical clocks, which appeared starting in the 14th century, had displays that divided the hour into halves, thirds, quarters and sometimes even 12 parts, but never by 60. In fact, the hour was not commonly divided in 60 minutes as it was not uniform in duration. It was not practical for timekeepers to consider minutes until the first mechanical clocks that displayed minutes appeared near the end of the 16th century. Mechanical clocks kept the mean time, as opposed to the apparent time displayed by sundials. \nBy that time, sexagesimal divisions of time were well established in Europe.\nThe earliest clocks to display seconds appeared during the last half of the 16th century. The second became accurately measurable with the development of mechanical clocks. The earliest spring-driven timepiece with a second hand that marked seconds is an unsigned clock depicting Orpheus in the Fremersdorf collection, dated between 1560 and 1570.:\u200a417\u2013418\u200a During the 3rd quarter of the 16th century, Taqi al-Din built a clock with marks every 1/5 minute.\nIn 1579, Jost B\u00fcrgi built a clock for William of Hesse that marked seconds.:\u200a105\u200a In 1581, Tycho Brahe redesigned clocks that had displayed only minutes at his observatory so they also displayed seconds, even though those seconds were not accurate. In 1587, Tycho complained that his four clocks disagreed by plus or minus four seconds.:\u200a104\u200a\nIn 1656, Dutch scientist Christiaan Huygens invented the first pendulum clock. It had a pendulum length of just under a meter, giving it a swing of one second, and an escapement that ticked every second. It was the first clock that could accurately keep time in seconds. By the 1730s, 80 years later, John Harrison's maritime chronometers could keep time accurate to within one second in 100 days.\nIn 1832, Gauss proposed using the second as the base unit of time in his millimeter\u2013milligram\u2013second system of units. The British Association for the Advancement of Science (BAAS) in 1862 stated that \"All men of science are agreed to use the second of mean solar time as the unit of time.\" BAAS formally proposed the CGS system in 1874, although this system was gradually replaced over the next 70 years by MKS units. Both the CGS and MKS systems used the same second as their base unit of time. MKS was adopted internationally during the 1940s, defining the second as 1\u204486,400 of a mean solar day.\n\n\n=== Fraction of an ephemeris year ===\n\nSometime in the late 1940s, quartz crystal oscillator clocks with an operating frequency of ~100 kHz advanced to keep time with accuracy better than 1 part in 108 over an operating period of a day. It became apparent that a consensus of such clocks kept better time than the rotation of the Earth. Metrologists also knew that Earth's orbit around the Sun (a year) was much more stable than Earth's rotation. This led to proposals as early as 1950 to define the second as a fraction of a year.\nThe Earth's motion was described in Newcomb's Tables of the Sun (1895), which provided a formula for estimating the motion of the Sun relative to the epoch 1900 based on astronomical observations made between 1750 and 1892. This resulted in adoption of an ephemeris time scale expressed in units of the sidereal year at that epoch by the IAU in 1952. This extrapolated timescale brings the observed positions of the celestial bodies into accord with Newtonian dynamical theories of their motion. In 1955, the tropical year, considered more fundamental than the sidereal year, was chosen by the IAU as the unit of time. The tropical year in the definition was not measured but calculated from a formula describing a mean tropical year that decreased linearly over time.\nIn 1956, the second was redefined in terms of a year relative to that epoch. The second was thus defined as \"the fraction 1\u204431,556,925.9747 of the tropical year for 1900 January 0 at 12 hours ephemeris time\". This definition was adopted as part of the International System of Units in 1960.\n\n\n=== Atomic definition ===\nEven the best mechanical, electric motorized and quartz crystal-based clocks develop discrepancies from environmental conditions; far better for timekeeping is the natural and exact \"vibration\" in an energized atom. The frequency of vibration (i.e., radiation) is very specific depending on the type of atom and how it is excited. Since 1967, the second has been defined as exactly \"the duration of 9,192,631,770 periods of the radiation corresponding to the transition between the two hyperfine levels of the ground state of the caesium-133 atom\". This length of a second was selected to correspond exactly to the length of the ephemeris second previously defined. Atomic clocks use such a frequency to measure seconds by counting cycles per second at that frequency. Radiation of this kind is one of the most stable and reproducible phenomena of nature. The current generation of atomic clocks is accurate to within one second in a few hundred million years. Since 1967, atomic clocks based on atoms other than caesium-133 have been developed with increased precision by a factor of 100. Therefore a new definition of the second is planned.\nAtomic clocks now set the length of a second and the time standard for the world.:\u200a231\u2013232\u200a\n\n\n=== Table ===\n\n\n== Future redefinition ==\nIn 2022, the best realisation of the second is done with caesium primary standard clocks such as IT-CsF2, NIST-F2, NPL-CsF2, PTB-CSF2, SU\u2013CsFO2 or SYRTE-FO2. These clocks work by laser-cooling a cloud of Cs atoms to a microkelvin in a magneto-optic trap. These cold atoms are then launched vertically by laser light. The atoms then undergo Ramsey excitation in a microwave cavity. The fraction of excited atoms is then detected by laser beams. These clocks have 5\u00d710\u221216 systematic uncertainty, which is equivalent to 50 picoseconds per day. A system of several fountains worldwide contribute to International Atomic Time. These caesium clocks also underpin optical frequency measurements.\nOptical clocks are based on forbidden optical transitions in ions or atoms. They have frequencies around 1015 Hz, with a natural linewidth \n  \n    \n      \n        \u0394\n        f\n      \n    \n    {\\displaystyle \\Delta f}\n  \n of typically 1 Hz, so the Q-factor is about 1015, or even higher. They have better stabilities than microwave clocks, which means that they can facilitate evaluation of lower uncertainties. They also have better time resolution, which means the clock \"ticks\" faster. Optical clocks use either a single ion, or an optical lattice with 104\u2013106 atoms.\n\n\n=== Rydberg constant ===\nA definition based on the Rydberg constant would involve fixing the value to a certain value: \n  \n    \n      \n        \n          R\n          \n            \u221e\n          \n        \n        =\n        \n          \n            \n              \n                m\n                \n                  e\n                \n              \n              \n                e\n                \n                  4\n                \n              \n            \n            \n              8\n              \n                \u03b5\n                \n                  0\n                \n                \n                  2\n                \n              \n              \n                h\n                \n                  3\n                \n              \n              c\n            \n          \n        \n        =\n        \n          \n            \n              \n                m\n                \n                  e\n                \n              \n              c\n              \n                \u03b1\n                \n                  2\n                \n              \n            \n            \n              2\n              h\n            \n          \n        \n      \n    \n    {\\displaystyle R_{\\infty }={\\frac {m_{\\text{e}}e^{4}}{8\\varepsilon _{0}^{2}h^{3}c}}={\\frac {m_{\\text{e}}c\\alpha ^{2}}{2h}}}\n  \n. The Rydberg constant describes the energy levels in a hydrogen atom with the nonrelativistic approximation \n  \n    \n      \n        \n          E\n          \n            n\n          \n        \n        \u2248\n        \u2212\n        \n          \n            \n              \n                R\n                \n                  \u221e\n                \n              \n              c\n              h\n            \n            \n              n\n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle E_{n}\\approx -{\\frac {R_{\\infty }ch}{n^{2}}}}\n  \n.\nThe only viable way to fix the Rydberg constant involves trapping and cooling hydrogen. This is difficult because it is very light and the atoms move very fast, causing Doppler shifts. The radiation needed to cool the hydrogen \u2013 121.5 nm \u2013 is also difficult. Another hurdle involves improving the uncertainty in QED calculations, specifically the Lamb shift in the 1s-2s transition of the hydrogen atom.\n\n\n=== Requirements ===\nA redefinition must include improved optical clock reliability. TAI must be contributed to by optical clocks before the BIPM affirms a redefinition. A consistent method of sending signals must be developed before the second is redefined, such as fiber-optics.\n\n\n== SI multiples ==\n\nSI prefixes are commonly used for times shorter than one second, but rarely for multiples of a second. Instead, certain non-SI units are permitted for use with SI: minutes, hours, days, and in astronomy Julian years.\n\n\n== See also ==\nCaesium standard\nOrders of magnitude (time)\nSeconds pendulum\nTime standard\n\n\n== Notes ==\n\n\n== References ==\n\n\n== External links ==\n\nNational Physical Laboratory: Trapped ion optical frequency standards \nHigh-accuracy strontium ion optical clock; National Physical Laboratory (2005)\nNational Research Council of Canada: Optical frequency standard based on a single trapped ion\nNIST: Definition of the second; notice the cesium atom must be in its ground state at 0 K\nOfficial BIPM definition of the second\nThe leap second: its history and possible future\nWhat is a Cesium atom clock?\nSLAC: Scales of Time \u2013 Our universe from 1018 to 10\u221218 seconds",
        "unit": "second",
        "url": "https://en.wikipedia.org/wiki/Second"
    },
    {
        "_id": "Pound_(mass)",
        "clean": "Pound (mass)",
        "text": "The pound or pound-mass is a unit of mass used in both the British imperial and United States customary systems of measurement. Various definitions have been used; the most common today is the international avoirdupois pound, which is legally defined as exactly 0.45359237 kilograms, and which is divided into 16 avoirdupois ounces. The international standard symbol for the avoirdupois pound is lb; an alternative symbol (when there might otherwise be a risk of confusion with the pound-force) is lbm (for most pound definitions), # (chiefly in the U.S.), and \u2114 or \u2033\u0336 (specifically for the apothecaries' pound).\nThe unit is descended from the Roman libra (hence the symbol lb, descended from the scribal abbreviation, \u2114).  The English word pound comes from the Roman libra pondo ('the weight measured in libra'), and is cognate with, among others, German Pfund, Dutch pond, and Swedish pund. These units are now designated as historical and are no longer in common usage, being replaced by the metric system.\nUsage of the unqualified term pound reflects the historical conflation of mass and weight. This accounts for the modern distinguishing terms pound-mass and pound-force.\n\n\n== Etymology ==\nThe word 'pound' and its cognates ultimately derive from a borrowing into Proto-Germanic of the Latin expression libra pondo ('the weight measured in libra'), in which the word pondo is the ablative singular of the Latin noun pondus ('weight').\n\n\n== Current use ==\nThe United States and the Commonwealth of Nations agreed upon common definitions for the pound and the yard. Since 1 July 1959, the international avoirdupois pound (symbol lb) has been defined as exactly 0.45359237 kg.\nIn the United Kingdom, the use of the international pound was implemented in the Weights and Measures Act 1963.\n\nThe yard or the metre shall be the unit of measurement of length and the pound or the kilogram shall be the unit of measurement of mass by reference to which any measurement involving a measurement of length or mass shall be made in the United Kingdom; and- (a) the yard shall be 0.9144 metre exactly; (b) the pound shall be 0.45359237 kilogram exactly.\nAn avoirdupois pound is equal to 16 avoirdupois ounces and to exactly 7,000 grains. The conversion factor between the kilogram and the international pound was therefore chosen to be divisible by 7 with a terminating decimal representation, and an (international) grain is thus equal to exactly 64.79891 milligrams.\nIn the United Kingdom, the process of metrication and European units of measurement directives were expected to eliminate the use of the pound and ounce, but in 2007 the European Commission abandoned the requirement for metric-only labelling on packaged goods there, and allowed for dual metric\u2013imperial marking to continue indefinitely.\nIn the United States, the Metric Conversion Act of 1975 declared the metric system to be the \"preferred system of weights and measures\" but did not suspend use of United States customary units, and the United States is the only industrialised country where commercial activities do not predominantly use the metric system, despite many efforts to do so, and the pound remains widely used as one of the key customary units.\n\n\n== Historical use ==\n\nHistorically, in different parts of the world, at different points in time, and for different applications, the pound (or its translation) has referred to broadly similar but not identical standards of mass or force.\n\n\n=== Roman libra ===\n\nThe libra (Latin for 'scale'/'balance') is an ancient Roman unit of mass that is now equivalent to 328.9 g (11.60 oz). It was divided into 12 unciae (singular: uncia), or ounces. The libra is the origin of the abbreviation for pound, \"lb\".\n\n\n=== In Britain ===\nA number of different definitions of the pound have historically been used in Britain. Among these are the avoirdupois pound, which is the common pound used for weights, and the obsolete tower, merchants' and London pounds. The troy pound and ounce remain in use only for the weight of precious metals, especially in their trade. The weights of traded precious metals, such as gold and silver, are normally quoted just in ounces (e.g. \"500 ounces\") and, when the type of ounce is not explicitly stated, the troy system is assumed.\nThe pound sterling money system, which was introduced during the reign of King Offa of Mercia (757\u201396), was based originally on a Saxon pound of silver. After the Norman conquest the Saxon pound was known as the tower pound or moneyer's pound. In 1528, during the reign of Henry VIII, the coinage standard was changed by parliament from the tower pound to the troy pound.\n\n\n==== Avoirdupois pound ====\n\nThe avoirdupois pound, also known as the wool pound, first came into general use c. 1300. It was initially equal to 6,992 troy grains. The pound avoirdupois was divided into 16 ounces. During the reign of Queen Elizabeth I, the avoirdupois pound was redefined as 7,000 troy grains. Since then, the grain has often been an integral part of the avoirdupois system. By 1758, two Elizabethan Exchequer standard weights for the avoirdupois pound existed, and when measured in troy grains they were found to be of 7,002 grains and 6,999 grains.\n\n\n===== Imperial Standard Pound =====\nIn the United Kingdom, weights and measures have been defined by a long series of Acts of Parliament, the intention of which has been to regulate the sale of commodities. Materials traded in the marketplace are quantified according to accepted units and standards in order to avoid fraud. The standards themselves are legally defined so as to facilitate the resolution of disputes brought to the courts; only legally defined measures will be recognised by the courts. Quantifying devices used by traders (weights, weighing machines, containers of volumes, measures of length) are subject to official inspection, and penalties apply if they are fraudulent.\nThe Weights and Measures Act 1878 (41 & 42 Vict. c. 49) marked a major overhaul of the British system of weights and measures, and the definition of the pound given there remained in force until the 1960s. The pound was defined thus (Section 4) \"The ... platinum weight ... deposited in the Standards department of the Board of Trade ... shall continue to be the imperial standard of ... weight ... and the said platinum weight shall continue to be the Imperial Standard for determining the Imperial Standard Pound for the United Kingdom\". Paragraph 13 states that the weight in vacuo of this standard shall be called the Imperial Standard Pound, and that all other weights mentioned in the act and permissible for commerce shall be ascertained from it alone. The first schedule of the act gave more details of the standard pound: it is a platinum cylinder nearly 1.35 inches (34 mm) high, and 1.15 inches (29 mm) diameter, and the edges are carefully rounded off. It has a groove about 0.34 inches (8.6 mm) from the top, to allow the cylinder to be lifted using an ivory fork. It was constructed following the destruction of the Houses of Parliament by fire in 1834, and is stamped \"P.S. 1844, 1 lb\" (P.S. stands for \"Parliamentary Standard\").\n\n\n===== Redefinition in terms of the kilogram =====\nThe British Weights and Measures Act 1878 (41 & 42 Vict. c. 49) said that contracts worded in terms of metric units would be deemed by the courts to be made according to the Imperial units defined in the Act, and a table of metric equivalents was supplied so that the Imperial equivalents could be legally calculated. This defined, in UK law, metric units in terms of Imperial ones. The equivalence for the pound was given as 1 lb = 453.59265 g or 0.45359 kg, which made the kilogram equivalent to about 2.2046213 lb. \nIn 1883, it was determined jointly by the standards department of the British Board of Trade and the Bureau International that 0.4535924277 kg was a better approximation, and this figure, rounded to 0.45359243 kg was given legal status by an Order in Council in May 1898.\nIn 1959, based on further measurements and international coordination, the International Yard and Pound Agreement defined an \"international pound\" as being equivalent to exactly 0.45359237 kg. This meant that the existing legal definition of the UK pound differed from the international standard pound by 0.06 milligrams. To remedy this, the pound was again redefined in the United Kingdom by the Weights and Measures Act 1963 to match the international pound, stating: \"the pound shall be 0.453 592 37 kilogramme exactly\", a definition which remains valid to the present day.\nThe 2019 revision of the SI means that the pound is now defined precisely in terms of fundamental constants, ending the era of its definition in terms of physical prototypes.\n\n\n==== Troy pound ====\n\nA troy pound (abbreviated lb t) is equal to 12 troy ounces and to 5,760 grains, that is exactly 373.2417216 grams. Troy weights were used in England by jewellers. Apothecaries also used the troy pound and ounce, but added the drachms and scruples unit in the apothecaries' system of weights.\nTroy weight may take its name from the French market town of Troyes in France where English merchants traded at least as early as the early 9th century. The troy pound is no longer in general use or a legal unit for trade (it was abolished in the United Kingdom on 6 January 1879 by the Weights and Measures Act 1878), but the troy ounce, 1\u204412 of a troy pound, is still used for measurements of gems such as opals, and precious metals such as silver, platinum and particularly gold.\n\n\n==== Tower pound ====\n\nA tower pound is equal to 12 tower ounces and to 5,400 troy grains, which equals around 350 grams. The tower pound is the historical weight standard that was used for England's coinage. Before the Norman conquest in 1066, the tower pound was known as the Saxon pound. During the reign of King Offa (757\u201396) of Mercia, a Saxon pound of silver was used to set the original weight of a pound sterling. From one Saxon pound of silver (that is a tower pound) the king had 240 silver pennies minted. In the pound sterling monetary system, twelve pennies equaled a shilling and twenty shillings equaled a pound sterling.\nThe tower pound was referenced to a standard prototype found in the Tower of London. The tower system ran concurrently with the avoirdupois and troy systems until the reign of Henry VIII, when a royal proclamation dated 1526 required that the troy pound be used for mint purposes instead of the tower pound. No standards of the tower pound are known to have survived.\nThe tower pound was also called the moneyers' pound (referring to the Saxon moneyers before the Norman conquest); the easterling pound, which may refer to traders of eastern Germany, or to traders on the shore of the eastern Baltic sea, or dealers of Asiatic goods who settled at the London Steelyard wharf; and the Rochelle pound by French writers, because it was also in use at La Rochelle. An almost identical weight was employed by the Germans for weighing gold and silver.\nThe mercantile pound (1304) of 6750 troy grains, or 9600 Tower grains, derives from this pound, as 25 shilling-weights or 15 Tower ounces, for general commercial use. Multiple pounds based on the same ounce were quite common. In much of Europe, the apothecaries' and commercial pounds were different numbers of the same ounce.\n\n\n==== Merchants' pound ====\nThe merchants' pound (mercantile pound, libra mercantoria, or commercial pound) was considered to be composed of 25 rather than 20 Tower shillings of 12 pence. It was equal to 9,600 wheat grains (15 tower ounces or 6,750 grains) and was used in England until the 14th century for goods other than money and medicine (\"electuaries\").\n\n\n==== London pound ====\n\nThe London pound is that of the Hansa, as used in their various trading places.  The London pound is based on 16 ounces, each ounce divided as the tower ounce.  It never became a legal standard in England; the use of this pound waxed and waned with the influence of the Hansa itself.\nA London pound was equal to 7,200 troy grains (16 troy ounces) or, equivalently, 10,240 tower grains (16 tower ounces).\n\n\n=== In the United States ===\nIn the United States, the avoirdupois pound as a unit of mass has been officially defined in terms of the kilogram since the Mendenhall Order of 1893. That order defined the pound to be 2.20462 pounds to a kilogram. The following year, this relationship was refined as 2.20462234 pounds to a kilogram, following a determination of the British pound.\nIn 1959, the United States National Bureau of Standards redefined the pound (avoirdupois) to be exactly equal to 0.453 592 37 kilograms, as had been declared by the International Yard and Pound Agreement of that year. According to a 1959 NIST publication, the United States 1894 pound differed from the international pound by approximately one part in 10 million. The difference is so insignificant that it can be ignored for almost all practical purposes.\n\n\n=== Byzantine litra ===\n\nThe Byzantines used a series of measurements known as pounds (Latin: libra, Ancient Greek: \u03bb\u03af\u03c4\u03c1\u03b1, romanized: litra). The most common was the logarik\u0113 litra (\u03bb\u03bf\u03b3\u03b1\u03c1\u03b9\u03ba\u03ae \u03bb\u03af\u03c4\u03c1\u03b1, \"pound of account\"), established by Constantine the Great in 309/310. It formed the basis of the Byzantine monetary system, with one litra of gold equivalent to 72 solidi. A hundred litrai were known as a kent\u0113narion (\u03ba\u03b5\u03bd\u03c4\u03b7\u03bd\u03ac\u03c1\u03b9\u03bf\u03bd, \"hundredweight\"). Its weight seems to have decreased gradually from the original 324 g (11.4 oz) to 319 g (11.3 oz). Due to its association with gold, it was also known as the chrysaphik\u0113 litra (\u03c7\u03c1\u03c5\u03c3\u03b1\u03c6\u03b9\u03ba\u03ae \u03bb\u03af\u03c4\u03c1\u03b1, \"gold pound\") or thalassia litra (\u03b8\u03b1\u03bb\u03ac\u03c3\u03c3\u03b9\u03b1 \u03bb\u03af\u03c4\u03c1\u03b1, \"maritime pound\"), but it could also be used as a measure of land, equalling a fortieth of the thalassios modios.\nThe soualia litra was specifically used for weighing olive oil or wood, and corresponded to 4/5 of the logarik\u0113 or 256 g (9.0 oz). Some outlying regions, especially in later times, adopted various local measures, based on Italian, Arab or Turkish measures. The most important of these was the argyrik\u0113 litra (\u03b1\u03c1\u03b3\u03c5\u03c1\u03b9\u03ba\u03ae \u03bb\u03af\u03c4\u03c1\u03b1, \"silver pound\") of 333 g (11.7 oz), found in Trebizond and Cyprus, and probably of Arab origin.\n\n\n=== French livre ===\n\nSince the Middle Ages, various pounds (livre) have been used in France. Since the 19th century, a livre has referred to the metric pound, 500 g.\nThe livre esterlin is equivalent to about 367.1 grams (5,665 gr) and was used between the late 9th century and the mid-14th century.\nThe livre poids de marc or livre de Paris is equivalent to about 489.5 grams (7,554 gr) and was used between the 1350s and the late 18th century. It was introduced by the government of John II.\nThe livre m\u00e9trique was set equal to the kilogram by the decree of 13 Brumaire an IX between 1800 and 1812. This was a form of official metric pound.\nThe livre usuelle (customary unit) was defined as 500 g (18 oz) by the decree of 28 March 1812. It was abolished as a unit of mass effective 1 January 1840 by a decree of 4 July 1837, but is still used informally.\n\n\n=== German and Austrian Pfund ===\nOriginally derived from the Roman libra, the definition varied throughout the Holy Roman Empire in the Middle Ages and onward. For example, the measures and weights of the Habsburg monarchy were reformed in 1761 by Empress Maria Theresa of Austria. The unusually heavy Habsburg (civil) pound of 16 ounces was later defined in terms of 560.012 g (19.7538 oz). Bavarian reforms in 1809 and 1811 adopted essentially the same standard as the Austrian pound. In Prussia, a reform in 1816 defined a uniform civil pound in terms of the Prussian foot and distilled water, resulting in a Prussian pound of 467.711 g (16.4980 oz).\nBetween 1803 and 1815, all German regions west of the River Rhine were under French control, organised in the departements: Roer, Sarre, Rhin-et-Moselle, and Mont-Tonnerre. As a result of the Congress of Vienna, these regions again became part of various German states. However, many of these regions retained the metric system and adopted a metric pound of precisely 500 g (17.64 oz). In 1854, the pound of 500 g also became the official mass standard of the German Customs Union and was renamed the Zollpfund, but local pounds continued to co-exist with the Zollverein pound for some time in some German states. Nowadays, the term Pfund is sometimes still in use and universally refers to a pound of 500 g.\n\n\n=== Russian funt ===\nThe Russian pound (\u0424\u0443\u043d\u0442, funt) is an obsolete Russian unit of measurement of mass. It is equal to 409.51718 g (14.445293 oz). In 1899, the funt was the basic unit of weight, and all other units of weight were formed from it; in particular, a zolotnik was 1\u204496 of a funt, and a pood was 40 f\u00fanty.\n\n\n=== Sk\u00e5lpund ===\nThe Sk\u00e5lpund was a Scandinavian measurement that varied in weight between regions. From the 17th century onward, it was equal to 425.076 g (14.9941 oz) in Sweden but was abandoned in 1889 when Sweden switched to the metric system.\nIn Norway, the same name was used for a weight of 425.076 g (14.9941 oz). In Denmark, it equaled 471 g (16.6 oz).\nIn the 19th century, Denmark followed Germany's lead and redefined the pound as 500 g (18 oz).\n\n\n=== Portuguese libra and arr\u00e1tel ===\nThe Portuguese unit that corresponds to the pounds of different nations is the arr\u00e1tel, equivalent to 16 ounces of Colonha, a variant of the Cologne standard. This arr\u00e1tel was introduced in 1499 by Manuel I, king of Portugal. Based on an evaluation of bronze nesting weight piles distributed by Manuel I to different towns, the arr\u00e1tel of Manuel I has been estimated to be of 457.8 g (16.15 oz). In the early 19th century, the arr\u00e1tel was evaluated at 459 g (16.2 oz).\nIn the 15th century, the arr\u00e1tel was of 14 ounces of Colonha or 400.6 g (14.13 oz). The Portuguese libra was the same as 2 arr\u00e1teis. There were also arr\u00e1teis of 12.5 and 13 ounces and libras of 15 and 16 ounces. The Troyes or Tria standard was also used.\n\n\n=== Jersey pound ===\nA Jersey pound is an obsolete unit of mass used on the island of Jersey from the 14th century to the 19th century. It was equivalent to about 7,561 grains (490 g (17 oz)). It may have been derived from the French livre poids de marc.\n\n\n=== Trone pound ===\nThe trone pound is one of a number of obsolete Scottish units of measurement. It was equivalent to between 21 and 28 avoirdupois ounces (about 600\u2013800 g (21\u201328 oz)).\n\n\n=== Metric pound ===\nIn many countries, upon the introduction of a metric system, the pound (or its translation) became an historic and obsolete term, although some have kept it as an informal term without a specific value. In German, the term is Pfund, in French livre, in Dutch pond, in Spanish and Portuguese libra, in Italian libbra, and in Danish and Swedish pund.\nThough not from the same linguistic origin, the Chinese j\u012bn (\u65a4, also known as the \"catty\") in mainland China has a modern definition of exactly 500 g (18 oz), divided into 10 li\u01ceng (\u4e24). Traditionally around 600 g (21 oz), the jin has been in use for more than two thousand years varying in exact value from one period to another, serving the same purpose as \"pound\" for the common-use measure of weight. In Hong Kong, for the purposes of commerce and trade between Britain and Imperial China in the preceding centuries, three Chinese catties were equivalent to four British imperial pounds, defining one catty as 604.78982 g (21.333333 oz) in weight precisely. \nHundreds of older pounds were replaced in this way. Examples of the older pounds are one of around 459\u2013460 g (16.19\u201316.23 oz) in Spain, Portugal, and Latin America; one of 498.1 g (17.57 oz) in Norway; and several different ones in what is now Germany.\nFrom the introduction of the kilogram scales and measuring devices are denominated only in grams and kilograms. A pound of product must be determined by weighing the product in grams as the use of the pound is not sanctioned for trade within the European Union.\n\n\n== Use in weaponry ==\nSmoothbore cannon and carronades are designated by the weight in imperial pounds of round solid iron shot of diameter to fit the barrel. A cannon that fires a six-pound ball, for example, is called a six-pounder. Standard sizes are 6, 12, 18, 24, 32, and 42 pounds; 60-pounders and 68-pounders also exist,  along with other nonstandard weapons using the same scheme. See carronade.\nA similar definition, using lead balls, exists for determining the gauge of shotguns and shotgun shells.\n\n\n== See also ==\nPound-force\nSlug (unit)\n\n\n== Notes ==\n\n\n== References ==\n\n\n== External links ==\n\n\n=== Conversion between units ===\nU.S. National Institute of Standards and Technology Special Publication 811 Archived 14 October 2017 at the Wayback Machine\nNational Institute of Standards and Technology Handbook 130",
        "unit": "pound-mass",
        "url": "https://en.wikipedia.org/wiki/Pound_(mass)"
    },
    {
        "_id": "Foot_(unit)",
        "clean": "Foot (unit)",
        "text": "The foot (standard symbol: ft) is a unit of length in the British imperial and United States customary systems of measurement. The prime symbol, \u2032, is commonly used to represent the foot. In both customary and imperial units, one foot comprises 12 inches, and one yard comprises three feet. Since an international agreement in 1959, the foot is  defined as equal to exactly 0.3048 meters. \nHistorically, the \"foot\" was a part of many local systems of units, including the Greek, Roman, Chinese, French, and English systems. It varied in length from country to country, from city to city, and sometimes from trade to trade. Its length was usually between 250 mm and 335 mm and was generally, but not always, subdivided into 12 inches or 16 digits.\nThe United States is the only industrialized country that uses the (international) foot in preference to the meter in its commercial, engineering, and standards activities. The foot is legally recognized in the United Kingdom; road distance signs must use imperial units (however, distances on road signs are always marked in miles or yards, not feet; bridge clearances are given in meters as well as feet and inches), while its usage is widespread among the British public as a measurement of height. The foot is recognized as an alternative expression of length in Canada. Both the UK and Canada have partially metricated their units of measurement. The measurement of altitude in international aviation (the flight level unit) is one of the few areas where the foot is used outside the English-speaking world.\nThe most common plural of foot is feet. However, the singular form may be used like a plural when it is preceded by a number, as in \"he is six foot tall.\"\n\n\n== Historical origin ==\n\nHistorically, the human body has been used to provide the basis for units of length. The foot of an adult European-American male is typically about 15.3% of his height, giving a person of 175 cm (5 ft 9 in) a foot-length of about 268 mm (10.6 in), on average.\nArchaeologists believe that, in the past, the people of Egypt, India, and Mesopotamia preferred the cubit, while the people of Rome, Greece, and China preferred the foot. Under the Harappan linear measures, Indus cities during the Bronze Age used a foot of 13.2 inches (335 mm) and a cubit of 20.8 inches (528 mm). The Egyptian equivalent of the foot\u2014a measure of four palms or 16 digits\u2014was known as the djeser and has been reconstructed as about 30 cm (11.8 in).\nThe Greek foot (\u03c0\u03bf\u03cd\u03c2, pous) had a length of \u20601/600\u2060 of a stadion, one stadion being about 181.2 m (594 ft); therefore a foot was, at the time, about 302 mm (11.9 in). Its exact size varied from city to city and could range between 270 mm (10.6 in) and 350 mm (13.8 in), but lengths used for temple construction appear to have been about 295 mm (11.6 in) to 325 mm (12.8 in); the former was close to the size of the Roman foot.\nThe standard Roman foot  (pes) was normally about 295.7 mm (11.6 in) (97% of today's measurement), but in some provinces, particularly Germania Inferior, the so-called pes Drusianus (foot of Nero Claudius Drusus) was sometimes used, with a length of about 334 mm (13.1 in). (In reality, this foot predated Drusus.)\nOriginally both the Greeks and the Romans subdivided the foot into 16 digits, but in later years, the Romans also subdivided the foot into 12 unciae (from which both the English words \"inch\" and \"ounce\" are derived).\nAfter the fall of the Roman Empire, some Roman traditions were continued but others fell into disuse. In AD 790 Charlemagne attempted to reform the units of measure in his domains. His units of length were based on the toise and in particular the toise de l'\u00c9critoire, the distance between the fingertips of the outstretched arms of a man. The toise has 6 pieds (feet) each of 326.6 mm (12.9 in).\nHe was unsuccessful in introducing a standard unit of length throughout his realm: an analysis of the measurements of Charlieu Abbey shows that during the 9th century the Roman foot of 296.1 mm (11.66 in) was used; when it was rebuilt in the 10th century, a foot of about 320 mm (12.6 in) was used. At the same time, monastic buildings used the Carolingian foot of 340 mm (13.4 in).\nThe procedure for verification of the foot as described in the 16th century posthumously published work by Jacob K\u00f6bel in his book Geometrei. Von k\u00fcnstlichem Feldmessen und absehen is:\n\nStand at the door of a church on a Sunday and bid 16 men to stop, tall ones and small ones, as they happen to pass out when the service is finished; then make them put their left feet one behind the other, and the length thus obtained shall be a right and lawful rood to measure and survey the land with, and the 16th part of it shall be the right and lawful foot.\n\n\n=== England ===\n\nThe Neolithic long foot, first proposed by archeologists Mike Parker Pearson and Andrew Chamberlain, is based upon calculations from surveys of Phase 1 elements at Stonehenge. They found that the underlying diameters of the stone circles had been consistently laid out using  multiples of a base unit amounting to 30 long feet, which they calculated to be 1.056 of a modern international foot (thus 12.672 inches or 0.3219 m). Furthermore, this unit is identifiable in the dimensions of some stone lintels at the site and in the diameter of the \"southern circle\"  at nearby Durrington Walls. Evidence that this unit was in widespread use across southern Britain is available from the Folkton Drums from  Yorkshire (neolithic artifacts, made from chalk, with circumferences that exactly divide as integers into ten long feet) and a similar object, the Lavant drum, excavated at Lavant, Sussex, again with a circumference divisible as a whole number into ten long feet.\nThe measures of Iron Age Britain are uncertain and proposed reconstructions such as the Megalithic Yard are controversial. Later Welsh legend credited Dyfnwal Moelmud with the establishment of their units, including a foot of 9 inches. The Belgic or North German foot of 335 mm (13.2 in) was introduced to England either by the Belgic Celts during their invasions prior to the Romans or by the Anglo-Saxons in the 5th and 6th century.\nRoman units were introduced following their invasion in AD 43. Following the Roman withdrawal and Saxon invasions, the Roman foot continued to be used in the construction crafts while the Belgic foot was used for land measurement. Both the Welsh and Belgic feet seem to have been based on multiples of the barleycorn, but by as early as 950 the English kings seem to have (ineffectually) ordered measures to be based upon an iron yardstick at Winchester and then London. Henry I was said to have ordered a new standard to be based upon the length of his own arm and, by the c.\u20091300 Act concerning the Composition of Yards and Perches traditionally credited to Edward I or II, the statute foot was a different measure, exactly \u206010/11\u2060 of the old (Belgic) foot. The barleycorn, inch, ell, and yard were likewise shrunk, while rods and furlongs remained the same. The ambiguity over the state of the mile was resolved by the 1593 Act against Converting of Great Houses into Several Tenements and for Restraint of Inmates and Inclosures in and near about the City of London and Westminster, which codified the statute mile as comprising 5,280 feet. The differences among the various physical standard yards around the world, revealed by increasingly powerful microscopes, eventually led to the 1959 adoption of the international foot defined in terms of the meter.\n\n\n== Definition ==\n\n\n=== International foot ===\nThe international yard and pound agreement of July 1959 defined the length of the international yard in the United States and countries of the Commonwealth of Nations as exactly 0.9144 meters. Consequently, since a foot is one third of a yard, the international foot is defined to be equal to exactly 0.3048 meters. This was 2 ppm shorter than the previous US definition and 1.7 ppm longer than the previous British definition.\nThe 1959 agreement concluded a series of step-by-step events, set off in particular by the British Standards Institution's adoption of a scientific standard inch of 25.4 millimeters in 1930.\n\n\n==== Symbol ====\nThe IEEE standard symbol for a foot is \"ft\". In some cases, the foot is denoted by a prime, often approximated by an apostrophe, and the inch by a double prime; for example, 2 feet 4 inches is sometimes denoted 2\u2032 4\u2033.\n\n\n=== Imperial units ===\nIn Imperial units, the foot was defined as \u20601/3\u2060 yard, with the yard being realized as a physical standard (separate from the standard meter). The yard standards of the different Commonwealth countries were periodically compared with one another. The value of the United Kingdom primary standard of the yard was determined in terms of the meter by the National Physical Laboratory in 1964 to be 0.9143969 m, implying a pre-1959 UK foot of 0.3047990 m.\nThe UK adopted the international yard for all purposes through the Weights and Measures Act 1963, effective January 1, 1964.\n\n\n=== Survey foot ===\nWhen the international foot was defined in 1959, a great deal of survey data was already available based on the former definitions, especially in the United States and in India. The small difference between the survey foot and the international foot would not be detectable on a survey of a small parcel, but becomes significant for mapping, or when the state plane coordinate system (SPCS) is used in the US, because the origin of the system may be hundreds of thousands of feet (hundreds of miles) from the point of interest. Hence the previous definitions continued to be used for surveying in the United States and India for many years, and are denoted survey feet to distinguish them from the international foot. The United Kingdom was unaffected by this problem, as the retriangulation of Great Britain (1936\u201362) had been done in meters.\n\n\n==== US survey foot ====\nIn the United States, the foot was defined as 12 inches, with the inch being defined by the Mendenhall Order of 1893 via 39.37 inches = 1 m (making a US foot exactly \u20601200/3937\u2060 meters, approximately 0.30480061 m).\nOn December 31, 2022, the National Institute of Standards and Technology, the National Geodetic Survey, and the United States Department of Commerce deprecated use of the US survey foot and recommended conversion to either the meter or the international foot (0.3048 m). However, the historic relevance of the US survey foot persists, as the Federal Register notes:\n\nThe date of December 31, 2022, was selected to accompany the modernization of the National Spatial Reference System (NSRS) by NOAA's National Geodetic Survey (NGS). The reason for associating the deprecation of the U.S. survey foot with the modernization of the NSRS is that the biggest impact of the uniform adoption of the international foot will be for users of the NSRS, due to very large coordinate values currently given in U.S. survey feet in many areas of the U.S. Impacts related to the change to international feet will be minimized if a transition occurs concurrently with others [sic] changes in the NSRS. ...\n\nThe difference in timelines will have no effect on users of the existing NSRS (National Spatial Reference System), because NGS (NOAA's National Geodetic Survey) will continue to support the U.S. survey foot for components of the NSRS where it is used now and in the past [emphasis added]. In other words, to minimize disruption in the use of U.S. survey foot for existing NSRS coordinate systems, the change will apply only to the modernized NSRS.\nState legislation is also important for determining the conversion factor to be used for everyday land surveying and real estate transactions, although the difference (two parts per million) is of no practical significance given the precision of normal surveying measurements over short distances (usually much less than a mile). Out of 50 states and six other jurisdictions, 40 have legislated that surveying measures should be based on the US survey foot, six have legislated that they be made on the basis of the international foot, and ten have not specified.\n\n\n==== Indian survey foot ====\nThe Indian survey foot is defined as exactly 0.3047996 m, presumably derived from a measurement of the previous Indian standard of the yard. However, it is now obsolete as the current National Topographic Database of the Survey of India is based on the metric WGS-84 datum, which is also used by the Global Positioning System.\n\n\n== Historical use ==\n\n\n=== Metric foot ===\nAn ISO 2848 measure of 3 basic modules (30 cm) is called a \"metric foot\", but there were earlier distinct definitions of a metric foot during metrication in France and Germany.\n\n\n==== France ====\nIn 1799 the meter became the official unit of length in France. This was not fully enforced, and in 1812 Napoleon introduced the system of mesures usuelles which restored the traditional French measurements in the retail trade, but redefined them in terms of metric units. The foot, or pied m\u00e9trique, was defined as one third of a meter. This unit continued in use until 1837.\n\n\n==== Germany ====\nIn southwestern Germany in 1806, the Confederation of the Rhine was founded and three different reformed feet were defined, all of which were based on the metric system:\n\nIn Hesse, the Fu\u00df (foot) was redefined as 25 cm.\nIn Baden, the Fu\u00df was redefined as 30 cm.\nIn the Palatinate, the Fu\u00df was redefined as being \u206033+1/3\u2060 cm (as in France).\n\n\n=== Other obsolete feet ===\nPrior to the introduction of the metric system, many European cities and countries used the foot, but it varied considerably in length: the voet in Ypres, Belgium, was 273.8 millimeters (10.78 in) while the piede in Venice was 347.73 millimeters (13.690 in). Lists of conversion factors between the various units of measure were given in many European reference works including:\n\nTrait\u00e9, Paris \u2013 1769\nPalaiseau \u2013 Bordeaux: 1816 \nde Gelder, Amsterdam and The Hague \u2013 1824\nHorace, Brussels \u2013 1840\nNoback & Noback (2 volumes), Leipzig \u2013 1851\nBruhns, Leipzig \u2013 1881\nMany of these standards were peculiar to a particular city, especially in Germany (which, before German unification in 1871, consisted of many kingdoms, principalities, free cities and so on). In many cases the length of the unit was not uniquely fixed: for example, the English foot was stated as 11 pouces 2.6 lignes (French inches and lines) by Picard, 11 pouces 3.11 lignes by Maskelyne, and 11 pouces 3 lignes by D'Alembert.\nMost of the various feet in this list ceased to be used when the countries adopted the metric system. The Netherlands and modern Belgium adopted the metric system in 1817, having used the mesures usuelles under Napoleon and the newly formed German Empire adopted the metric system in 1871.\nThe palm (typically 200\u2013280 mm, ie. 7\u20607/8\u2060 to 11\u20601/32\u2060 inches) was used in many Mediterranean cities instead of the foot. Horace Doursther, whose reference was published in Belgium which had the smallest foot measurements, grouped both units together, while J. F. G. Palaiseau devoted three chapters to units of length: one for linear measures (palms and feet); one for cloth measures (ells); and one for distances traveled (miles and leagues).\n\n\n==== Obsolete feet details ====\n\nIn Belgium, the words pied (French) and voet (Dutch) would have been used interchangeably.\n\n\n==== Notes ====\n\n\n== Present day uses ==\n\n\n=== International ISO-standard and other intermodal shipping containers ===\nInternational Standards Organisation (ISO)-defined intermodal containers for efficient global freight/cargo shipping, were defined using feet rather than meters for their leading outside (corner) dimensions. All ISO-standard containers to this day are eight feet wide, and their outer heights and lengths are also primarily defined in, or derived from feet. \nQuantities of global shipping containers are still primarily counted in Twenty-foot Equivalent Units, or TEUs.\n\n\n=== Aviation ===\nEveryday global (civilian) air traffic / aviation continues to be controlled in flight levels (flying altitudes) separated by thousands of feet (although typically read out in hundreds \u2013 e.g. flight level 330 actually means 33,000 feet, or about 10 kilometres in altitude).\n\n\n=== Relation to shoe size ===\nThe length of the (international) foot corresponds to a human foot with shoe size of 13 (UK), 14 (US male), 15.5 (US female) or 48 (EU sizing).\n\n\n== Dimension ==\nIn measurement, the term \"linear foot\" (sometimes incorrectly referred to as \"lineal foot\") refers to the number of feet in a length of material (such as lumber or fabric) without regard to the width; it is used to distinguish from surface area in square foot.\n\n\n== See also ==\nAnthropic units\nHistory of measurement\nInternational System of Units\nKorean units of measurement\nMermin's foot\nPous\nSystems of measurement\n\n\n== Notes ==\n\n\n== References ==",
        "unit": "foot",
        "url": "https://en.wikipedia.org/wiki/Foot_(unit)"
    },
    {
        "_id": "Area",
        "clean": "Area",
        "text": "Area is the measure of a region's size on a surface. The area of a plane region or plane area refers to the area of a shape or planar lamina, while surface area refers to the area of an open surface or the boundary of a three-dimensional object. Area can be understood as the amount of material with a given thickness that would be necessary to fashion a model of the shape, or the amount of paint necessary to cover the surface with a single coat. It is the two-dimensional analogue of the length of a curve (a one-dimensional concept) or the volume of a solid (a three-dimensional concept).\nTwo different regions may have the same area (as in squaring the circle); by synecdoche, \"area\" sometimes is used to refer to the region, as in a \"polygonal area\".\nThe area of a shape can be measured by comparing the shape to squares of a fixed size. In the International System of Units (SI), the standard unit of area is the square metre (written as m2), which is the area of a square whose sides are one metre long.  A shape with an area of three square metres would have the same area as three such squares.  In mathematics, the unit square is defined to have area one, and the area of any other shape or surface is a dimensionless real number.\nThere are several well-known formulas for the areas of simple shapes such as triangles, rectangles, and circles.  Using these formulas, the area of any polygon can be found by dividing the polygon into triangles.  For shapes with curved boundary, calculus is usually required to compute the area.  Indeed, the problem of determining the area of plane figures was a major motivation for the historical development of calculus.\nFor a solid shape such as a sphere, cone, or cylinder, the area of its boundary surface is called the surface area. Formulas for the surface areas of simple shapes were computed by the ancient Greeks, but computing the surface area of a more complicated shape usually requires multivariable calculus.\nArea plays an important role in modern mathematics.  In addition to its obvious importance in geometry and calculus, area is related to the definition of determinants in linear algebra, and is a basic property of surfaces in differential geometry. In analysis, the area of a subset of the plane is defined using Lebesgue measure, though not every subset is measurable if one supposes the axiom of choice.  In general, area in higher mathematics is seen as a special case of volume for two-dimensional regions.\nArea can be defined through the use of axioms, defining it as a function of a collection of certain plane figures to the set of real numbers. It can be proved that such a function exists.\n\n\n== Formal definition ==\n\nAn approach to defining what is meant by \"area\" is through axioms. \"Area\" can be defined as a function from a collection M of a special kinds of plane figures (termed measurable sets) to the set of real numbers, which satisfies the following properties:\n\nFor all S in M, a(S) \u2265 0.\nIf S and T are in M then so are S \u222a T and S \u2229 T, and also a(S\u222aT) = a(S) + a(T) \u2212 a(S \u2229 T).\nIf S and T are in M with S \u2286 T then T \u2212 S is in M and a(T\u2212S) = a(T) \u2212 a(S).\nIf a set S is in M and S is congruent to T then T is also in M and a(S) = a(T).\nEvery rectangle R is in M. If the rectangle has length h and breadth k then a(R) = hk.\nLet Q be a set enclosed between two step regions S and T. A step region is formed from a finite union of adjacent rectangles resting on a common base, i.e. S \u2286 Q \u2286 T. If there is a unique number c such that a(S) \u2264 c \u2264 a(T) for all such step regions S and T, then a(Q) = c.\nIt can be proved that such an area function actually exists.\n\n\n== Units ==\n\nEvery unit of length has a corresponding unit of area, namely the area of a square with the given side length.  Thus areas can be measured in square metres (m2), square centimetres (cm2), square millimetres (mm2), square kilometres (km2), square feet (ft2), square yards (yd2), square miles (mi2), and so forth.  Algebraically, these units can be thought of as the squares of the corresponding length units.\nThe SI unit of area is the square metre, which is considered an SI derived unit.\n\n\n=== Conversions ===\n\nCalculation of the area of a square whose length and width are 1 metre would be:\n1 metre \u00d7 1 metre = 1 m2\nand so, a rectangle with different sides (say length of 3 metres and width of 2 metres) would have an area in square units that can be calculated as:\n3 metres \u00d7 2 metres = 6 m2. This is equivalent to 6 million square millimetres. Other useful conversions are:\n\n1 square kilometre = 1,000,000 square metres\n1 square metre = 10,000 square centimetres = 1,000,000 square millimetres\n1 square centimetre = 100 square millimetres.\n\n\n==== Non-metric units ====\nIn non-metric units, the conversion between two square units is the square of the conversion between the corresponding length units.\n\n1 foot = 12 inches,\nthe relationship between square feet and square inches is\n\n1 square foot = 144 square inches,\nwhere 144 = 122 = 12 \u00d7 12.  Similarly:\n\n1 square yard = 9 square feet\n1 square mile = 3,097,600 square yards = 27,878,400 square feet\nIn addition, conversion factors include:\n\n1 square inch = 6.4516 square centimetres\n1 square foot = 0.09290304 square metres\n1 square yard = 0.83612736 square metres\n1 square mile = 2.589988110336 square kilometres\n\n\n=== Other units including historical ===\n\nThere are several other common units for area.  The are was the original unit of area in the metric system, with:\n\n1 are = 100 square metres\nThough the are has fallen out of use, the hectare is still commonly used to measure land:\n\n1 hectare = 100 ares = 10,000 square metres = 0.01 square kilometres\nOther uncommon metric units of area include the tetrad, the hectad, and the myriad.\nThe acre is also commonly used to measure land areas, where\n\n1 acre = 4,840 square yards = 43,560 square feet.\nAn acre is approximately 40% of a hectare.\nOn the atomic scale, area is measured in units of barns, such that:\n\n1 barn = 10\u221228 square meters.\nThe barn is commonly used in describing the cross-sectional area of interaction in nuclear physics.\nIn South Asia (mainly Indians), although the countries use SI units as official, many South Asians still use traditional units. Each administrative division has its own area unit, some of them have same names, but with different values. There's no official consensus about the traditional units values. Thus, the conversions between the SI units and the traditional units may have different results, depending on what reference that has been used.\nSome traditional South Asian units that have fixed value:\n\n1 Killa = 1 acre\n1 Ghumaon = 1 acre\n1 Kanal = 0.125 acre (1 acre = 8 kanal)\n1 Decimal = 48.4 square yards\n1 Chatak = 180 square feet\n\n\n== History ==\n\n\n=== Circle area ===\n\nIn the 5th century BCE, Hippocrates of Chios was the first to show that the area of a disk (the region enclosed by a circle) is proportional to the square of its diameter, as part of his quadrature of the lune of Hippocrates, but did not identify the constant of proportionality. Eudoxus of Cnidus, also in the 5th century BCE, also found that the area of a disk is proportional to its radius squared.\nSubsequently, Book I of Euclid's Elements dealt with equality of areas between two-dimensional figures. The mathematician Archimedes used the tools of Euclidean geometry to show that the area inside a circle is equal to that of a right triangle whose base has the length of the circle's circumference and whose height equals the circle's radius, in his book Measurement of a Circle. (The circumference is 2\u03c0r, and the area of a triangle is half the base times the height, yielding the area \u03c0r2 for the disk.) Archimedes approximated the value of \u03c0 (and hence the area of a unit-radius circle) with his doubling method, in which he inscribed a regular triangle in a circle and noted its area, then doubled the number of sides to give a regular hexagon, then repeatedly doubled the number of sides as the polygon's area got closer and closer to that of the circle (and did the same with circumscribed polygons).\n\n\n=== Triangle area ===\n\n\n=== Quadrilateral area ===\nIn the 7th century CE, Brahmagupta developed a formula, now known as Brahmagupta's formula, for the area of a cyclic quadrilateral (a quadrilateral inscribed in a circle) in terms of its sides. In 1842, the German mathematicians Carl Anton Bretschneider and Karl Georg Christian von Staudt independently found a formula, known as Bretschneider's formula, for the area of any quadrilateral.\n\n\n=== General polygon area ===\nThe development of Cartesian coordinates by Ren\u00e9 Descartes in the 17th century allowed the development of the surveyor's formula for the area of any polygon with known vertex locations by Gauss in the 19th century.\n\n\n=== Areas determined using calculus ===\nThe development of integral calculus in the late 17th century provided tools that could subsequently be used for computing more complicated areas, such as the area of an ellipse and the surface areas of various curved three-dimensional objects.\n\n\n== Area formulas ==\n\n\n=== Polygon formulas ===\n\nFor a non-self-intersecting (simple) polygon, the Cartesian coordinates \n  \n    \n      \n        (\n        \n          x\n          \n            i\n          \n        \n        ,\n        \n          y\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle (x_{i},y_{i})}\n  \n (i=0, 1, ..., n-1) of whose n vertices are known, the area is given by the surveyor's formula:\n\n  \n    \n      \n        A\n        =\n        \n          \n            1\n            2\n          \n        \n        \n          \n            |\n          \n        \n        \n          \u2211\n          \n            i\n            =\n            0\n          \n          \n            n\n            \u2212\n            1\n          \n        \n        (\n        \n          x\n          \n            i\n          \n        \n        \n          y\n          \n            i\n            +\n            1\n          \n        \n        \u2212\n        \n          x\n          \n            i\n            +\n            1\n          \n        \n        \n          y\n          \n            i\n          \n        \n        )\n        \n          \n            |\n          \n        \n      \n    \n    {\\displaystyle A={\\frac {1}{2}}{\\Biggl \\vert }\\sum _{i=0}^{n-1}(x_{i}y_{i+1}-x_{i+1}y_{i}){\\Biggr \\vert }}\n  \n\nwhere when i=n-1, then i+1 is expressed as modulus n and so refers to 0.\n\n\n==== Rectangles ====\n\nThe most basic area formula is the formula for the area of a rectangle. Given a rectangle with length l and width w, the formula for the area is:\n\nA = lw  (rectangle).\nThat is, the area of the rectangle is the length multiplied by the width.  As a special case, as l = w in the case of a square, the area of a square with side length s is given by the formula:\n\nA = s2  (square).\nThe formula for the area of a rectangle follows directly from the basic properties of area, and is sometimes taken as a definition or axiom.  On the other hand, if geometry is developed before arithmetic, this formula can be used to define multiplication of real numbers.\n\n\n==== Dissection, parallelograms, and triangles ====\n\nMost other simple formulas for area follow from the method of dissection.\nThis involves cutting a shape into pieces, whose areas must sum to the area of the original shape.\nFor an example, any parallelogram can be subdivided into a trapezoid and a right triangle, as shown in figure to the left.  If the triangle is moved to the other side of the trapezoid, then the resulting figure is a rectangle.  It follows that the area of the parallelogram is the same as the area of the rectangle:\n\nA = bh  (parallelogram).\n\nHowever, the same parallelogram can also be cut along a diagonal into two congruent triangles, as shown in the figure to the right.  It follows that the area of each triangle is half the area of the parallelogram:\n\n  \n    \n      \n        A\n        =\n        \n          \n            1\n            2\n          \n        \n        b\n        h\n      \n    \n    {\\displaystyle A={\\frac {1}{2}}bh}\n  \n  (triangle).\nSimilar arguments can be used to find area formulas for the trapezoid as well as more complicated polygons.\n\n\n=== Area of curved shapes ===\n\n\n==== Circles ====\n\nThe formula for the area of a circle (more properly called the area enclosed by a circle or the area of a disk) is based on a similar method.  Given a circle of radius r, it is possible to partition the circle into sectors, as shown in the figure to the right.  Each sector is approximately triangular in shape, and the sectors can be rearranged to form an approximate parallelogram.  The height of this parallelogram is r, and the width is half the circumference of the circle, or \u03c0r.  Thus, the total area of the circle is \u03c0r2:\n\nA = \u03c0r2  (circle).\nThough the dissection used in this formula is only approximate, the error becomes smaller and smaller as the circle is partitioned into more and more sectors.  The limit of the areas of the approximate parallelograms is exactly \u03c0r2, which is the area of the circle.\nThis argument is actually a simple application of the ideas of calculus.  In ancient times, the method of exhaustion was used in a similar way to find the area of the circle, and this method is now recognized as a precursor to integral calculus.  Using modern methods, the area of a circle can be computed using a definite integral:\n\n  \n    \n      \n        A\n        \n        =\n        \n        2\n        \n          \u222b\n          \n            \u2212\n            r\n          \n          \n            r\n          \n        \n        \n          \n            \n              r\n              \n                2\n              \n            \n            \u2212\n            \n              x\n              \n                2\n              \n            \n          \n        \n        \n        d\n        x\n        \n        =\n        \n        \u03c0\n        \n          r\n          \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle A\\;=\\;2\\int _{-r}^{r}{\\sqrt {r^{2}-x^{2}}}\\,dx\\;=\\;\\pi r^{2}.}\n  \n\n\n==== Ellipses ====\n\nThe formula for the area enclosed by an ellipse is related to the formula of a circle; for an ellipse with semi-major and semi-minor axes x and y the formula is:\n\n  \n    \n      \n        A\n        =\n        \u03c0\n        x\n        y\n        .\n      \n    \n    {\\displaystyle A=\\pi xy.}\n  \n\n\n=== Non-planar surface area ===\n\nMost basic formulas for surface area can be obtained by cutting surfaces and flattening them out (see: developable surfaces).  For example, if the side surface of a cylinder (or any prism) is cut lengthwise, the surface can be flattened out into a rectangle.  Similarly, if a cut is made along the side of a cone, the side surface can be flattened out into a sector of a circle, and the resulting area computed.\nThe formula for the surface area of a sphere is more difficult to derive: because a sphere has nonzero Gaussian curvature, it cannot be flattened out.  The formula for the surface area of a sphere was first obtained by Archimedes in his work On the Sphere and Cylinder.  The formula is:\n\nA = 4\u03c0r2  (sphere),\nwhere r is the radius of the sphere.  As with the formula for the area of a circle, any derivation of this formula inherently uses methods similar to calculus.\n\n\n=== General formulas ===\n\n\n==== Areas of 2-dimensional figures ====\n\nA triangle: \n  \n    \n      \n        \n          \n            \n              1\n              2\n            \n          \n        \n        B\n        h\n      \n    \n    {\\displaystyle {\\tfrac {1}{2}}Bh}\n  \n (where B is any side, and h is the distance from the line on which B lies to the other vertex of the triangle). This formula can be used if the height h is known. If the lengths of the three sides are known then Heron's formula can be used: \n  \n    \n      \n        \n          \n            s\n            (\n            s\n            \u2212\n            a\n            )\n            (\n            s\n            \u2212\n            b\n            )\n            (\n            s\n            \u2212\n            c\n            )\n          \n        \n      \n    \n    {\\displaystyle {\\sqrt {s(s-a)(s-b)(s-c)}}}\n  \n where a, b, c are the sides of the triangle, and \n  \n    \n      \n        s\n        =\n        \n          \n            \n              1\n              2\n            \n          \n        \n        (\n        a\n        +\n        b\n        +\n        c\n        )\n      \n    \n    {\\displaystyle s={\\tfrac {1}{2}}(a+b+c)}\n  \n is half of its perimeter. If an angle and its two included sides are given, the area is \n  \n    \n      \n        \n          \n            \n              1\n              2\n            \n          \n        \n        a\n        b\n        sin\n        \u2061\n        (\n        C\n        )\n      \n    \n    {\\displaystyle {\\tfrac {1}{2}}ab\\sin(C)}\n  \n where C is the given angle and a and b are its included sides. If the triangle is graphed on a coordinate plane, a matrix can be used and is simplified to the absolute value of \n  \n    \n      \n        \n          \n            \n              1\n              2\n            \n          \n        \n        (\n        \n          x\n          \n            1\n          \n        \n        \n          y\n          \n            2\n          \n        \n        +\n        \n          x\n          \n            2\n          \n        \n        \n          y\n          \n            3\n          \n        \n        +\n        \n          x\n          \n            3\n          \n        \n        \n          y\n          \n            1\n          \n        \n        \u2212\n        \n          x\n          \n            2\n          \n        \n        \n          y\n          \n            1\n          \n        \n        \u2212\n        \n          x\n          \n            3\n          \n        \n        \n          y\n          \n            2\n          \n        \n        \u2212\n        \n          x\n          \n            1\n          \n        \n        \n          y\n          \n            3\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\tfrac {1}{2}}(x_{1}y_{2}+x_{2}y_{3}+x_{3}y_{1}-x_{2}y_{1}-x_{3}y_{2}-x_{1}y_{3})}\n  \n. This formula is also known as the shoelace formula and is an easy way to solve for the area of a coordinate triangle by substituting the 3 points (x1,y1), (x2,y2), and (x3,y3). The shoelace formula can also be used to find the areas of other polygons when their vertices are known. Another approach for a coordinate triangle is to use calculus to find the area.\nA simple polygon constructed on a grid of equal-distanced points (i.e., points with integer coordinates) such that all the polygon's vertices are grid points: \n  \n    \n      \n        i\n        +\n        \n          \n            b\n            2\n          \n        \n        \u2212\n        1\n      \n    \n    {\\displaystyle i+{\\frac {b}{2}}-1}\n  \n, where i is the number of grid points inside the polygon and b is the number of boundary points. This result is known as Pick's theorem.\n\n\n==== Area in calculus ====\n\nThe area between a positive-valued curve and the horizontal axis, measured between two values a and b (b is defined as the larger of the two values) on the horizontal axis, is given by the integral from a to b of the function that represents the curve:\n\n  \n    \n      \n        A\n        =\n        \n          \u222b\n          \n            a\n          \n          \n            b\n          \n        \n        f\n        (\n        x\n        )\n        \n        d\n        x\n        .\n      \n    \n    {\\displaystyle A=\\int _{a}^{b}f(x)\\,dx.}\n  \n\nThe area between the graphs of two functions is equal to the integral of one function, f(x), minus the integral of the other function, g(x):\n\n  \n    \n      \n        A\n        =\n        \n          \u222b\n          \n            a\n          \n          \n            b\n          \n        \n        (\n        f\n        (\n        x\n        )\n        \u2212\n        g\n        (\n        x\n        )\n        )\n        \n        d\n        x\n        ,\n      \n    \n    {\\displaystyle A=\\int _{a}^{b}(f(x)-g(x))\\,dx,}\n  \n where \n  \n    \n      \n        f\n        (\n        x\n        )\n      \n    \n    {\\displaystyle f(x)}\n  \n is the curve with the greater y-value.\nAn area bounded by a function \n  \n    \n      \n        r\n        =\n        r\n        (\n        \u03b8\n        )\n      \n    \n    {\\displaystyle r=r(\\theta )}\n  \n expressed in polar coordinates is:\n\n  \n    \n      \n        A\n        =\n        \n          \n            1\n            2\n          \n        \n        \u222b\n        \n          r\n          \n            2\n          \n        \n        \n        d\n        \u03b8\n        .\n      \n    \n    {\\displaystyle A={1 \\over 2}\\int r^{2}\\,d\\theta .}\n  \n\nThe area enclosed by a parametric curve \n  \n    \n      \n        \n          \n            \n              u\n              \u2192\n            \n          \n        \n        (\n        t\n        )\n        =\n        (\n        x\n        (\n        t\n        )\n        ,\n        y\n        (\n        t\n        )\n        )\n      \n    \n    {\\displaystyle {\\vec {u}}(t)=(x(t),y(t))}\n  \n with endpoints \n  \n    \n      \n        \n          \n            \n              u\n              \u2192\n            \n          \n        \n        (\n        \n          t\n          \n            0\n          \n        \n        )\n        =\n        \n          \n            \n              u\n              \u2192\n            \n          \n        \n        (\n        \n          t\n          \n            1\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\vec {u}}(t_{0})={\\vec {u}}(t_{1})}\n  \n is given by the line integrals:\n\n  \n    \n      \n        \n          \u222e\n          \n            \n              t\n              \n                0\n              \n            \n          \n          \n            \n              t\n              \n                1\n              \n            \n          \n        \n        x\n        \n          \n            \n              y\n              \u02d9\n            \n          \n        \n        \n        d\n        t\n        =\n        \u2212\n        \n          \u222e\n          \n            \n              t\n              \n                0\n              \n            \n          \n          \n            \n              t\n              \n                1\n              \n            \n          \n        \n        y\n        \n          \n            \n              x\n              \u02d9\n            \n          \n        \n        \n        d\n        t\n        =\n        \n          \n            1\n            2\n          \n        \n        \n          \u222e\n          \n            \n              t\n              \n                0\n              \n            \n          \n          \n            \n              t\n              \n                1\n              \n            \n          \n        \n        (\n        x\n        \n          \n            \n              y\n              \u02d9\n            \n          \n        \n        \u2212\n        y\n        \n          \n            \n              x\n              \u02d9\n            \n          \n        \n        )\n        \n        d\n        t\n      \n    \n    {\\displaystyle \\oint _{t_{0}}^{t_{1}}x{\\dot {y}}\\,dt=-\\oint _{t_{0}}^{t_{1}}y{\\dot {x}}\\,dt={1 \\over 2}\\oint _{t_{0}}^{t_{1}}(x{\\dot {y}}-y{\\dot {x}})\\,dt}\n  \n\nor the z-component of\n\n  \n    \n      \n        \n          \n            1\n            2\n          \n        \n        \n          \u222e\n          \n            \n              t\n              \n                0\n              \n            \n          \n          \n            \n              t\n              \n                1\n              \n            \n          \n        \n        \n          \n            \n              u\n              \u2192\n            \n          \n        \n        \u00d7\n        \n          \n            \n              \n                \n                  u\n                  \u2192\n                \n              \n              \u02d9\n            \n          \n        \n        \n        d\n        t\n        .\n      \n    \n    {\\displaystyle {1 \\over 2}\\oint _{t_{0}}^{t_{1}}{\\vec {u}}\\times {\\dot {\\vec {u}}}\\,dt.}\n  \n\n(For details, see Green's theorem \u00a7 Area calculation.) This is the principle of the planimeter mechanical device.\n\n\n==== Bounded area between two quadratic functions ====\nTo find the bounded area between two quadratic functions, we first subtract one from the other, writing the difference as\n\n  \n    \n      \n        f\n        (\n        x\n        )\n        \u2212\n        g\n        (\n        x\n        )\n        =\n        a\n        \n          x\n          \n            2\n          \n        \n        +\n        b\n        x\n        +\n        c\n        =\n        a\n        (\n        x\n        \u2212\n        \u03b1\n        )\n        (\n        x\n        \u2212\n        \u03b2\n        )\n      \n    \n    {\\displaystyle f(x)-g(x)=ax^{2}+bx+c=a(x-\\alpha )(x-\\beta )}\n  \n\nwhere f(x) is the quadratic upper bound and g(x) is the quadratic lower bound. \nBy the area integral formulas above and Vieta's formula, we can obtain that\n\n  \n    \n      \n        A\n        =\n        \n          \n            \n              (\n              \n                b\n                \n                  2\n                \n              \n              \u2212\n              4\n              a\n              c\n              \n                )\n                \n                  3\n                  \n                    /\n                  \n                  2\n                \n              \n            \n            \n              6\n              \n                a\n                \n                  2\n                \n              \n            \n          \n        \n        =\n        \n          \n            a\n            6\n          \n        \n        (\n        \u03b2\n        \u2212\n        \u03b1\n        \n          )\n          \n            3\n          \n        \n        ,\n        \n        a\n        \u2260\n        0.\n      \n    \n    {\\displaystyle A={\\frac {(b^{2}-4ac)^{3/2}}{6a^{2}}}={\\frac {a}{6}}(\\beta -\\alpha )^{3},\\qquad a\\neq 0.}\n  \n\nThe above remains valid if one of the bounding functions is linear instead of quadratic.\n\n\n==== Surface area of 3-dimensional figures ====\nCone: \n  \n    \n      \n        \u03c0\n        r\n        \n          (\n          \n            r\n            +\n            \n              \n                \n                  r\n                  \n                    2\n                  \n                \n                +\n                \n                  h\n                  \n                    2\n                  \n                \n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\pi r\\left(r+{\\sqrt {r^{2}+h^{2}}}\\right)}\n  \n, where r is the radius of the circular base, and h is the height. That can also be rewritten as \n  \n    \n      \n        \u03c0\n        \n          r\n          \n            2\n          \n        \n        +\n        \u03c0\n        r\n        l\n      \n    \n    {\\displaystyle \\pi r^{2}+\\pi rl}\n  \n or \n  \n    \n      \n        \u03c0\n        r\n        (\n        r\n        +\n        l\n        )\n        \n        \n      \n    \n    {\\displaystyle \\pi r(r+l)\\,\\!}\n  \n where r is the radius and l is the slant height of the cone. \n  \n    \n      \n        \u03c0\n        \n          r\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\pi r^{2}}\n  \n is the base area while \n  \n    \n      \n        \u03c0\n        r\n        l\n      \n    \n    {\\displaystyle \\pi rl}\n  \n is the lateral surface area of the cone.\nCube: \n  \n    \n      \n        6\n        \n          s\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle 6s^{2}}\n  \n, where s is the length of an edge.\nCylinder: \n  \n    \n      \n        2\n        \u03c0\n        r\n        (\n        r\n        +\n        h\n        )\n      \n    \n    {\\displaystyle 2\\pi r(r+h)}\n  \n, where r is the radius of a base and h is the height. The \n  \n    \n      \n        2\n        \u03c0\n        r\n      \n    \n    {\\displaystyle 2\\pi r}\n  \n can also be rewritten as \n  \n    \n      \n        \u03c0\n        d\n      \n    \n    {\\displaystyle \\pi d}\n  \n, where d is the diameter.\nPrism: \n  \n    \n      \n        2\n        B\n        +\n        P\n        h\n      \n    \n    {\\displaystyle 2B+Ph}\n  \n, where B is the area of a base, P is the perimeter of a base, and h is the height of the prism.\npyramid: \n  \n    \n      \n        B\n        +\n        \n          \n            \n              P\n              L\n            \n            2\n          \n        \n      \n    \n    {\\displaystyle B+{\\frac {PL}{2}}}\n  \n, where B is the area of the base, P is the perimeter of the base, and L is the length of the slant.\nRectangular prism: \n  \n    \n      \n        2\n        (\n        \u2113\n        w\n        +\n        \u2113\n        h\n        +\n        w\n        h\n        )\n      \n    \n    {\\displaystyle 2(\\ell w+\\ell h+wh)}\n  \n, where \n  \n    \n      \n        \u2113\n      \n    \n    {\\displaystyle \\ell }\n  \n is the length, w is the width, and h is the height.\n\n\n==== General formula for surface area ====\nThe general formula for the surface area of the graph of a continuously differentiable function \n  \n    \n      \n        z\n        =\n        f\n        (\n        x\n        ,\n        y\n        )\n        ,\n      \n    \n    {\\displaystyle z=f(x,y),}\n  \n where \n  \n    \n      \n        (\n        x\n        ,\n        y\n        )\n        \u2208\n        D\n        \u2282\n        \n          \n            R\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle (x,y)\\in D\\subset \\mathbb {R} ^{2}}\n  \n and \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  \n is a region in the xy-plane with the smooth boundary:\n\n  \n    \n      \n        A\n        =\n        \n          \u222c\n          \n            D\n          \n        \n        \n          \n            \n              \n                (\n                \n                  \n                    \n                      \u2202\n                      f\n                    \n                    \n                      \u2202\n                      x\n                    \n                  \n                \n                )\n              \n              \n                2\n              \n            \n            +\n            \n              \n                (\n                \n                  \n                    \n                      \u2202\n                      f\n                    \n                    \n                      \u2202\n                      y\n                    \n                  \n                \n                )\n              \n              \n                2\n              \n            \n            +\n            1\n          \n        \n        \n        d\n        x\n        \n        d\n        y\n        .\n      \n    \n    {\\displaystyle A=\\iint _{D}{\\sqrt {\\left({\\frac {\\partial f}{\\partial x}}\\right)^{2}+\\left({\\frac {\\partial f}{\\partial y}}\\right)^{2}+1}}\\,dx\\,dy.}\n  \n\nAn even more general formula for the area of the graph of a parametric surface in the vector form \n  \n    \n      \n        \n          r\n        \n        =\n        \n          r\n        \n        (\n        u\n        ,\n        v\n        )\n        ,\n      \n    \n    {\\displaystyle \\mathbf {r} =\\mathbf {r} (u,v),}\n  \n where \n  \n    \n      \n        \n          r\n        \n      \n    \n    {\\displaystyle \\mathbf {r} }\n  \n is a continuously differentiable vector function of \n  \n    \n      \n        (\n        u\n        ,\n        v\n        )\n        \u2208\n        D\n        \u2282\n        \n          \n            R\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle (u,v)\\in D\\subset \\mathbb {R} ^{2}}\n  \n is:\n\n  \n    \n      \n        A\n        =\n        \n          \u222c\n          \n            D\n          \n        \n        \n          |\n          \n            \n              \n                \n                  \u2202\n                  \n                    r\n                  \n                \n                \n                  \u2202\n                  u\n                \n              \n            \n            \u00d7\n            \n              \n                \n                  \u2202\n                  \n                    r\n                  \n                \n                \n                  \u2202\n                  v\n                \n              \n            \n          \n          |\n        \n        \n        d\n        u\n        \n        d\n        v\n        .\n      \n    \n    {\\displaystyle A=\\iint _{D}\\left|{\\frac {\\partial \\mathbf {r} }{\\partial u}}\\times {\\frac {\\partial \\mathbf {r} }{\\partial v}}\\right|\\,du\\,dv.}\n  \n\n\n=== List of formulas ===\n\nThe above calculations show how to find the areas of many common shapes.\nThe areas of irregular (and thus arbitrary) polygons can be calculated using the \"Surveyor's formula\" (shoelace formula).\n\n\n=== Relation of area to perimeter ===\nThe isoperimetric inequality states that, for a closed curve of length L (so the region it encloses has perimeter L) and for area A of the region that it encloses,\n\n  \n    \n      \n        4\n        \u03c0\n        A\n        \u2264\n        \n          L\n          \n            2\n          \n        \n        ,\n      \n    \n    {\\displaystyle 4\\pi A\\leq L^{2},}\n  \n\nand  equality holds if and only if the curve is a circle. Thus a circle has the largest area of any closed figure with a given perimeter.\nAt the other extreme, a figure with given perimeter L could have an arbitrarily small area, as illustrated by a rhombus that is \"tipped over\" arbitrarily far so that two of its angles are arbitrarily close to 0\u00b0 and the other two are arbitrarily close to 180\u00b0.\nFor a circle, the ratio of the area to the circumference (the term for the perimeter of a circle) equals half the radius r. This can be seen from the area formula \u03c0r2 and the circumference formula 2\u03c0r.\nThe area of a regular polygon is half its perimeter times the apothem (where the apothem is the distance from the center to the nearest point on any side).\n\n\n=== Fractals ===\nDoubling the edge lengths of a polygon multiplies its area by four, which is two (the ratio of the new to the old side length) raised to the power of two (the dimension of the space the polygon resides in). But if the one-dimensional lengths of a  fractal drawn in two dimensions are all doubled, the spatial content of the fractal scales by a power of two that is not necessarily an integer. This power is called the fractal dimension of the fractal.\n\n\n== Area bisectors ==\n\nThere are an infinitude of lines that bisect the area of a triangle. Three of them are the medians of the triangle (which connect the sides' midpoints with the opposite vertices), and these are concurrent at the triangle's centroid; indeed, they are the only area bisectors that go through the centroid. Any line through a triangle that splits both the triangle's area and its perimeter in half goes through the triangle's incenter (the center of its incircle). There are either one, two, or three of these for any given triangle.\nAny line through the midpoint of a parallelogram bisects the area.\nAll area bisectors of a circle or other ellipse go through the center, and any chords through the center bisect the area. In the case of a circle they are the diameters of the circle.\n\n\n== Optimization ==\nGiven a wire contour, the surface of least area spanning (\"filling\") it is a minimal surface.  Familiar examples include soap bubbles.\nThe question of the filling area of the Riemannian circle remains open.\nThe circle has the largest area of any two-dimensional object having the same perimeter.\nA cyclic polygon (one inscribed in a circle) has the largest area of any polygon with a given number of sides of the same lengths.\nA version of the isoperimetric inequality for triangles states that the triangle of greatest area among all those with a given perimeter is equilateral.\nThe triangle of largest area of all those inscribed in a given circle is equilateral; and the triangle of smallest area of all those circumscribed around a given circle is equilateral.\nThe ratio of the area of the incircle to the area of an equilateral triangle, \n  \n    \n      \n        \n          \n            \u03c0\n            \n              3\n              \n                \n                  3\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {\\pi }{3{\\sqrt {3}}}}}\n  \n, is larger than that of any non-equilateral triangle.\nThe ratio of the area to the square of the perimeter of an equilateral triangle, \n  \n    \n      \n        \n          \n            1\n            \n              12\n              \n                \n                  3\n                \n              \n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle {\\frac {1}{12{\\sqrt {3}}}},}\n  \n is larger than that for any other triangle.\n\n\n== See also ==\nBrahmagupta quadrilateral, a cyclic quadrilateral with integer sides, integer diagonals, and integer area.\nEquiareal map\nHeronian triangle, a triangle with integer sides and integer area.\nList of triangle inequalities\nOne-seventh area triangle, an inner triangle with one-seventh the area of the reference triangle.\nRouth's theorem, a generalization of the one-seventh area triangle.\nOrders of magnitude\u2014A list of areas by size.\nDerivation of the formula of a pentagon\nPlanimeter, an instrument for measuring small areas, e.g. on maps.\nArea of a convex quadrilateral\nRobbins pentagon, a cyclic pentagon whose side lengths and area are all rational numbers.\n\n\n== References ==\n\n\n== External links ==",
        "unit": "area",
        "url": "https://en.wikipedia.org/wiki/Area"
    },
    {
        "_id": "Speed_of_light",
        "clean": "Speed of light",
        "text": "The speed of light in vacuum, commonly denoted c, is a universal physical constant that is exactly equal to 299,792,458 metres per second (approximately 300,000 kilometres per second; 186,000 miles per second; 671 million miles per hour). According to the special theory of relativity, c is the upper limit for the speed at which conventional matter or energy (and thus any signal carrying information) can travel through space.\nAll forms of electromagnetic radiation, including visible light, travel at the speed of light. For many practical purposes, light and other electromagnetic waves will appear to propagate instantaneously, but for long distances and very sensitive measurements, their finite speed has noticeable effects. Much starlight viewed on Earth is from the distant past, allowing humans to study the history of the universe by viewing distant objects. When communicating with distant space probes, it can take minutes to hours for signals to travel. In computing, the speed of light fixes the ultimate minimum communication delay. The speed of light can be used in time of flight measurements to measure large distances to extremely high precision.\nOle R\u00f8mer first demonstrated in 1676 that light does not travel instantaneously by studying the apparent motion of Jupiter's moon Io. Progressively more accurate measurements of its speed came over the following centuries. In a paper published in 1865, James Clerk Maxwell proposed that light was an electromagnetic wave and, therefore, travelled at speed c. In 1905, Albert Einstein postulated that the speed of light c with respect to any inertial frame of reference is a constant and is independent of the motion of the light source. He explored the consequences of that postulate by deriving the theory of relativity and, in doing so, showed that the parameter c had relevance outside of the context of light and electromagnetism.\nMassless particles and field perturbations, such as gravitational waves, also travel at speed c in vacuum. Such particles and waves travel at c regardless of the motion of the source or the inertial reference frame of the observer. Particles with nonzero rest mass can be accelerated to approach c but can never reach it, regardless of the frame of reference in which their speed is measured. In the theory of relativity, c interrelates space and time and appears in the famous mass\u2013energy equivalence, E = mc2.\nIn some cases, objects or waves may appear to travel faster than light (e.g., phase velocities of waves, the appearance of certain high-speed astronomical objects, and particular quantum effects). The expansion of the universe is understood to exceed the speed of light beyond a certain boundary.\nThe speed at which light propagates through transparent materials, such as glass or air, is less than c; similarly, the speed of electromagnetic waves in wire cables is slower than c. The ratio between c and the speed v at which light travels in a material is called the refractive index n of the material (n = \u2060c/v\u2060). For example, for visible light, the refractive index of glass is typically around 1.5, meaning that light in glass travels at \u2060c/1.5\u2060 \u2248 200000 km/s (124000 mi/s); the refractive index of air for visible light is about 1.0003, so the speed of light in air is about 90 km/s (56 mi/s) slower than c.\n\n\n== Numerical value, notation, and units ==\nThe speed of light in vacuum is usually denoted by a lowercase c, for \"constant\" or the Latin celeritas (meaning 'swiftness, celerity'). In 1856, Wilhelm Eduard Weber and Rudolf Kohlrausch had used c for a different constant that was later shown to equal \u221a2 times the speed of light in vacuum. Historically, the symbol V was used as an alternative symbol for the speed of light, introduced by James Clerk Maxwell in 1865. In 1894, Paul Drude redefined c with its modern meaning. Einstein used V in his original German-language papers on special relativity in 1905, but in 1907 he switched to c, which by then had become the standard symbol for the speed of light.\nSometimes c is used for the speed of waves in any material medium, and c0 for the speed of light in vacuum. This subscripted notation, which is endorsed in official SI literature, has the same form as related electromagnetic constants: namely, \u03bc0 for the vacuum permeability or magnetic constant, \u03b50 for the vacuum permittivity or electric constant, and Z0 for the impedance of free space. This article uses c exclusively for the speed of light in vacuum.\n\n\n=== Use in unit systems ===\n\nSince 1983, the constant c has been defined in the International System of Units (SI) as exactly 299792458 m/s; this relationship is used to define the metre as exactly the distance that light travels in vacuum in 1\u2044299792458 of a second. By using the value of c, as well as an accurate measurement of the second, one can thus establish a standard for the metre. As a dimensional physical constant, the numerical value of c is different for different unit systems. For example, in imperial units, the speed of light is approximately 186282 miles per second, or roughly 1 foot per nanosecond.\nIn branches of physics in which c appears often, such as in relativity, it is common to use systems of natural units of measurement or the geometrized unit system where c = 1. Using these units, c does not appear explicitly because multiplication or division by 1 does not affect the result. Its unit of light-second per second is still relevant, even if omitted.\n\n\n== Fundamental role in physics ==\n\nThe speed at which light waves propagate in vacuum is independent both of the motion of the wave source and of the inertial frame of reference of the observer. This invariance of the speed of light was postulated by Einstein in 1905, after being motivated by Maxwell's theory of electromagnetism and the lack of evidence for motion against the luminiferous aether. It has since been consistently confirmed by many experiments. It is only possible to verify experimentally that the two-way speed of light (for example, from a source to a mirror and back again) is frame-independent, because it is impossible to measure the one-way speed of light (for example, from a source to a distant detector) without some convention as to how clocks at the source and at the detector should be synchronized.\nBy adopting Einstein synchronization for the clocks, the one-way speed of light becomes equal to the two-way speed of light by definition. The special theory of relativity explores the consequences of this invariance of c with the assumption that the laws of physics are the same in all inertial frames of reference. One consequence is that c is the speed at which all massless particles and waves, including light, must travel in vacuum.\n\nSpecial relativity has many counterintuitive and experimentally verified implications. These include the equivalence of mass and energy (E = mc2), length contraction (moving objects shorten), and time dilation (moving clocks run more slowly). The factor \u03b3 by which lengths contract and times dilate is known as the Lorentz factor and is given by \u03b3 = (1 \u2212 v2/c2)\u22121/2, where v is the speed of the object. The difference of \u03b3 from 1 is negligible for speeds much slower than c, such as most everyday speeds \u2013 in which case special relativity is closely approximated by Galilean relativity \u2013 but it increases at relativistic speeds and diverges to infinity as v approaches c. For example, a time dilation factor of \u03b3 = 2 occurs at a relative velocity of 86.6% of the speed of light (v = 0.866 c). Similarly, a time dilation factor of \u03b3 = 10 occurs at 99.5% the speed of light (v = 0.995 c).\nThe results of special relativity can be summarized by treating space and time as a unified structure known as spacetime (with c relating the units of space and time), and requiring that physical theories satisfy a special symmetry called Lorentz invariance, whose mathematical formulation contains the parameter c. Lorentz invariance is an almost universal assumption for modern physical theories, such as quantum electrodynamics, quantum chromodynamics, the Standard Model of particle physics, and general relativity. As such, the parameter c is ubiquitous in modern physics, appearing in many contexts that are unrelated to light. For example, general relativity predicts that c is also the speed of gravity and of gravitational waves, and observations of gravitational waves have been consistent with this prediction. In non-inertial frames of reference (gravitationally curved spacetime or accelerated reference frames), the local speed of light is constant and equal to c, but the speed of light can differ from c when measured from a remote frame of reference, depending on how measurements are extrapolated to the region.\nIt is generally assumed that fundamental constants such as c have the same value throughout spacetime, meaning that they do not depend on location and do not vary with time. However, it has been suggested in various theories that the speed of light may have changed over time. No conclusive evidence for such changes has been found, but they remain the subject of ongoing research.\nIt is generally assumed that the two-way speed of light is isotropic, meaning that it has the same value regardless of the direction in which it is measured. Observations of the emissions from nuclear energy levels as a function of the orientation of the emitting nuclei in a magnetic field (see Hughes\u2013Drever experiment), and of rotating optical resonators (see Resonator experiments) have put stringent limits on the possible two-way anisotropy.\n\n\n=== Upper limit on speeds ===\nAccording to special relativity, the energy of an object with rest mass m and speed v is given by \u03b3mc2, where \u03b3 is the Lorentz factor defined above. When v is zero, \u03b3 is equal to one, giving rise to the famous E = mc2 formula for mass\u2013energy equivalence. The \u03b3 factor approaches infinity as v approaches c, and it would take an infinite amount of energy to accelerate an object with mass to the speed of light. The speed of light is the upper limit for the speeds of objects with positive rest mass, and individual photons cannot travel faster than the speed of light. This is experimentally established in many tests of relativistic energy and momentum.\n\nMore generally, it is impossible for signals or energy to travel faster than c. One argument for this follows from the counter-intuitive implication of special relativity known as the relativity of simultaneity. If the spatial distance between two events A and B is greater than the time interval between them multiplied by c then there are frames of reference in which A precedes B, others in which B precedes A, and others in which they are simultaneous. As a result, if something were travelling faster than c relative to an inertial frame of reference, it would be travelling backwards in time relative to another frame, and causality would be violated. In such a frame of reference, an \"effect\" could be observed before its \"cause\". Such a violation of causality has never been recorded, and would lead to paradoxes such as the tachyonic antitelephone.\n\n\n== Faster-than-light observations and experiments ==\n\nThere are situations in which it may seem that matter, energy, or information-carrying signal travels at speeds greater than c, but they do not. For example, as is discussed in the propagation of light in a medium section below, many wave velocities can exceed c. The phase velocity of X-rays through most glasses can routinely exceed c, but phase velocity does not determine the velocity at which waves convey information.\nIf a laser beam is swept quickly across a distant object, the spot of light can move faster than c, although the initial movement of the spot is delayed because of the time it takes light to get to the distant object at the speed c. However, the only physical entities that are moving are the laser and its emitted light, which travels at the speed c from the laser to the various positions of the spot. Similarly, a shadow projected onto a distant object can be made to move faster than c, after a delay in time. In neither case does any matter, energy, or information travel faster than light.\nThe rate of change in the distance between two objects in a frame of reference with respect to which both are moving (their closing speed) may have a value in excess of c. However, this does not represent the speed of any single object as measured in a single inertial frame.\nCertain quantum effects appear to be transmitted instantaneously and therefore faster than c, as in the EPR paradox. An example involves the quantum states of two particles that can be entangled. Until either of the particles is observed, they exist in a superposition of two quantum states. If the particles are separated and one particle's quantum state is observed, the other particle's quantum state is determined instantaneously. However, it is impossible to control which quantum state the first particle will take on when it is observed, so information cannot be transmitted in this manner.\nAnother quantum effect that predicts the occurrence of faster-than-light speeds is called the Hartman effect: under certain conditions the time needed for a virtual particle to tunnel through a barrier is constant, regardless of the thickness of the barrier. This could result in a virtual particle crossing a large gap faster than light. However, no information can be sent using this effect.\nSo-called superluminal motion is seen in certain astronomical objects, such as the relativistic jets of radio galaxies and quasars. However, these jets are not moving at speeds in excess of the speed of light: the apparent superluminal motion is a projection effect caused by objects moving near the speed of light and approaching Earth at a small angle to the line of sight: since the light which was emitted when the jet was farther away took longer to reach the Earth, the time between two successive observations corresponds to a longer time between the instants at which the light rays were emitted.\nA 2011 experiment where neutrinos were observed to travel faster than light turned out to be due to experimental error.\nIn models of the expanding universe, the farther galaxies are from each other, the faster they drift apart. For example, galaxies far away from Earth are inferred to be moving away from the Earth with speeds proportional to their distances. Beyond a boundary called the Hubble sphere, the rate at which their distance from Earth increases becomes greater than the speed of light.\nThese recession rates, defined as the increase in proper distance per cosmological time, are not velocities in a relativistic sense. Faster-than-light cosmological recession speeds are only a coordinate artifact.\n\n\n== Propagation of light ==\nIn classical physics, light is described as a type of electromagnetic wave. The classical behaviour of the electromagnetic field is described by Maxwell's equations, which predict that the speed c with which electromagnetic waves (such as light) propagate in vacuum is related to the distributed capacitance and inductance of vacuum, otherwise respectively known as the electric constant \u03b50 and the magnetic constant \u03bc0, by the equation\n\n  \n    \n      \n        c\n        =\n        \n          \n            1\n            \n              \n                \u03b5\n                \n                  0\n                \n              \n              \n                \u03bc\n                \n                  0\n                \n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle c={\\frac {1}{\\sqrt {\\varepsilon _{0}\\mu _{0}}}}.}\n  \n\nIn modern quantum physics, the electromagnetic field is described by the theory of quantum electrodynamics (QED). In this theory, light is described by the fundamental excitations (or quanta) of the electromagnetic field, called photons. In QED, photons are massless particles and thus, according to special relativity, they travel at the speed of light in vacuum.\nExtensions of QED in which the photon has a mass have been considered. In such a theory, its speed would depend on its frequency, and the invariant speed c of special relativity would then be the upper limit of the speed of light in vacuum. No variation of the speed of light with frequency has been observed in rigorous testing, putting stringent limits on the mass of the photon. The limit obtained depends on the model used: if the massive photon is described by Proca theory, the experimental upper bound for its mass is about 10\u221257 grams; if photon mass is generated by a Higgs mechanism, the experimental upper limit is less sharp, m \u2264 10\u221214 eV/c2  (roughly 2 \u00d7 10\u221247 g).\nAnother reason for the speed of light to vary with its frequency would be the failure of special relativity to apply to arbitrarily small scales, as predicted by some proposed theories of quantum gravity. In 2009, the observation of gamma-ray burst GRB 090510 found no evidence for a dependence of photon speed on energy, supporting tight constraints in specific models of spacetime quantization on how this speed is affected by photon energy for energies approaching the Planck scale.\n\n\n=== In a medium ===\n\nIn a medium, light usually does not propagate at a speed equal to c; further, different types of light wave will travel at different speeds. The speed at which the individual crests and troughs of a plane wave (a wave filling the whole space, with only one frequency) propagate is called the phase velocity vp. A physical signal with a finite extent (a pulse of light) travels at a different speed. The overall envelope of the pulse travels at the group velocity vg, and its earliest part travels at the front velocity vf.\n\nThe phase velocity is important in determining how a light wave travels through a material or from one material to another. It is often represented in terms of a refractive index. The refractive index of a material is defined as the ratio of c to the phase velocity vp in the material: larger indices of refraction indicate lower speeds. The refractive index of a material may depend on the light's frequency, intensity, polarization, or direction of propagation; in many cases, though, it can be treated as a material-dependent constant. The refractive index of air is approximately 1.0003. Denser media, such as water, glass, and diamond, have refractive indexes of around 1.3, 1.5 and 2.4, respectively, for visible light.\nIn exotic materials like Bose\u2013Einstein condensates near absolute zero, the effective speed of light may be only a few metres per second. However, this represents absorption and re-radiation delay between atoms, as do all slower-than-c speeds in material substances. As an extreme example of light \"slowing\" in matter, two independent teams of physicists claimed to bring light to a \"complete standstill\" by passing it through a Bose\u2013Einstein condensate of the element rubidium. The popular description of light being \"stopped\" in these experiments refers only to light being stored in the excited states of atoms, then re-emitted at an arbitrarily later time, as stimulated by a second laser pulse. During the time it had \"stopped\", it had ceased to be light. This type of behaviour is generally microscopically true of all transparent media which \"slow\" the speed of light.\nIn transparent materials, the refractive index generally is greater than 1, meaning that the phase velocity is less than c. In other materials, it is possible for the refractive index to become smaller than 1 for some frequencies; in some exotic materials it is even possible for the index of refraction to become negative. The requirement that causality is not violated implies that the real and imaginary parts of the dielectric constant of any material, corresponding respectively to the index of refraction and to the attenuation coefficient, are linked by the Kramers\u2013Kronig relations. In practical terms, this means that in a material with refractive index less than 1, the wave will be absorbed quickly.\nA pulse with different group and phase velocities (which occurs if the phase velocity is not the same for all the frequencies of the pulse) smears out over time, a process known as dispersion. Certain materials have an exceptionally low (or even zero) group velocity for light waves, a phenomenon called slow light.\nThe opposite, group velocities exceeding c, was proposed theoretically in 1993 and achieved experimentally in 2000. It should even be possible for the group velocity to become infinite or negative, with pulses travelling instantaneously or backwards in time.\n\nNone of these options allow information to be transmitted faster than c. It is impossible to transmit information with a light pulse any faster than the speed of the earliest part of the pulse (the front velocity). It can be shown that this is (under certain assumptions) always equal to c. \nIt is possible for a particle to travel through a medium faster than the phase velocity of light in that medium (but still slower than c). When a charged particle does that in a dielectric material, the electromagnetic equivalent of a shock wave, known as Cherenkov radiation, is emitted.\n\n\n== Practical effects of finiteness ==\nThe speed of light is of relevance to telecommunications: the one-way and  round-trip delay time are greater than zero. This applies from small to astronomical scales. On the other hand, some techniques depend on the finite speed of light, for example in distance measurements.\n\n\n=== Small scales ===\nIn computers, the speed of light imposes a limit on how quickly data can be sent between processors. If a processor operates at 1 gigahertz, a signal can travel only a maximum of about 30 centimetres (1 ft) in a single clock cycle \u2013 in practice, this distance is even shorter since the printed circuit board refracts and slows down signals. Processors must therefore be placed close to each other, as well as memory chips, to minimize communication latencies, and care must be exercised when routing wires between them to ensure signal integrity. If clock frequencies continue to increase, the speed of light may eventually become a limiting factor for the internal design of single chips.\n\n\n=== Large distances on Earth ===\n\nGiven that the equatorial circumference of the Earth is about 40075 km and that c is about 300000 km/s, the theoretical shortest time for a piece of information to travel half the globe along the surface is about 67 milliseconds. When light is traveling in optical fibre (a transparent material) the actual transit time is longer, in part because the speed of light is slower by about 35% in optical fibre, depending on its refractive index n. Straight lines are rare in global communications and the travel time increases when signals pass through electronic switches or signal regenerators.\nAlthough this distance is largely irrelevant for most applications, latency becomes important in fields such as high-frequency trading, where traders seek to gain minute advantages by delivering their trades to exchanges fractions of a second ahead of other traders. For example, traders have been switching to microwave communications between trading hubs, because of the advantage which radio waves travelling at near to the speed of light through air have over comparatively slower fibre optic signals.\n\n\n=== Spaceflight and astronomy ===\n\nSimilarly, communications between the Earth and spacecraft are not instantaneous. There is a brief delay from the source to the receiver, which becomes more noticeable as distances increase. This delay was significant for communications between ground control and Apollo 8 when it became the first crewed spacecraft to orbit the Moon: for every question, the ground control station had to wait at least three seconds for the answer to arrive.\nThe communications delay between Earth and Mars can vary between five and twenty minutes depending upon the relative positions of the two planets. As a consequence of this, if a robot on the surface of Mars were to encounter a problem, its human controllers would not be aware of it until approximately 4\u201324 minutes later. It would then take a further 4\u201324 minutes for commands to travel from Earth to Mars.\nReceiving light and other signals from distant astronomical sources takes much longer. For example, it takes 13 billion (13\u00d7109) years for light to travel to Earth from the faraway galaxies viewed in the Hubble Ultra-Deep Field images. Those photographs, taken today, capture images of the galaxies as they appeared 13 billion years ago, when the universe was less than a billion years old. The fact that more distant objects appear to be younger, due to the finite speed of light, allows astronomers to infer the evolution of stars, of galaxies, and of the universe itself.\nAstronomical distances are sometimes expressed in light-years, especially in popular science publications and media. A light-year is the distance light travels in one Julian year, around 9461 billion kilometres, 5879 billion miles, or 0.3066 parsecs. In round figures, a light year is nearly 10 trillion kilometres or nearly 6 trillion miles. Proxima Centauri, the closest star to Earth after the Sun, is around 4.2 light-years away.\n\n\n=== Distance measurement ===\n\nRadar systems measure the distance to a target by the time it takes a radio-wave pulse to return to the radar antenna after being reflected by the target: the distance to the target is half the round-trip transit time multiplied by the speed of light. A Global Positioning System (GPS) receiver measures its distance to GPS satellites based on how long it takes for a radio signal to arrive from each satellite, and from these distances calculates the receiver's position. Because light travels about 300000 kilometres (186000 miles) in one second, these measurements of small fractions of a second must be very precise. The Lunar Laser Ranging experiment, radar astronomy and the Deep Space Network determine distances to the Moon, planets and spacecraft, respectively, by measuring round-trip transit times.\n\n\n== Measurement ==\nThere are different ways to determine the value of c. One way is to measure the actual speed at which light waves propagate, which can be done in various astronomical and Earth-based setups. It is also possible to determine c from other physical laws where it appears, for example, by determining the values of the electromagnetic constants \u03b50 and \u03bc0 and using their relation to c. Historically, the most accurate results have been obtained by separately determining the frequency and wavelength of a light beam, with their product equalling c. This is described in more detail in the \"Interferometry\" section below.\nIn 1983 the metre was defined as \"the length of the path travelled by light in vacuum during a time interval of 1\u2044299792458 of a second\", fixing the value of the speed of light at 299792458 m/s by definition, as described below. Consequently, accurate measurements of the speed of light yield an accurate realization of the metre rather than an accurate value of c.\n\n\n=== Astronomical measurements ===\n\nOuter space is a convenient setting for measuring the speed of light because of its large scale and nearly perfect vacuum. Typically, one measures the time needed for light to traverse some reference distance in the Solar System, such as the radius of the Earth's orbit. Historically, such measurements could be made fairly accurately, compared to how accurately the length of the reference distance is known in Earth-based units.\nOle R\u00f8mer used an astronomical measurement to make the first quantitative estimate of the speed of light in the year 1676. When measured from Earth, the periods of moons orbiting a distant planet are shorter when the Earth is approaching the planet than when the Earth is receding from it. The difference is small, but the cumulative time becomes significant when measured over months. The distance travelled by light from the planet (or its moon) to Earth is shorter when the Earth is at the point in its orbit that is closest to its planet than when the Earth is at the farthest point in its orbit, the difference in distance being the diameter of the Earth's orbit around the Sun. The observed change in the moon's orbital period is caused by the difference in the time it takes light to traverse the shorter or longer distance. R\u00f8mer observed this effect for Jupiter's innermost major moon Io and deduced that light takes 22 minutes to cross the diameter of the Earth's orbit.\n\nAnother method is to use the aberration of light, discovered and explained by James Bradley in the 18th century. This effect results from the vector addition of the velocity of light arriving from a distant source (such as a star) and the velocity of its observer (see diagram on the right). A moving observer thus sees the light coming from a slightly different direction and consequently sees the source at a position shifted from its original position. Since the direction of the Earth's velocity changes continuously as the Earth orbits the Sun, this effect causes the apparent position of stars to move around. From the angular difference in the position of stars (maximally 20.5 arcseconds) it is possible to express the speed of light in terms of the Earth's velocity around the Sun, which with the known length of a year can be converted to the time needed to travel from the Sun to the Earth. In 1729, Bradley used this method to derive that light travelled 10210 times faster than the Earth in its orbit (the modern figure is 10066 times faster) or, equivalently, that it would take light 8 minutes 12 seconds to travel from the Sun to the Earth.\n\n\n==== Astronomical unit ====\nAn astronomical unit (AU) is approximately the average distance between the Earth and Sun. It was redefined in 2012 as exactly 149597870700 m. Previously the AU was not based on the International System of Units but in terms of the gravitational force exerted by the Sun in the framework of classical mechanics. The current definition uses the recommended value in metres for the previous definition of the astronomical unit, which was determined by measurement. This redefinition is analogous to that of the metre and likewise has the effect of fixing the speed of light to an exact value in astronomical units per second (via the exact speed of light in metres per second).\nPreviously, the inverse of c expressed in seconds per astronomical unit was measured by comparing the time for radio signals to reach different spacecraft in the Solar System, with their position calculated from the gravitational effects of the Sun and various planets. By combining many such measurements, a best fit value for the light time per unit distance could be obtained. For example, in 2009, the best estimate, as approved by the International Astronomical Union (IAU), was:\n\nlight time for unit distance: tau = 499.004783836(10) s,\nc = 0.00200398880410(4) AU/s = 173.144632674(3) AU/d.\nThe relative uncertainty in these measurements is 0.02 parts per billion (2\u00d710\u221211), equivalent to the uncertainty in Earth-based measurements of length by interferometry. Since the metre is defined to be the length travelled by light in a certain time interval, the measurement of the light time in terms of the previous definition of the astronomical unit can also be interpreted as measuring the length of an AU (old definition) in metres.\n\n\n=== Time of flight techniques ===\n\nA method of measuring the speed of light is to measure the time needed for light to travel to a mirror at a known distance and back. This is the working principle behind experiments by Hippolyte Fizeau and L\u00e9on Foucault.\nThe setup as used by Fizeau consists of a beam of light directed at a mirror 8 kilometres (5 mi) away. On the way from the source to the mirror, the beam passes through a rotating cogwheel. At a certain rate of rotation, the beam passes through one gap on the way out and another on the way back, but at slightly higher or lower rates, the beam strikes a tooth and does not pass through the wheel. Knowing the distance between the wheel and the mirror, the number of teeth on the wheel, and the rate of rotation, the speed of light can be calculated.\nThe method of Foucault replaces the cogwheel with a rotating mirror. Because the mirror keeps rotating while the light travels to the distant mirror and back, the light is reflected from the rotating mirror at a different angle on its way out than it is on its way back. From this difference in angle, the known speed of rotation and the distance to the distant mirror the speed of light may be calculated. Foucault used this apparatus to measure the speed of light in air versus water, based on a suggestion by Fran\u00e7ois Arago.\nToday, using oscilloscopes with time resolutions of less than one nanosecond, the speed of light can be directly measured by timing the delay of a light pulse from a laser or an LED reflected from a mirror. This method is less precise (with errors of the order of 1%) than other modern techniques, but it is sometimes used as a laboratory experiment in college physics classes.\n\n\n=== Electromagnetic constants ===\nAn option for deriving c that does not directly depend on a measurement of the propagation of electromagnetic waves is to use the relation between c and the vacuum permittivity \u03b50 and vacuum permeability \u03bc0 established by Maxwell's theory: c2 = 1/(\u03b50\u03bc0). The vacuum permittivity may be determined by measuring the capacitance and dimensions of a capacitor, whereas the value of the vacuum permeability was historically fixed at exactly 4\u03c0\u00d710\u22127 H\u22c5m\u22121 through the definition of the ampere. Rosa and Dorsey used this method in 1907 to find a value of 299710\u00b122 km/s. Their method depended upon having a standard unit of electrical resistance, the \"international ohm\", and so its accuracy was limited by how this standard was defined.\n\n\n=== Cavity resonance ===\n\nAnother way to measure the speed of light is to independently measure the frequency f and wavelength \u03bb of an electromagnetic wave in vacuum. The value of c can then be found by using the relation c = f\u03bb. One option is to measure the resonance frequency of a cavity resonator. If the dimensions of the resonance cavity are also known, these can be used to determine the wavelength of the wave. In 1946, Louis Essen and A.C. Gordon-Smith established the frequency for a variety of normal modes of microwaves of a microwave cavity of precisely known dimensions. The dimensions were established to an accuracy of about \u00b10.8 \u03bcm using gauges calibrated by interferometry. As the wavelength of the modes was known from the geometry of the cavity and from electromagnetic theory, knowledge of the associated frequencies enabled a calculation of the speed of light.\nThe Essen\u2013Gordon-Smith result, 299792\u00b19 km/s, was substantially more precise than those found by optical techniques. By 1950, repeated measurements by Essen established a result of 299792.5\u00b13.0 km/s.\nA household demonstration of this technique is possible, using a microwave oven and food such as marshmallows or margarine: if the turntable is removed so that the food does not move, it will cook the fastest at the antinodes (the points at which the wave amplitude is the greatest), where it will begin to melt. The distance between two such spots is half the wavelength of the microwaves; by measuring this distance and multiplying the wavelength by the microwave frequency (usually displayed on the back of the oven, typically 2450 MHz), the value of c can be calculated, \"often with less than 5% error\".\n\n\n=== Interferometry ===\n\nInterferometry is another method to find the wavelength of electromagnetic radiation for determining the speed of light. A coherent beam of light (e.g. from a laser), with a known frequency (f), is split to follow two paths and then recombined. By adjusting the path length while observing the interference pattern and carefully measuring the change in path length, the wavelength of the light (\u03bb) can be determined. The speed of light is then calculated using the equation c = \u03bbf.\nBefore the advent of laser technology, coherent radio sources were used for interferometry measurements of the speed of light. Interferometric determination of wavelength becomes less precise with wavelength and the experiments were thus limited in precision by the long wavelength (~4 mm (0.16 in)) of the radiowaves. The precision can be improved by using light with a shorter wavelength, but then it becomes difficult to directly measure the frequency of the light.\nOne way around this problem is to start with a low frequency signal of which the frequency can be precisely measured, and from this signal progressively synthesize higher frequency signals whose frequency can then be linked to the original signal. A laser can then be locked to the frequency, and its wavelength can be determined using interferometry. This technique was due to a group at the National Bureau of Standards (which later became the National Institute of Standards and Technology). They used it in 1972 to measure the speed of light in vacuum with a fractional uncertainty of 3.5\u00d710\u22129.\n\n\n== History ==\nUntil the early modern period, it was not known whether light travelled instantaneously or at a very fast finite speed. The first extant recorded examination of this subject was in ancient Greece. The ancient Greeks, Arabic scholars, and classical European scientists long debated this until R\u00f8mer provided the first calculation of the speed of light. Einstein's theory of special relativity postulates that the speed of light is constant regardless of one's frame of reference. Since then, scientists have provided increasingly accurate measurements.\n\n\n=== Early history ===\nEmpedocles (c. 490\u2013430 BCE) was the first to propose a theory of light and claimed that light has a finite speed. He maintained that light was something in motion, and therefore must take some time to travel. Aristotle argued, to the contrary, that \"light is due to the presence of something, but it is not a movement\". Euclid and Ptolemy advanced Empedocles' emission theory of vision, where light is emitted from the eye, thus enabling sight. Based on that theory, Heron of Alexandria argued that the speed of light must be infinite because distant objects such as stars appear immediately upon opening the eyes.\nEarly Islamic philosophers initially agreed with the Aristotelian view that light had no speed of travel. In 1021, Alhazen (Ibn al-Haytham) published the Book of Optics, in which he presented a series of arguments dismissing the emission theory of vision in favour of the now accepted intromission theory, in which light moves from an object into the eye. This led Alhazen to propose that light must have a finite speed, and that the speed of light is variable, decreasing in denser bodies. He argued that light is substantial matter, the propagation of which requires time, even if this is hidden from the senses. Also in the 11th century, Ab\u016b Rayh\u0101n al-B\u012br\u016bn\u012b agreed that light has a finite speed, and observed that the speed of light is much faster than the speed of sound.\nIn the 13th century, Roger Bacon argued that the speed of light in air was not infinite, using philosophical arguments backed by the writing of Alhazen and Aristotle. In the 1270s, Witelo considered the possibility of light travelling at infinite speed in vacuum, but slowing down in denser bodies.\nIn the early 17th century, Johannes Kepler believed that the speed of light was infinite since empty space presents no obstacle to it. Ren\u00e9 Descartes argued that if the speed of light were to be finite, the Sun, Earth, and Moon would be noticeably out of alignment during a lunar eclipse. Although this argument fails when aberration of light is taken into account, the latter was not recognized until the following century. Since such misalignment had not been observed, Descartes concluded the speed of light was infinite. Descartes speculated that if the speed of light were found to be finite, his whole system of philosophy might be demolished. Despite this, in his derivation of Snell's law, Descartes assumed that some kind of motion associated with light was faster in denser media. Pierre de Fermat derived Snell's law using the opposing assumption, the denser the medium the slower light travelled. Fermat also argued in support of a finite speed of light.\n\n\n=== First measurement attempts ===\nIn 1629, Isaac Beeckman proposed an experiment in which a person observes the flash of a cannon reflecting off a mirror about one mile (1.6 km) away. In 1638, Galileo Galilei proposed an experiment, with an apparent claim to having performed it some years earlier, to measure the speed of light by observing the delay between uncovering a lantern and its perception some distance away. He was unable to distinguish whether light travel was instantaneous or not, but concluded that if it were not, it must nevertheless be extraordinarily rapid. In 1667, the Accademia del Cimento of Florence reported that it had performed Galileo's experiment, with the lanterns separated by about one mile, but no delay was observed. The actual delay in this experiment would have been about 11 microseconds.\n\nThe first quantitative estimate of the speed of light was made in 1676 by Ole R\u00f8mer. From the observation that the periods of Jupiter's innermost moon Io appeared to be shorter when the Earth was approaching Jupiter than when receding from it, he concluded that light travels at a finite speed, and estimated that it takes light 22 minutes to cross the diameter of Earth's orbit. Christiaan Huygens combined this estimate with an estimate for the diameter of the Earth's orbit to obtain an estimate of speed of light of 220000 km/s, which is 27% lower than the actual value.\nIn his 1704 book Opticks, Isaac Newton reported R\u00f8mer's calculations of the finite speed of light and gave a value of \"seven or eight minutes\" for the time taken for light to travel from the Sun to the Earth (the modern value is 8 minutes 19 seconds). Newton queried whether R\u00f8mer's eclipse shadows were coloured. Hearing that they were not, he concluded the different colours travelled at the same speed. In 1729, James Bradley discovered stellar aberration. From this effect he determined that light must travel 10,210 times faster than the Earth in its orbit (the modern figure is 10,066 times faster) or, equivalently, that it would take light 8 minutes 12 seconds to travel from the Sun to the Earth.\n\n\n=== Connections with electromagnetism ===\n\nIn the 19th century Hippolyte Fizeau developed a method to determine the speed of light based on time-of-flight measurements on Earth and reported a value of 315000 km/s. His method was improved upon by L\u00e9on Foucault who obtained a value of 298000 km/s in 1862. In the year 1856, Wilhelm Eduard Weber and Rudolf Kohlrausch measured the ratio of the electromagnetic and electrostatic units of charge, 1/\u221a\u03b50\u03bc0, by discharging a Leyden jar, and found that its numerical value was very close to the speed of light as measured directly by Fizeau. The following year Gustav Kirchhoff calculated that an electric signal in a resistanceless wire travels along the wire at this speed.\nIn the early 1860s, Maxwell showed that, according to the theory of electromagnetism he was working on, electromagnetic waves propagate in empty space at a speed equal to the above Weber/Kohlrausch ratio, and drawing attention to the numerical proximity of this value to the speed of light as measured by Fizeau, he proposed that light is in fact an electromagnetic wave. Maxwell backed up his claim with his own experiment published in the 1868 Philosophical Transactions which determined the ratio of the electrostatic and electromagnetic units of electricity.\n\n\n=== \"Luminiferous aether\" ===\n\nThe wave properties of light were well known since Thomas Young. In the 19th century, physicists believed light was propagating in a medium called aether (or ether). But for electric force, it looks more like the gravitational force in Newton's law. A transmitting medium was not required. After Maxwell theory unified light and electric and magnetic waves, it was favored that both light and electric magnetic waves propagate in the same aether medium (or called the luminiferous aether).\n\nIt was thought at the time that empty space was filled with a background medium called the luminiferous aether in which the electromagnetic field existed. Some physicists thought that this aether acted as a preferred frame of reference for the propagation of light and therefore it should be possible to measure the motion of the Earth with respect to this medium, by measuring the isotropy of the speed of light. Beginning in the 1880s several experiments were performed to try to detect this motion, the most famous of which is the experiment performed by Albert A. Michelson and Edward W. Morley in 1887. The detected motion was found to always be nil (within observational error). Modern experiments indicate that the two-way speed of light is isotropic (the same in every direction) to within 6 nanometres per second.\nBecause of this experiment Hendrik Lorentz proposed that the motion of the apparatus through the aether may cause the apparatus to contract along its length in the direction of motion, and he further assumed that the time variable for moving systems must also be changed accordingly (\"local time\"), which led to the formulation of the Lorentz transformation. Based on Lorentz's aether theory, Henri Poincar\u00e9 (1900) showed that this local time (to first order in v/c) is indicated by clocks moving in the aether, which are synchronized under the assumption of constant light speed. In 1904, he speculated that the speed of light could be a limiting velocity in dynamics, provided that the assumptions of Lorentz's theory are all confirmed. In 1905, Poincar\u00e9 brought Lorentz's aether theory into full observational agreement with the principle of relativity.\n\n\n=== Special relativity ===\nIn 1905 Einstein postulated from the outset that the speed of light in vacuum, measured by a non-accelerating observer, is independent of the motion of the source or observer. Using this and the principle of relativity as a basis he derived the special theory of relativity, in which the speed of light in vacuum c featured as a fundamental constant, also appearing in contexts unrelated to light. This made the concept of the stationary aether (to which Lorentz and Poincar\u00e9 still adhered) useless and revolutionized the concepts of space and time.\n\n\n=== Increased accuracy of c and redefinition of the metre and second ===\n\nIn the second half of the 20th century, much progress was made in increasing the accuracy of measurements of the speed of light, first by cavity resonance techniques and later by laser interferometer techniques. These were aided by new, more precise, definitions of the metre and second. In 1950, Louis Essen determined the speed as 299792.5\u00b13.0 km/s, using cavity resonance. This value was adopted by the 12th General Assembly of the Radio-Scientific Union in 1957. In 1960, the metre was redefined in terms of the wavelength of a particular spectral line of krypton-86, and, in 1967, the second was redefined in terms of the hyperfine transition frequency of the ground state of caesium-133.\nIn 1972, using the laser interferometer method and the new definitions, a group at the US National Bureau of Standards in Boulder, Colorado determined the speed of light in vacuum to be c = 299792456.2\u00b11.1 m/s. This was 100 times less uncertain than the previously accepted value. The remaining uncertainty was mainly related to the definition of the metre. As similar experiments found comparable results for c, the 15th General Conference on Weights and Measures in 1975 recommended using the value 299792458 m/s for the speed of light.\n\n\n=== Defined as an explicit constant ===\nIn 1983 the 17th meeting of the General Conference on Weights and Measures (CGPM) found that wavelengths from frequency measurements and a given value for the speed of light are more reproducible than the previous standard. They kept the 1967 definition of second, so the caesium hyperfine frequency would now determine both the second and the metre. To do this, they redefined the metre as \"the length of the path traveled by light in vacuum during a time interval of 1/299792458 of a second\".\nAs a result of this definition, the value of the speed of light in vacuum is exactly 299792458 m/s and has become a defined constant in the SI system of units. Improved experimental techniques that, prior to 1983, would have measured the speed of light no longer affect the known value of the speed of light in SI units, but instead allow a more precise realization of the metre by more accurately measuring the wavelength of krypton-86 and other light sources.\nIn 2011, the CGPM stated its intention to redefine all seven SI base units using what it calls \"the explicit-constant formulation\", where each \"unit is defined indirectly by specifying explicitly an exact value for a well-recognized fundamental constant\", as was done for the speed of light. It proposed a new, but completely equivalent, wording of the metre's definition: \"The metre, symbol m, is the unit of length; its magnitude is set by fixing the numerical value of the speed of light in vacuum to be equal to exactly 299792458 when it is expressed in the SI unit m s\u22121.\" This was one of the changes that was incorporated in the 2019 revision of the SI, also termed the New SI.\n\n\n== See also ==\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Further reading ==\n\n\n=== Historical references ===\n\n\n=== Modern references ===\n\n\n== External links ==\n\n\"Test Light Speed in Mile Long Vacuum Tube\". Popular Science Monthly, September 1930, pp. 17\u201318.\nDefinition of the metre (International Bureau of Weights and Measures, BIPM)\nSpeed of light in vacuum (National Institute of Standards and Technology, NIST)\nData Gallery: Michelson Speed of Light (Univariate Location Estimation) (download data gathered by Albert A. Michelson)\nSubluminal (Java applet by Greg Egan demonstrating group velocity information limits)\nLight discussion on adding velocities\nSpeed of Light (Sixty Symbols, University of Nottingham Department of Physics [video])\nSpeed of Light, BBC Radio 4 discussion (In Our Time, 30 November 2006)\nSpeed of Light (Live-Counter \u2013 Illustrations)\nSpeed of Light \u2013 animated demonstrations\n\"The Velocity of Light\", Albert A. Nicholson, Scientific American, 28 September 1878, p. 193",
        "unit": "speed of light",
        "url": "https://en.wikipedia.org/wiki/Speed_of_light"
    },
    {
        "_id": "Basis_point",
        "clean": "Basis point",
        "text": "A basis point (often abbreviated as bp, often pronounced as \"bip\" or \"beep\") is one hundredth of 1 percentage point. Changes of interest rates are often stated in basis points. For example, if an existing interest rate of 10 percent is increased by 1 basis point, the new interest rate would be 10.01 percent.\nThe related term permyriad means one part per ten thousand.\n\n\n== Definition ==\n\n1 basis point (bp) = 1\u2031, 0.1\u2030, 0.01%, 10\u22124, \u20601/10,000\u2060, or 0.0001.\n10 bp = 10\u2031, 1\u2030, 0.1%, 10\u22123, \u20601/1,000\u2060, or 0.001.\n100 bp = 100\u2031, 10\u2030, 1%, 10\u22122, \u20601/100\u2060, or 0.01.\nBasis points are used as a convenient unit of measurement in contexts where percentage differences of less than 1% are discussed. The most common example is interest rates, where differences in interest rates of less than 1% per year are usually meaningful to talk about. For example, a difference of 0.10 percentage points is equivalent to a change of 10 basis points (e.g., a 4.67% rate increases by 10 basis points to 4.77%). In other words, an increase of 100 basis points means a rise by 1 percentage point.\nLike percentage points, basis points avoid the ambiguity between relative and absolute discussions about interest rates by dealing only with the absolute change in numeric value of a rate. For example,  if a report says there has been a \"1% increase\" from a 10% interest rate, this could refer to an increase either from 10% to 10.1% (relative, 1% of 10%), or from 10% to 11% (absolute, 1% plus 10%).  However, if the report says there has been a \"100 basis point increase\" from a 10% interest rate, then the interest rate of 10% has increased by 1.00% (the absolute change) to an 11% rate.\nIt is common practice in the financial industry to use basis points to denote a rate change in a financial instrument, or the difference (spread) between two interest rates, including the yields of fixed-income securities.\nSince certain loans and bonds may commonly be quoted in relation to some index or underlying security, they will often be quoted as a spread over (or under) the index. For example, a loan that bears interest of 0.50% per annum above the Secured Overnight Financing Rate (SOFR) is said to be 50 basis points over SOFR, which is commonly expressed as \"S+50bps\" or simply \"S+50\".\nThe term \"basis point\" has its origins in trading the \"basis\" or the spread between two interest rates.  Since the basis is usually small, these are quoted multiplied up by 10,000, and hence a \"full point\" movement in the \"basis\" is a basis point.  Contrast with pips in FX forward markets.\nExpense ratios of investment funds are often quoted in basis points.\n\n\n== Permyriad ==\nA related concept is one part per ten thousand, \u20601/10,000\u2060. The same unit is also (rarely) called a permyriad, literally meaning \"for (every) myriad (ten thousand)\".  If used interchangeably with basis point, the permyriad is potentially confusing because an increase of one basis point to a 10 basis point value is generally understood to mean an increase to 11 basis points; not an increase of one part in ten thousand, meaning an increase to 10.001 basis points. This is akin to the difference between percentage and percentage point. \n\n\n=== Unicode ===\nA permyriad is written with U+2031 \u2031 PER TEN THOUSAND SIGN (&pertenk;) which looks like a percent sign % with three zeroes to the right of the slash. (It can be regarded as a stylized form of the four zeros in the denominator of \"\u20601/10,000\u2060\", although it originates as a natural extension of the percent % and permille \u2030 signs.)\n\n\n== Related units ==\nPercentage point difference of 1 part in 100\nPercentage (%) 1 part in 100\nPer mille (\u2030) 1 part in 1,000\nPer cent mille (pcm) 1 part in 100,000\nParts per million (ppm) 1 part in 1,000,000\n\n\n== See also ==\nParts-per notation\nPer-unit system\nPercent point function\nTick size\n\n\n== References ==\n\n\n== External links ==\n Media related to Basis point at Wikimedia Commons",
        "unit": "basis point",
        "url": "https://en.wikipedia.org/wiki/Basis_point"
    },
    {
        "_id": "Energy",
        "clean": "Energy",
        "text": "Energy (from Ancient Greek  \u1f10\u03bd\u03ad\u03c1\u03b3\u03b5\u03b9\u03b1 (en\u00e9rgeia) 'activity') is the quantitative property that is transferred to a body or to a physical system, recognizable in the performance of work and in the form of heat and light. Energy is a conserved quantity\u2014the law of conservation of energy states that energy can be converted in form, but not created or destroyed; matter and energy may also be converted to one another. The unit of measurement for energy in the International System of Units (SI) is the joule (J).\nForms of energy include the kinetic energy of a moving object, the potential energy stored by an object (for instance due to its position in a field), the elastic energy stored in a solid object, chemical energy associated with chemical reactions, the radiant energy carried by electromagnetic radiation, the internal energy contained within a thermodynamic system, and rest energy associated with an object's rest mass.\nAll living organisms constantly take in and release energy. The Earth's climate and ecosystems processes are driven primarily by radiant energy from the sun. The energy industry provides the energy required for human civilization to function, which it obtains from energy resources such as fossil fuels, nuclear fuel, and renewable energy.\n\n\n== Forms ==\n\nThe total energy of a system can be subdivided and classified into potential energy, kinetic energy, or combinations of the two in various ways. Kinetic energy is determined by the movement of an object \u2013 or the composite motion of the object's components \u2013 while potential energy reflects the potential of an object to have motion, generally being based upon the object's position within a field or what is stored within the field itself.\nWhile these two categories are sufficient to describe all forms of energy, it is often convenient to refer to particular combinations of potential and kinetic energy as its own form. For example, the sum of translational and rotational kinetic and potential energy within a system is referred to as mechanical energy, whereas nuclear energy refers to the combined potentials within an atomic nucleus from either the nuclear force or the weak force, among other examples.\n\n\n== History ==\n\nThe word energy derives from the Ancient Greek: \u1f10\u03bd\u03ad\u03c1\u03b3\u03b5\u03b9\u03b1, romanized: energeia, lit.\u2009'activity, operation', which possibly appears for the first time in the work of Aristotle in the 4th century BC. In contrast to the modern definition, energeia was a qualitative philosophical concept, broad enough to include ideas such as happiness and pleasure.\nIn the late 17th century, Gottfried Leibniz proposed the idea of the Latin: vis viva, or living force, which defined as the product of the mass of an object and its velocity squared; he believed that total vis viva was conserved. To account for slowing due to friction, Leibniz theorized that thermal energy consisted of the motions of the constituent parts of matter, although it would be more than a century until this was generally accepted. The modern analog of this property, kinetic energy, differs from vis viva only by a factor of two. Writing in the early 18th century, \u00c9milie du Ch\u00e2telet proposed the concept of conservation of energy in the marginalia of her French language translation of Newton's Principia Mathematica, which represented the first formulation of a conserved measurable quantity that was distinct from momentum, and which would later be called \"energy\".\nIn 1807, Thomas Young was possibly the first to use the term \"energy\" instead of vis viva, in its modern sense. Gustave-Gaspard Coriolis described \"kinetic energy\" in 1829 in its modern sense, and in 1853, William Rankine coined the term \"potential energy\". The law of conservation of energy was also first postulated in the early 19th century, and applies to any isolated system. It was argued for some years whether heat was a physical substance, dubbed the caloric, or merely a physical quantity, such as momentum. In 1845 James Prescott Joule discovered the link between mechanical work and the generation of heat.\nThese developments led to the theory of conservation of energy, formalized largely by William Thomson (Lord Kelvin) as the field of thermodynamics. Thermodynamics aided the rapid development of explanations of chemical processes by Rudolf Clausius, Josiah Willard Gibbs, and Walther Nernst. It also led to a mathematical formulation of the concept of entropy by Clausius and to the introduction of laws of radiant energy by Jo\u017eef Stefan. According to Noether's theorem, the conservation of energy is a consequence of the fact that the laws of physics do not change over time. Thus, since 1918, theorists have understood that the law of conservation of energy is the direct mathematical consequence of the translational symmetry of the quantity conjugate to energy, namely time.\n\n\n== Units of measure ==\n\nIn the International System of Units (SI), the unit of energy is the joule. It is a derived unit that is equal to the energy expended, or work done, in applying a force of one newton through a distance of one metre. However energy can also be expressed in many other units not part of the SI, such as ergs, calories, British thermal units, kilowatt-hours and kilocalories, which require a conversion factor when expressed in SI units.\nThe SI unit of power, defined as energy per unit of time, is the watt, which is a joule per second. Thus, one joule is one watt-second, and 3600 joules equal one watt-hour. The CGS energy unit is the erg and the imperial and US customary unit is the foot pound. Other energy units such as the electronvolt, food calorie or thermodynamic kcal (based on the temperature change of water in a heating process), and BTU are used in specific areas of science and commerce.\nIn 1843, French physicist James Prescott Joule, namesake of the unit of measure, discovered that the gravitational potential energy lost by a descending weight attached via a string was equal to the internal energy gained by the water through friction with the paddle.\n\n\n== Scientific use ==\n\n\n=== Classical mechanics ===\n\nIn classical mechanics, energy is a conceptually and mathematically useful property, as it is a conserved quantity. Several formulations of mechanics have been developed using energy as a core concept.\nWork, a function of energy, is force times distance.\n\n  \n    \n      \n        W\n        =\n        \n          \u222b\n          \n            C\n          \n        \n        \n          F\n        \n        \u22c5\n        \n          d\n        \n        \n          s\n        \n      \n    \n    {\\displaystyle W=\\int _{C}\\mathbf {F} \\cdot \\mathrm {d} \\mathbf {s} }\n  \n\nThis says that the work (\n  \n    \n      \n        W\n      \n    \n    {\\displaystyle W}\n  \n) is equal to the line integral of the force F along a path C; for details see the mechanical work article. Work and thus energy is frame dependent. For example, consider a ball being hit by a bat. In the center-of-mass reference frame, the bat does no work on the ball. But, in the reference frame of the person swinging the bat, considerable work is done on the ball.\nThe total energy of a system is sometimes called the Hamiltonian, after William Rowan Hamilton. The classical equations of motion can be written in terms of the Hamiltonian, even for highly complex or abstract systems. These classical equations have direct analogs in nonrelativistic quantum mechanics.\nAnother energy-related concept is called the Lagrangian, after Joseph-Louis Lagrange. This formalism is as fundamental as the Hamiltonian, and both can be used to derive the equations of motion or be derived from them. It was invented in the context of classical mechanics, but is generally useful in modern physics. The Lagrangian is defined as the kinetic energy minus the potential energy. Usually, the Lagrange formalism is mathematically more convenient than the Hamiltonian for non-conservative systems (such as systems with friction).\nNoether's theorem (1918) states that any differentiable symmetry of the action of a physical system has a corresponding conservation law. Noether's theorem has become a fundamental tool of modern theoretical physics and the calculus of variations. A generalisation of the seminal formulations on constants of motion in Lagrangian and Hamiltonian mechanics (1788 and 1833, respectively), it does not apply to systems that cannot be modeled with a Lagrangian; for example, dissipative systems with continuous symmetries need not have a corresponding conservation law.\n\n\n=== Chemistry ===\nIn the context of chemistry, energy is an attribute of a substance as a consequence of its atomic, molecular, or aggregate structure. Since a chemical transformation is accompanied by a change in one or more of these kinds of structure, it is usually accompanied by a decrease, and sometimes an increase, of the total energy of the substances involved. Some energy may be transferred between the surroundings and the reactants in the form of heat or light; thus the products of a reaction have sometimes more but usually less energy than the reactants. A reaction is said to be exothermic or exergonic if the final state is lower on the energy scale than the initial state; in the less common case of endothermic reactions the situation is the reverse.\nChemical reactions are usually not possible unless the reactants surmount an energy barrier known as the activation energy. The speed of a chemical reaction (at a given temperature T) is related to the activation energy E by the Boltzmann's population factor e\u2212E/kT; that is, the probability of a molecule to have energy greater than or equal to E at a given temperature T. This exponential dependence of a reaction rate on temperature is known as the Arrhenius equation. The activation energy necessary for a chemical reaction can be provided in the form of thermal energy.\n\n\n=== Biology ===\n\nIn biology, energy is an attribute of all biological systems, from the biosphere to the smallest living organism. Within an organism it is responsible for growth and development of a biological cell or organelle of a biological organism. Energy used in respiration is stored in substances such as carbohydrates (including sugars), lipids, and proteins stored by cells. In human terms, the human equivalent (H-e) (Human energy conversion) indicates, for a given amount of energy expenditure, the relative quantity of energy needed for human metabolism, using as a standard an average human energy expenditure of 12,500 kJ per day and a basal metabolic rate of 80 watts.\nFor example, if our bodies run (on average) at 80 watts, then a light bulb running at 100 watts is running at 1.25 human equivalents (100 \u00f7 80) i.e. 1.25 H-e. For a difficult task of only a few seconds' duration, a person can put out thousands of watts, many times the 746 watts in one official horsepower. For tasks lasting a few minutes, a fit human can generate perhaps 1,000 watts. For an activity that must be sustained for an hour, output drops to around 300; for an activity kept up all day, 150 watts is about the maximum. The human equivalent assists understanding of energy flows in physical and biological systems by expressing energy units in human terms: it provides a \"feel\" for the use of a given amount of energy.\nSunlight's radiant energy is also captured by plants as chemical potential energy in photosynthesis, when carbon dioxide and water (two low-energy compounds) are converted into carbohydrates, lipids, proteins and oxygen. Release of the energy stored during photosynthesis as heat or light may be triggered suddenly by a spark in a forest fire, or it may be made available more slowly for animal or human metabolism when organic molecules are ingested and catabolism is triggered by enzyme action.\nAll living creatures rely on an external source of energy to be able to grow and reproduce \u2013 radiant energy from the Sun in the case of green plants and chemical energy (in some form) in the case of animals. The daily 1500\u20132000 Calories (6\u20138 MJ) recommended for a human adult are taken as food molecules, mostly carbohydrates and fats, of which glucose (C6H12O6) and stearin (C57H110O6) are convenient examples. The food molecules are oxidized to carbon dioxide and water in the mitochondria\n\n  \n    \n      \n        \n          \n            C\n            \n              6\n            \n            \n              \n            \n          \n          \n            H\n            \n              12\n            \n            \n              \n            \n          \n          \n            O\n            \n              6\n            \n            \n              \n            \n          \n          +\n          6\n          \n          \n            O\n            \n              2\n            \n            \n              \n            \n          \n          \u27f6\n          6\n          \n          \n            CO\n            \n              2\n            \n            \n              \n            \n          \n          +\n          6\n          \n          \n            H\n            \n              2\n            \n            \n              \n            \n          \n          O\n        \n      \n    \n    {\\displaystyle {\\ce {C6H12O6 + 6O2 -> 6CO2 + 6H2O}}}\n  \n\n  \n    \n      \n        \n          \n            C\n            \n              57\n            \n            \n              \n            \n          \n          \n            H\n            \n              110\n            \n            \n              \n            \n          \n          \n            O\n            \n              6\n            \n            \n              \n            \n          \n          +\n          \n            (\n            81\n            \n            \n              \n                1\n                2\n              \n            \n            )\n          \n          \n            O\n            \n              2\n            \n            \n              \n            \n          \n          \u27f6\n          57\n          \n          \n            CO\n            \n              2\n            \n            \n              \n            \n          \n          +\n          55\n          \n          \n            H\n            \n              2\n            \n            \n              \n            \n          \n          O\n        \n      \n    \n    {\\displaystyle {\\ce {C57H110O6 + (81 1/2) O2 -> 57CO2 + 55H2O}}}\n  \n\nand some of the energy is used to convert ADP into ATP:\n\nThe rest of the chemical energy of the carbohydrate or fat are converted into heat: the ATP is used as a sort of \"energy currency\", and some of the chemical energy it contains is used for other metabolism when ATP reacts with OH groups and eventually splits into ADP and phosphate (at each stage of a metabolic pathway, some chemical energy is converted into heat). Only a tiny fraction of the original chemical energy is used for work:\n\ngain in kinetic energy of a sprinter during a 100 m race: 4 kJ\ngain in gravitational potential energy of a 150 kg weight lifted through 2 metres: 3 kJ\nDaily food intake of a normal adult: 6\u20138 MJ\nIt would appear that living organisms are remarkably inefficient (in the physical sense) in their use of the energy they receive (chemical or radiant energy); most machines manage higher efficiencies. In growing organisms the energy that is converted to heat serves a vital purpose, as it allows the organism tissue to be highly ordered with regard to the molecules it is built from. The second law of thermodynamics states that energy (and matter) tends to become more evenly spread out across the universe: to concentrate energy (or matter) in one specific place, it is necessary to spread out a greater amount of energy (as heat) across the remainder of the universe (\"the surroundings\"). Simpler organisms can achieve higher energy efficiencies than more complex ones, but the complex organisms can occupy ecological niches that are not available to their simpler brethren. The conversion of a portion of the chemical energy to heat at each step in a metabolic pathway is the physical reason behind the pyramid of biomass observed in ecology. As an example, to take just the first step in the food chain: of the estimated 124.7 Pg/a of carbon that is fixed by photosynthesis, 64.3 Pg/a (52%) are used for the metabolism of green plants, i.e. reconverted into carbon dioxide and heat.\n\n\n=== Earth sciences ===\nIn geology, continental drift, mountain ranges, volcanoes, and earthquakes are phenomena that can be explained in terms of energy transformations in the Earth's interior, while meteorological phenomena like wind, rain, hail, snow, lightning, tornadoes and hurricanes are all a result of energy transformations in our atmosphere brought about by solar energy.\nSunlight is the main input to Earth's energy budget which accounts for its temperature and climate stability. Sunlight may be stored as gravitational potential energy after it strikes the Earth, as (for example when) water evaporates from oceans and is deposited upon mountains (where, after being released at a hydroelectric dam, it can be used to drive turbines or generators to produce electricity). Sunlight also drives most weather phenomena, save a few exceptions, like those generated by volcanic events for example. An example of a solar-mediated weather event is a hurricane, which occurs when large unstable areas of warm ocean, heated over months, suddenly give up some of their thermal energy to power a few days of violent air movement.\nIn a slower process, radioactive decay of atoms in the core of the Earth releases heat. This thermal energy drives plate tectonics and may lift mountains, via orogenesis. This slow lifting represents a kind of gravitational potential energy storage of the thermal energy, which may later be transformed into active kinetic energy during landslides, after a triggering event. Earthquakes also release stored elastic potential energy in rocks, a store that has been produced ultimately from the same radioactive heat sources. Thus, according to present understanding, familiar events such as landslides and earthquakes release energy that has been stored as potential energy in the Earth's gravitational field or elastic strain (mechanical potential energy) in rocks. Prior to this, they represent release of energy that has been stored in heavy atoms since the collapse of long-destroyed supernova stars (which created these atoms).\n\n\n=== Cosmology ===\nIn cosmology and astronomy the phenomena of stars, nova, supernova, quasars and gamma-ray bursts are the universe's highest-output energy transformations of matter. All stellar phenomena (including solar activity) are driven by various kinds of energy transformations. Energy in such transformations is either from gravitational collapse of matter (usually molecular hydrogen) into various classes of astronomical objects (stars, black holes, etc.), or from nuclear fusion (of lighter elements, primarily hydrogen).\nThe nuclear fusion of hydrogen in the Sun also releases another store of potential energy which was created at the time of the Big Bang. At that time, according to theory, space expanded and the universe cooled too rapidly for hydrogen to completely fuse into heavier elements. This meant that hydrogen represents a store of potential energy that can be released by fusion. Such a fusion process is triggered by heat and pressure generated from gravitational collapse of hydrogen clouds when they produce stars, and some of the fusion energy is then transformed into sunlight.\n\n\n=== Quantum mechanics ===\n\nIn quantum mechanics, energy is defined in terms of the energy operator\n(Hamiltonian) as a time derivative of the wave function. The Schr\u00f6dinger equation equates the energy operator to the full energy of a particle or a system. Its results can be considered as a definition of measurement of energy in quantum mechanics. The Schr\u00f6dinger equation describes the space- and time-dependence of a slowly changing (non-relativistic) wave function of quantum systems. The solution of this equation for a bound system is discrete (a set of permitted states, each characterized by an energy level) which results in the concept of quanta. In the solution of the Schr\u00f6dinger equation for any oscillator (vibrator) and for electromagnetic waves in a vacuum, the resulting energy states are related to the frequency by Planck's relation: \n  \n    \n      \n        E\n        =\n        h\n        \u03bd\n      \n    \n    {\\displaystyle E=h\\nu }\n  \n (where \n  \n    \n      \n        h\n      \n    \n    {\\displaystyle h}\n  \n is the Planck constant and \n  \n    \n      \n        \u03bd\n      \n    \n    {\\displaystyle \\nu }\n  \n the frequency). In the case of an electromagnetic wave these energy states are called quanta of light or photons.\n\n\n=== Relativity ===\nWhen calculating kinetic energy (work to accelerate a massive body from zero speed to some finite speed) relativistically \u2013 using Lorentz transformations instead of Newtonian mechanics \u2013 Einstein discovered an unexpected by-product of these calculations to be an energy term which does not vanish at zero speed. He called it rest energy: energy which every massive body must possess even when being at rest. The amount of energy is directly proportional to the mass of the body:\n\n  \n    \n      \n        \n          E\n          \n            0\n          \n        \n        =\n        \n          m\n          \n            0\n          \n        \n        \n          c\n          \n            2\n          \n        \n        ,\n      \n    \n    {\\displaystyle E_{0}=m_{0}c^{2},}\n  \nwhere\n\nm0 is the rest mass of the body,\nc is the speed of light in vacuum,\n\n  \n    \n      \n        \n          E\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle E_{0}}\n  \n is the rest energy.\nFor example, consider electron\u2013positron annihilation, in which the rest energy of these two individual particles (equivalent to their rest mass) is converted to the radiant energy of the photons produced in the process. In this system the matter and antimatter (electrons and positrons) are destroyed and changed to non-matter (the photons). However, the total mass and total energy do not change during this interaction. The photons each have no rest mass but nonetheless have radiant energy which exhibits the same inertia as did the two original particles. This is a reversible process \u2013 the inverse process is called pair creation \u2013 in which the rest mass of particles is created from the radiant energy of two (or more) annihilating photons.\nIn general relativity, the stress\u2013energy tensor serves as the source term for the gravitational field, in rough analogy to the way mass serves as the source term in the non-relativistic Newtonian approximation.\nEnergy and mass are manifestations of one and the same underlying physical property of a system. This property is responsible for the inertia and strength of gravitational interaction of the system (\"mass manifestations\"), and is also responsible for the potential ability of the system to perform work or heating (\"energy manifestations\"), subject to the limitations of other physical laws.\nIn classical physics, energy is a scalar quantity, the canonical conjugate to time. In special relativity energy is also a scalar (although not a Lorentz scalar but a time component of the energy\u2013momentum 4-vector). In other words, energy is invariant with respect to rotations of space, but not invariant with respect to rotations of spacetime (= boosts).\n\n\n== Transformation ==\n\nEnergy may be transformed between different forms at various efficiencies. Items that transform between these forms are called transducers. Examples of transducers include a battery (from chemical energy to electric energy), a dam (from gravitational potential energy to kinetic energy of moving water (and the blades of a turbine) and ultimately to electric energy through an electric generator), and a heat engine (from heat to work).\nExamples of energy transformation include generating electric energy from heat energy via a steam turbine, or lifting an object against gravity using electrical energy driving a crane motor. Lifting against gravity performs mechanical work on the object and stores gravitational potential energy in the object. If the object falls to the ground, gravity does mechanical work on the object which transforms the potential energy in the gravitational field to the kinetic energy released as heat on impact with the ground. The Sun transforms nuclear potential energy to other forms of energy; its total mass does not decrease due to that itself (since it still contains the same total energy even in different forms) but its mass does decrease when the energy escapes out to its surroundings, largely as radiant energy.\nThere are strict limits to how efficiently heat can be converted into work in a cyclic process, e.g. in a heat engine, as described by Carnot's theorem and the second law of thermodynamics. However, some energy transformations can be quite efficient. The direction of transformations in energy (what kind of energy is transformed to what other kind) is often determined by entropy (equal energy spread among all available degrees of freedom) considerations. In practice all energy transformations are permitted on a small scale, but certain larger transformations are not permitted because it is statistically unlikely that energy or matter will randomly move into more concentrated forms or smaller spaces.\nEnergy transformations in the universe over time are characterized by various kinds of potential energy, that has been available since the Big Bang, being \"released\" (transformed to more active types of energy such as kinetic or radiant energy) when a triggering mechanism is available. Familiar examples of such processes include nucleosynthesis, a process ultimately using the gravitational potential energy released from the gravitational collapse of supernovae to \"store\" energy in the creation of heavy isotopes (such as uranium and thorium), and nuclear decay, a process in which energy is released that was originally stored in these heavy elements, before they were incorporated into the Solar System and the Earth. This energy is triggered and released in nuclear fission bombs or in civil nuclear power generation. Similarly, in the case of a chemical explosion, chemical potential energy is transformed to kinetic and thermal energy in a very short time.\nYet another example is that of a pendulum. At its highest points the kinetic energy is zero and the gravitational potential energy is at its maximum. At its lowest point the kinetic energy is at its maximum and is equal to the decrease in potential energy. If one (unrealistically) assumes that there is no friction or other losses, the conversion of energy between these processes would be perfect, and the pendulum would continue swinging forever.\nEnergy is also transferred from potential energy (\n  \n    \n      \n        \n          E\n          \n            p\n          \n        \n      \n    \n    {\\displaystyle E_{p}}\n  \n) to kinetic energy (\n  \n    \n      \n        \n          E\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle E_{k}}\n  \n) and then back to potential energy constantly. This is referred to as conservation of energy. In this isolated system, energy cannot be created or destroyed; therefore, the initial energy and the final energy will be equal to each other. This can be demonstrated by the following:\n\nThe equation can then be simplified further since \n  \n    \n      \n        \n          E\n          \n            p\n          \n        \n        =\n        m\n        g\n        h\n      \n    \n    {\\displaystyle E_{p}=mgh}\n  \n (mass times acceleration due to gravity times the height) and \n  \n    \n      \n        \n          E\n          \n            k\n          \n        \n        =\n        \n          \n            1\n            2\n          \n        \n        m\n        \n          v\n          \n            2\n          \n        \n      \n    \n    {\\textstyle E_{k}={\\frac {1}{2}}mv^{2}}\n  \n (half mass times velocity squared). Then the total amount of energy can be found by adding \n  \n    \n      \n        \n          E\n          \n            p\n          \n        \n        +\n        \n          E\n          \n            k\n          \n        \n        =\n        \n          E\n          \n            total\n          \n        \n      \n    \n    {\\displaystyle E_{p}+E_{k}=E_{\\text{total}}}\n  \n.\n\n\n=== Conservation of energy and mass in transformation ===\nEnergy gives rise to weight when it is trapped in a system with zero momentum, where it can be weighed. It is also equivalent to mass, and this mass is always associated with it. Mass is also equivalent to a certain amount of energy, and likewise always appears associated with it, as described in mass\u2013energy equivalence. The formula E = mc\u00b2, derived by Albert Einstein (1905) quantifies the relationship between relativistic mass and energy within the concept of special relativity. In different theoretical frameworks, similar formulas were derived by J.J. Thomson (1881), Henri Poincar\u00e9 (1900), Friedrich Hasen\u00f6hrl (1904) and others (see Mass\u2013energy equivalence#History for further information).\nPart of the rest energy (equivalent to rest mass) of matter may be converted to other forms of energy (still exhibiting mass), but neither energy nor mass can be destroyed; rather, both remain constant during any process. However, since \n  \n    \n      \n        \n          c\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle c^{2}}\n  \n is extremely large relative to ordinary human scales, the conversion of an everyday amount of rest mass (for example, 1 kg) from rest energy to other forms of energy (such as kinetic energy, thermal energy, or the radiant energy carried by light and other radiation) can liberate tremendous amounts of energy (~\n  \n    \n      \n        9\n        \u00d7\n        \n          10\n          \n            16\n          \n        \n      \n    \n    {\\displaystyle 9\\times 10^{16}}\n  \n joules = 21 megatons of TNT), as can be seen in nuclear reactors and nuclear weapons.\nConversely, the mass equivalent of an everyday amount energy is minuscule, which is why a loss of energy (loss of mass) from most systems is difficult to measure on a weighing scale, unless the energy loss is very large. Examples of large transformations between rest energy (of matter) and other forms of energy (e.g., kinetic energy into particles with rest mass) are found in nuclear physics and particle physics. Often, however, the complete conversion of matter (such as atoms) to non-matter (such as photons) is forbidden by conservation laws.\n\n\n=== Reversible and non-reversible transformations ===\nThermodynamics divides energy transformation into two kinds: reversible processes and irreversible processes. An irreversible process is one in which energy is dissipated (spread) into empty energy states available in a volume, from which it cannot be recovered into more concentrated forms (fewer quantum states), without degradation of even more energy. A reversible process is one in which this sort of dissipation does not happen. For example, conversion of energy from one type of potential field to another is reversible, as in the pendulum system described above.\nIn processes where heat is generated, quantum states of lower energy, present as possible excitations in fields between atoms, act as a reservoir for part of the energy, from which it cannot be recovered, in order to be converted with 100% efficiency into other forms of energy. In this case, the energy must partly stay as thermal energy and cannot be completely recovered as usable energy, except at the price of an increase in some other kind of heat-like increase in disorder in quantum states, in the universe (such as an expansion of matter, or a randomization in a crystal).\nAs the universe evolves with time, more and more of its energy becomes trapped in irreversible states (i.e., as heat or as other kinds of increases in disorder). This has led to the hypothesis of the inevitable thermodynamic heat death of the universe. In this heat death the energy of the universe does not change, but the fraction of energy which is available to do work through a heat engine, or be transformed to other usable forms of energy (through the use of generators attached to heat engines), continues to decrease.\n\n\n== Conservation of energy ==\n\nThe fact that energy can be neither created nor destroyed is called the law of conservation of energy. In the form of the first law of thermodynamics, this states that a closed system's energy is constant unless energy is transferred in or out as work or heat, and that no energy is lost in transfer. The total inflow of energy into a system must equal the total outflow of energy from the system, plus the change in the energy contained within the system. Whenever one measures (or calculates) the total energy of a system of particles whose interactions do not depend explicitly on time, it is found that the total energy of the system always remains constant.\nWhile heat can always be fully converted into work in a reversible isothermal expansion of an ideal gas, for cyclic processes of practical interest in heat engines the second law of thermodynamics states that the system doing work always loses some energy as waste heat. This creates a limit to the amount of heat energy that can do work in a cyclic process, a limit called the available energy. Mechanical and other forms of energy can be transformed in the other direction into thermal energy without such limitations. The total energy of a system can be calculated by adding up all forms of energy in the system.\nRichard Feynman said during a 1961 lecture:\n\nThere is a fact, or if you wish, a law, governing all natural phenomena that are known to date. There is no known exception to this law \u2013 it is exact so far as we know. The law is called the conservation of energy. It states that there is a certain quantity, which we call energy, that does not change in manifold changes which nature undergoes. That is a most abstract idea, because it is a mathematical principle; it says that there is a numerical quantity which does not change when something happens. It is not a description of a mechanism, or anything concrete; it is just a strange fact that we can calculate some number and when we finish watching nature go through her tricks and calculate the number again, it is the same.\nMost kinds of energy (with gravitational energy being a notable exception) are subject to strict local conservation laws as well. In this case, energy can only be exchanged between adjacent regions of space, and all observers agree as to the volumetric density of energy in any given space. There is also a global law of conservation of energy, stating that the total energy of the universe cannot change; this is a corollary of the local law, but not vice versa.\nThis law is a fundamental principle of physics. As shown rigorously by Noether's theorem, the conservation of energy is a mathematical consequence of translational symmetry of time, a property of most phenomena below the cosmic scale that makes them independent of their locations on the time coordinate. Put differently, yesterday, today, and tomorrow are physically indistinguishable. This is because energy is the quantity which is canonical conjugate to time. This mathematical entanglement of energy and time also results in the uncertainty principle \u2013 it is impossible to define the exact amount of energy during any definite time interval (though this is practically significant only for very short time intervals). The uncertainty principle should not be confused with energy conservation \u2013 rather it provides mathematical limits to which energy can in principle be defined and measured.\nEach of the basic forces of nature is associated with a different type of potential energy, and all types of potential energy (like all other types of energy) appear as system mass, whenever present. For example, a compressed spring will be slightly more massive than before it was compressed. Likewise, whenever energy is transferred between systems by any mechanism, an associated mass is transferred with it.\nIn quantum mechanics energy is expressed using the Hamiltonian operator. On any time scales, the uncertainty in the energy is by\n\n  \n    \n      \n        \u0394\n        E\n        \u0394\n        t\n        \u2265\n        \n          \n            \u210f\n            2\n          \n        \n      \n    \n    {\\displaystyle \\Delta E\\Delta t\\geq {\\frac {\\hbar }{2}}}\n  \n\nwhich is similar in form to the Heisenberg Uncertainty Principle (but not really mathematically equivalent thereto, since H and t are not dynamically conjugate variables, neither in classical nor in quantum mechanics).\nIn particle physics, this inequality permits a qualitative understanding of virtual particles, which carry momentum. The exchange of virtual particles with real particles is responsible for the creation of all known fundamental forces (more accurately known as fundamental interactions). Virtual photons are also responsible for the electrostatic interaction between electric charges (which results in Coulomb's law), for spontaneous radiative decay of excited atomic and nuclear states, for the Casimir force, for the Van der Waals force and some other observable phenomena.\n\n\n== Energy transfer ==\n\n\n=== Closed systems ===\nEnergy transfer can be considered for the special case of systems which are closed to transfers of matter. The portion of the energy which is transferred by conservative forces over a distance is measured as the work the source system does on the receiving system. The portion of the energy which does not do work during the transfer is called heat. Energy can be transferred between systems in a variety of ways. Examples include the transmission of electromagnetic energy via photons, physical collisions which transfer kinetic energy, tidal interactions, and the conductive transfer of thermal energy.\nEnergy is strictly conserved and is also locally conserved wherever it can be defined. In thermodynamics, for closed systems, the process of energy transfer is described by the first law:\n\nwhere \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n  \n is the amount of energy transferred, \n  \n    \n      \n        W\n      \n    \n    {\\displaystyle W}\n  \n  represents the work done on or by the system, and \n  \n    \n      \n        Q\n      \n    \n    {\\displaystyle Q}\n  \n represents the heat flow into or out of the system. As a simplification, the heat term, \n  \n    \n      \n        Q\n      \n    \n    {\\displaystyle Q}\n  \n, can sometimes be ignored, especially for fast processes involving gases, which are poor conductors of heat, or when the thermal efficiency of the transfer is high. For such adiabatic processes,\n\nThis simplified equation is the one used to define the joule, for example.\n\n\n=== Open systems ===\nBeyond the constraints of closed systems, open systems can gain or lose energy in association with matter transfer (this process is illustrated by injection of an air-fuel mixture into a car engine, a system which gains in energy thereby, without addition of either work or heat). Denoting this energy by \n  \n    \n      \n        \n          E\n          \n            matter\n          \n        \n      \n    \n    {\\displaystyle E_{\\text{matter}}}\n  \n, one may write\n\n\n== Thermodynamics ==\n\n\n=== Internal energy ===\nInternal energy is the sum of all microscopic forms of energy of a system. It is the energy needed to create the system. It is related to the potential energy, e.g., molecular structure, crystal structure, and other geometric aspects, as well as the motion of the particles, in form of kinetic energy. Thermodynamics is chiefly concerned with changes in internal energy and not its absolute value, which is impossible to determine with thermodynamics alone.\n\n\n=== First law of thermodynamics ===\nThe first law of thermodynamics asserts that the total energy of a system and its surroundings (but not necessarily thermodynamic free energy) is always conserved and that heat flow is a form of energy transfer. For homogeneous systems, with a well-defined temperature and pressure, a commonly used corollary of the first law is that, for a system subject only to pressure forces and heat transfer (e.g., a cylinder-full of gas) without chemical changes, the differential change in the internal energy of the system (with a gain in energy signified by a positive quantity) is given as\n\n  \n    \n      \n        \n          d\n        \n        E\n        =\n        T\n        \n          d\n        \n        S\n        \u2212\n        P\n        \n          d\n        \n        V\n        \n      \n    \n    {\\displaystyle \\mathrm {d} E=T\\mathrm {d} S-P\\mathrm {d} V\\,}\n  \n,\nwhere the first term on the right is the heat transferred into the system, expressed in terms of temperature T and entropy S (in which entropy increases and its change dS is positive when heat is added to the system), and the last term on the right hand side is identified as work done on the system, where pressure is P and volume V (the negative sign results since compression of the system requires work to be done on it and so the volume change, dV, is negative when work is done on the system).\nThis equation is highly specific, ignoring all chemical, electrical, nuclear, and gravitational forces, effects such as advection of any form of energy other than heat and PV-work. The general formulation of the first law (i.e., conservation of energy) is valid even in situations in which the system is not homogeneous. For these cases the change in internal energy of a closed system is expressed in a general form by\n\n  \n    \n      \n        \n          d\n        \n        E\n        =\n        \u03b4\n        Q\n        +\n        \u03b4\n        W\n      \n    \n    {\\displaystyle \\mathrm {d} E=\\delta Q+\\delta W}\n  \n\nwhere \n  \n    \n      \n        \u03b4\n        Q\n      \n    \n    {\\displaystyle \\delta Q}\n  \n is the heat supplied to the system and \n  \n    \n      \n        \u03b4\n        W\n      \n    \n    {\\displaystyle \\delta W}\n  \n is the work applied to the system.\n\n\n=== Equipartition of energy ===\nThe energy of a mechanical harmonic oscillator (a mass on a spring) is alternately kinetic and potential energy. At two points in the oscillation cycle it is entirely kinetic, and at two points it is entirely potential. Over a whole cycle, or over many cycles, average energy is equally split between kinetic and potential. This is an example of the equipartition principle: the total energy of a system with many degrees of freedom is equally split among all available degrees of freedom, on average.\nThis principle is vitally important to understanding the behavior of a quantity closely related to energy, called entropy. Entropy is a measure of evenness of a distribution of energy between parts of a system. When an isolated system is given more degrees of freedom (i.e., given new available energy states that are the same as existing states), then total energy spreads over all available degrees equally without distinction between \"new\" and \"old\" degrees. This mathematical result is part of the second law of thermodynamics. The second law of thermodynamics is simple only for systems which are near or in a physical equilibrium state. For non-equilibrium systems, the laws governing the systems' behavior are still debatable. One of the guiding principles for these systems is the principle of maximum entropy production. It states that nonequilibrium systems behave in such a way as to maximize their entropy production.\n\n\n== See also ==\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Further reading ==\n\n\n=== Journals ===\nThe Journal of Energy History / Revue d'histoire de l'\u00e9nergie (JEHRHE), 2018\u2013 \n\n\n== External links ==\n\nDifferences between Heat and Thermal energy (Archived 2016-08-27 at the Wayback Machine) \u2013 BioCab",
        "unit": "energy",
        "url": "https://en.wikipedia.org/wiki/Energy"
    },
    {
        "_id": "Concentration",
        "clean": "Concentration",
        "text": "In chemistry, concentration is the abundance of a constituent divided by the total volume of a mixture. Several types of mathematical description can be distinguished: mass concentration, molar concentration, number concentration, and volume concentration. The concentration can refer to any kind of chemical mixture, but most frequently refers to solutes and solvents in solutions. The molar (amount) concentration has variants, such as normal concentration and osmotic concentration. Dilution is reduction of concentration, e.g. by adding solvent to a solution. The verb to concentrate means to increase concentration, the opposite of dilute.\n\n\n== Etymology ==\nConcentration-, concentratio, action or an act of coming together at a single place, bringing to a common center, was used in post-classical Latin in 1550 or earlier, similar terms attested in Italian (1589), Spanish (1589), English (1606), French (1632).\n\n\n== Qualitative description ==\n\nOften in informal, non-technical language, concentration is described in a qualitative way, through the use of adjectives such as \"dilute\" for solutions of relatively low concentration and \"concentrated\" for solutions of relatively high concentration. To concentrate a solution, one must add more solute (for example, alcohol), or reduce the amount of solvent (for example, water). By contrast, to dilute a solution, one must add more solvent, or reduce the amount of solute. Unless two substances are miscible, there exists a concentration at which no further solute will dissolve in a solution. At this point, the solution is said to be saturated. If additional solute is added to a saturated solution, it will not dissolve, except in certain circumstances, when supersaturation may occur. Instead, phase separation will occur, leading to coexisting phases, either completely separated or mixed as a suspension. The point of saturation depends on many variables, such as ambient temperature and the precise chemical nature of the solvent and solute.\nConcentrations are often called levels, reflecting the mental schema of levels on the vertical axis of a graph, which can be high or low (for example, \"high serum levels of bilirubin\" are concentrations of bilirubin in the blood serum that are greater than normal).\n\n\n== Quantitative notation ==\nThere are four quantities that describe concentration:\n\n\n=== Mass concentration ===\n\nThe mass concentration \n  \n    \n      \n        \n          \u03c1\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\rho _{i}}\n  \n is defined as the mass of a constituent \n  \n    \n      \n        \n          m\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle m_{i}}\n  \n divided by the volume of the mixture \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n  \n:\n\n  \n    \n      \n        \n          \u03c1\n          \n            i\n          \n        \n        =\n        \n          \n            \n              m\n              \n                i\n              \n            \n            V\n          \n        \n        .\n      \n    \n    {\\displaystyle \\rho _{i}={\\frac {m_{i}}{V}}.}\n  \n\nThe SI unit is kg/m3 (equal to g/L).\n\n\n=== Molar concentration ===\n\nThe molar concentration \n  \n    \n      \n        \n          c\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle c_{i}}\n  \n is defined as the amount of a constituent \n  \n    \n      \n        \n          n\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle n_{i}}\n  \n (in moles) divided by the volume of the mixture \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n  \n:\n\n  \n    \n      \n        \n          c\n          \n            i\n          \n        \n        =\n        \n          \n            \n              n\n              \n                i\n              \n            \n            V\n          \n        \n        .\n      \n    \n    {\\displaystyle c_{i}={\\frac {n_{i}}{V}}.}\n  \n\nThe SI unit is mol/m3. However, more commonly the unit mol/L (= mol/dm3) is used.\n\n\n=== Number concentration ===\n\nThe number concentration \n  \n    \n      \n        \n          C\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle C_{i}}\n  \n is defined as the number of entities of a constituent \n  \n    \n      \n        \n          N\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle N_{i}}\n  \n in a mixture divided by the volume of the mixture \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n  \n:\n\n  \n    \n      \n        \n          C\n          \n            i\n          \n        \n        =\n        \n          \n            \n              N\n              \n                i\n              \n            \n            V\n          \n        \n        .\n      \n    \n    {\\displaystyle C_{i}={\\frac {N_{i}}{V}}.}\n  \n\nThe SI unit is 1/m3.\n\n\n=== Volume concentration ===\nThe volume concentration \n  \n    \n      \n        \n          \u03c3\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\sigma _{i}}\n  \n (not to be confused with volume fraction) is defined as the volume of a constituent \n  \n    \n      \n        \n          V\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle V_{i}}\n  \n divided by the volume of the mixture \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n  \n:\n\n  \n    \n      \n        \n          \u03c3\n          \n            i\n          \n        \n        =\n        \n          \n            \n              V\n              \n                i\n              \n            \n            V\n          \n        \n        .\n      \n    \n    {\\displaystyle \\sigma _{i}={\\frac {V_{i}}{V}}.}\n  \n\nBeing dimensionless, it is expressed as a number, e.g., 0.18 or 18%.\nThere seems to be no standard notation in the English literature. The letter \n  \n    \n      \n        \n          \u03c3\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\sigma _{i}}\n  \n used here is normative in German literature (see Volumenkonzentration).\n\n\n== Related quantities ==\nSeveral other quantities can be used to describe the composition of a mixture. These should not be called concentrations.\n\n\n=== Normality ===\n\nNormality is defined as the molar concentration \n  \n    \n      \n        \n          c\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle c_{i}}\n  \n divided by an equivalence factor \n  \n    \n      \n        \n          f\n          \n            \n              e\n              q\n            \n          \n        \n      \n    \n    {\\displaystyle f_{\\mathrm {eq} }}\n  \n. Since the definition of the equivalence factor depends on context (which reaction is being studied), the International Union of Pure and Applied Chemistry and National Institute of Standards and Technology discourage the use of normality.\n\n\n=== Molality ===\n\nThe molality of a solution \n  \n    \n      \n        \n          b\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle b_{i}}\n  \n is defined as the amount of a constituent \n  \n    \n      \n        \n          n\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle n_{i}}\n  \n (in moles) divided by the mass of the solvent \n  \n    \n      \n        \n          m\n          \n            \n              s\n              o\n              l\n              v\n              e\n              n\n              t\n            \n          \n        \n      \n    \n    {\\displaystyle m_{\\mathrm {solvent} }}\n  \n (not the mass of the solution):\n\n  \n    \n      \n        \n          b\n          \n            i\n          \n        \n        =\n        \n          \n            \n              n\n              \n                i\n              \n            \n            \n              m\n              \n                \n                  s\n                  o\n                  l\n                  v\n                  e\n                  n\n                  t\n                \n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle b_{i}={\\frac {n_{i}}{m_{\\mathrm {solvent} }}}.}\n  \n\nThe SI unit for molality is mol/kg.\n\n\n=== Mole fraction ===\n\nThe mole fraction \n  \n    \n      \n        \n          x\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle x_{i}}\n  \n is defined as the amount of a constituent \n  \n    \n      \n        \n          n\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle n_{i}}\n  \n (in moles) divided by the total amount of all constituents in a mixture \n  \n    \n      \n        \n          n\n          \n            \n              t\n              o\n              t\n            \n          \n        \n      \n    \n    {\\displaystyle n_{\\mathrm {tot} }}\n  \n:\n\n  \n    \n      \n        \n          x\n          \n            i\n          \n        \n        =\n        \n          \n            \n              n\n              \n                i\n              \n            \n            \n              n\n              \n                \n                  t\n                  o\n                  t\n                \n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle x_{i}={\\frac {n_{i}}{n_{\\mathrm {tot} }}}.}\n  \n\nThe SI unit is mol/mol. However, the deprecated parts-per notation is often used to describe small mole fractions.\n\n\n=== Mole ratio ===\n\nThe mole ratio \n  \n    \n      \n        \n          r\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle r_{i}}\n  \n is defined as the amount of a constituent \n  \n    \n      \n        \n          n\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle n_{i}}\n  \n divided by the total amount of all other constituents in a mixture:\n\n  \n    \n      \n        \n          r\n          \n            i\n          \n        \n        =\n        \n          \n            \n              n\n              \n                i\n              \n            \n            \n              \n                n\n                \n                  \n                    t\n                    o\n                    t\n                  \n                \n              \n              \u2212\n              \n                n\n                \n                  i\n                \n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle r_{i}={\\frac {n_{i}}{n_{\\mathrm {tot} }-n_{i}}}.}\n  \n\nIf \n  \n    \n      \n        \n          n\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle n_{i}}\n  \n is much smaller than \n  \n    \n      \n        \n          n\n          \n            \n              t\n              o\n              t\n            \n          \n        \n      \n    \n    {\\displaystyle n_{\\mathrm {tot} }}\n  \n, the mole ratio is almost identical to the mole fraction.\nThe SI unit is mol/mol. However, the deprecated parts-per notation is often used to describe small mole ratios.\n\n\n=== Mass fraction ===\n\nThe mass fraction \n  \n    \n      \n        \n          w\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle w_{i}}\n  \n is the fraction of one substance with mass \n  \n    \n      \n        \n          m\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle m_{i}}\n  \n to the mass of the total mixture \n  \n    \n      \n        \n          m\n          \n            \n              t\n              o\n              t\n            \n          \n        \n      \n    \n    {\\displaystyle m_{\\mathrm {tot} }}\n  \n, defined as:\n\n  \n    \n      \n        \n          w\n          \n            i\n          \n        \n        =\n        \n          \n            \n              m\n              \n                i\n              \n            \n            \n              m\n              \n                \n                  t\n                  o\n                  t\n                \n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle w_{i}={\\frac {m_{i}}{m_{\\mathrm {tot} }}}.}\n  \n\nThe SI unit is kg/kg. However, the deprecated parts-per notation is often used to describe small mass fractions.\n\n\n=== Mass ratio ===\n\nThe mass ratio \n  \n    \n      \n        \n          \u03b6\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\zeta _{i}}\n  \n is defined as the mass of a constituent \n  \n    \n      \n        \n          m\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle m_{i}}\n  \n divided by the total mass of all other constituents in a mixture:\n\n  \n    \n      \n        \n          \u03b6\n          \n            i\n          \n        \n        =\n        \n          \n            \n              m\n              \n                i\n              \n            \n            \n              \n                m\n                \n                  \n                    t\n                    o\n                    t\n                  \n                \n              \n              \u2212\n              \n                m\n                \n                  i\n                \n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\zeta _{i}={\\frac {m_{i}}{m_{\\mathrm {tot} }-m_{i}}}.}\n  \n\nIf \n  \n    \n      \n        \n          m\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle m_{i}}\n  \n is much smaller than \n  \n    \n      \n        \n          m\n          \n            \n              t\n              o\n              t\n            \n          \n        \n      \n    \n    {\\displaystyle m_{\\mathrm {tot} }}\n  \n, the mass ratio is almost identical to the mass fraction.\nThe SI unit is kg/kg. However, the deprecated parts-per notation is often used to describe small mass ratios.\n\n\n== Dependence on volume and temperature ==\nConcentration depends on the variation of the volume of the solution with temperature, due mainly to thermal expansion.\n\n\n== Table of concentrations and related quantities ==\n\n\n== See also ==\nDilution ratio \u2013 Change in concentration when mixing two liquids\nDose concentration \u2013 Ratio of part of a mixture to the wholePages displaying short descriptions of redirect targets\nSerial dilution \u2013 Step-wise dilution of a substance in solution\nWine/water mixing problem\nStandard state \u00a7 Solutes\n\n\n== References ==\n\n\n== External links ==\n Media related to Concentration (chemistry) at Wikimedia Commons",
        "unit": "concentration",
        "url": "https://en.wikipedia.org/wiki/Concentration"
    },
    {
        "_id": "Centavo",
        "clean": "Centavo",
        "text": "The centavo (Spanish and Portuguese 'one hundredth') is a fractional monetary unit that represents one hundredth of a basic monetary unit in many countries around the world. The term comes from Latin centum (lit.\u2009'one hundred'), with the added suffix -avo ('portion').\nCoins of various denominations of centavos have been made from copper, stainless steel, aluminum-bronze, and silver.\n\n\n== Circulating ==\nPlaces that currently use the centavo include: \n\nArgentine peso\nBolivian boliviano\nBrazilian real\nCape Verdean escudo\nColombian peso\nCuban peso\nDominican peso\nEast Timorese centavo coins\nEcuadorian centavo coins\nGuatemalan quetzal\nHonduran lempira\nMacanese avos\nMexican peso\nMozambican metical\nNicaraguan c\u00f3rdoba\nPhilippine peso (In English usage; sentimo or c\u00e9ntimo is used in Tagalog and Spanish respectively.)\n\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\n\n== Obsolete ==\nFormer forms of the centavo that are no longer in use include: \n\nBrazilian cruzeiro (from 1942 to 1986 and from 1990 to 1993)\nBrazilian cruzado (from 1986 to 1989)\nBrazilian cruzado novo (from 1989 to 1990)\nCosta Rican col\u00f3n (Between 1917 and 1920 only. As c\u00e9ntimo for other periods.)\nEcuadorian sucre (New centavo coins continued to circulate after the sucre was replaced by U.S. dollar in 2000.)\nSalvadoran col\u00f3n\nGuinea Bissau peso\nMozambican escudo\nPortuguese escudo (before the euro was introduced)\nPortuguese Guinean escudo\nPortuguese Indian escudo\nPuerto Rican peso\nS\u00e3o Tom\u00e9 and Pr\u00edncipe escudo\nVenezuelan venezolano\nVenezuelan peso\nChilean Cent (from 1975 to 1983, as a subdivision of the Chilean peso; out of circulation due to inflation)\n\n\n== See also ==\n\nCent (currency)\nCoin\nC\u00e9ntimo\n\n\n== References ==",
        "unit": "centavo",
        "url": "https://en.wikipedia.org/wiki/Centavo"
    },
    {
        "_id": "Roentgen_(unit)",
        "clean": "Roentgen (unit)",
        "text": "The roentgen or r\u00f6ntgen (; symbol R) is a legacy unit of measurement for the exposure of X-rays and gamma rays, and is defined as the electric charge freed by such radiation in a specified volume of air divided by the mass of that air (statcoulomb per kilogram).\nIn 1928, it was adopted as the first international measurement quantity for ionizing radiation to be defined for radiation protection, as it was then the most easily replicated method of measuring air ionization by using ion chambers. It is named after the German physicist Wilhelm R\u00f6ntgen, who discovered X-rays and was awarded the first Nobel Prize in Physics for the discovery.\nHowever, although this was a major step forward in standardising radiation measurement, the roentgen has the disadvantage that it is only a measure of air ionisation, and not a direct measure of radiation absorption in other materials, such as different forms of human tissue. For instance, one roentgen deposits 0.00877 grays (0.877 rads) of absorbed dose in dry air, or 0.0096 Gy (0.96 rad) in soft tissue. One roentgen of X-rays may deposit anywhere from 0.01 to 0.04 Gy (1.0 to 4.0 rad) in bone depending on the beam energy.\nAs the science of radiation dosimetry developed, it was realised that the ionising effect, and hence tissue damage, was linked to the energy absorbed, not just radiation exposure. Consequently new radiometric units for radiation protection were defined which took this into account. In 1953 the International Commission on Radiation Units and Measurements (ICRU) recommended the rad, equal to 100 erg/g, as the unit of measure of the new radiation quantity absorbed dose. The rad was expressed in coherent cgs units. In 1975 the unit gray was named as the SI unit of absorbed dose. One gray is equal to 1 J/kg (i.e. 100 rad). Additionally, a new quantity, kerma, was defined for air ionisation as the exposure for instrument calibration, and from this the absorbed dose can be calculated using known coefficients for specific target materials. Today, for radiation protection, the modern units, absorbed dose for energy absorption and the equivalent dose (sievert) for stochastic effect, are overwhelmingly used, and the roentgen is rarely used. The International Committee for Weights and Measures (CIPM) has never accepted the use of the roentgen.\nThe roentgen has been redefined over the years. It was last defined by the U.S.'s National Institute of Standards and Technology (NIST) in 1998 as 2.58\u00d710\u22124 C/kg, with a recommendation that the definition be given in every document where the roentgen is used.\n\n\n== History ==\nThe roentgen has its roots in the Villard unit defined in 1908 by the American Roentgen Ray Society as \"the quantity of radiation which liberates by ionisation one esu of electricity per cm3 of air under normal conditions of temperature and pressure.\" Using 1 esu \u2248 3.33564\u00d710\u221210 C and the air density of ~1.293 kg/m3 at 0 \u00b0C and 101 kPa, this converts to 2.58 \u00d7 10\u22124 C/kg, which is the modern value given by NIST.\n1 \u2060esu/cm3\u2060 \u00d7 3.33564 \u00d7 10\u221210 \u2060C/esu\u2060 \u00d7 1,000,000 \u2060cm3/m3\u2060 \u00f7 1.293 \u2060kg/m3\u2060 = 2.58 \u00d7 10\u22124 \u2060C/kg\u2060\nThis definition was used under different names (e, R, and German unit of radiation) for the next 20 years. In the meantime, the French Roentgen was given a different definition which amounted to 0.444 German R.\n\n\n=== ICR definitions ===\nIn 1928, the International Congress of Radiology (ICR) defined the roentgen as \"the quantity of X-radiation which, when the secondary electrons are fully utilised and the wall effect of the chamber is avoided, produce in 1 cc of atmospheric air at 0 \u00b0C and 76 cm of mercury pressure such a degree of conductivity that 1 esu of charge is measured at saturation current.\" The stated 1 cc of air would have a mass of 1.293 mg at the conditions given, so in 1937 the ICR rewrote this definition in terms of this mass of air instead of volume, temperature and pressure. The 1937 definition was also extended to gamma rays, but later capped at 3 MeV in 1950.\n\n\n=== GOST definition ===\nThe USSR all-union committee of standards (GOST) had meanwhile adopted a significantly different definition of the roentgen in 1934. GOST standard 7623 defined it as \"the physical dose of X-rays which produces charges each of one electrostatic unit in magnitude per cm3 of irradiated volume in air at 0 \u00b0C and normal atmospheric pressure when ionization is complete.\" The distinction of physical dose from dose caused confusion, some of which may have led Cantrill and Parker report that the roentgen had become shorthand for 83 ergs per gram (0.0083 Gy) of tissue. They named this derivative quantity the roentgen equivalent physical (rep) to distinguish it from the ICR roentgen.\n\n\n=== ICRP definition ===\nThe introduction of the roentgen measurement unit, which relied upon measuring the ionisation of air, replaced earlier less accurate practices that relied on timed exposure, film exposure, or fluorescence. This led the way to setting exposure limits, and the National Council on Radiation Protection and Measurements of the United States established the first formal dose limit in 1931 as 0.1 roentgen per day. The International X-ray and Radium Protection Committee, now known as the International Commission on Radiological Protection (ICRP) soon followed with a limit of 0.2 roentgen per day in 1934. In 1950, the ICRP reduced their recommended limit to 0.3 roentgen per week for whole-body exposure.\nThe International Commission on Radiation Units and Measurements (ICRU) took over the definition of the roentgen in 1950, defining it as \"the quantity of X or \u03b3-radiation such that the associated corpuscular emission per 0.001293 gram of air produces, in air, ions carrying 1 electrostatic unit of quantity of electricity of either sign.\" The 3 MeV cap was no longer part of the definition, but the degraded usefulness of this unit at high beam energies was mentioned in the accompanying text. In the meantime, the new concept of roentgen equivalent man (rem) had been developed.\nStarting in 1957, the ICRP began to publish their recommendations in terms of rem, and the roentgen fell into disuse. The medical imaging community still has a need for ionization measurements, but they gradually converted to using C/kg as legacy equipment was replaced. The ICRU recommended redefining the roentgen to be exactly 2.58 \u00d7 10\u22124 C/kg in 1971.\n\n\n=== European Union ===\nIn 1971 the European Economic Community, in Directive 71/354/EEC, catalogued the units of measure that could be used \"for ... public health ... purposes\". The directive included the curie, rad, rem, and roentgen as permissible units, but required that the use of the rad, rem and roentgen be reviewed before 31 December 1977. This document defined the roentgen as exactly 2.58 \u00d7 10\u22124 C/kg, as per the ICRU recommendation. Directive 80/181/EEC, published in December 1979, which replaced directive 71/354/EEC, explicitly catalogued the gray, becquerel, and sievert for this purpose and required that the curie, rad, rem and roentgen be phased out by 31 December 1985.\n\n\n=== NIST definition ===\nToday the roentgen is rarely used, and the International Committee for Weights and Measures (CIPM) never accepted the use of the roentgen. From 1977 to 1998, the US NIST's translations of the SI brochure stated that the CIPM temporarily accepted the use of the roentgen (and other radiology units) with SI units since 1969. However, the only related CIPM decision shown in the appendix are with regards to the curie in 1964. The NIST brochures defined the roentgen as 2.58 \u00d7 10\u22124 C/kg, to be employed with exposures of x or \u03b3 radiation, but did not state the medium to be ionized. The CIPM's current SI brochure excludes the roentgen from the tables of non-SI units accepted for use with the SI. The US NIST clarified in 1998 that it was providing its own interpretations of the SI system, whereby it accepted the roentgen for use in the US with the SI, while recognizing that the CIPM did not. By then, the limitation to x and \u03b3 radiation had been dropped. NIST recommends defining the roentgen in every document where this unit is used. The continued use of the roentgen is strongly discouraged by the NIST.\n\n\n== Development of replacement radiometric quantities ==\n\nAlthough a convenient quantity to measure with an air ion chamber, the roentgen had the disadvantage that it was not a direct measure of either the intensity of X-rays or their absorption, but rather was a measurement of the ionising effect of X-rays in a specific circumstance; which was dry air at 0 \u00b0C and 1 standard atmosphere of pressure.\nBecause of this the roentgen had a variable relationship to the amount of energy absorbed dose per unit mass in the target material, as different materials have different absorption characteristics. As the science of radiation dosimetry developed, this was seen as a serious shortcoming.\nIn 1940, Louis Harold Gray, who had been studying the effect of neutron damage on human tissue, together with William Valentine Mayneord and the radiobiologist John Read, published a paper in which a unit of measure, dubbed the \"gram roentgen\" (symbol: gr) defined as \"that amount of neutron radiation which produces an increment in energy in unit volume of tissue equal to the increment of energy produced in unit volume of water by one roentgen of radiation\" was proposed. This unit was found to be equivalent to 88 ergs in air. In 1953 the ICRU recommended the rad, equal to 100 erg/g, as the new unit of measure of absorbed radiation. The rad was expressed in coherent cgs units.\nIn the late 1950s the General Conference on Weights and Measures (CGPM) invited the ICRU to join other scientific bodies to work with the International Committee for Weights and Measures (CIPM) in the development of a system of units that could be used consistently over many disciplines. This body, initially known as the \"Commission for the System of Units\", renamed in 1964 as the \"Consultative Committee for Units\" (CCU), was responsible for overseeing the development of the International System of Units (SI). At the same time it was becoming increasingly obvious that the definition of the roentgen was unsound, and in 1962 it was redefined.\nThe CCU decided to define the SI unit of absorbed radiation in terms of energy per unit mass, which in MKS units was J/kg. This was confirmed in 1975 by the 15th CGPM, and the unit was named the \"gray\" in honour of Louis Harold Gray, who had died in 1965. The gray was equal to 100 rad. The definition of the roentgen had had the attraction of being relatively simple to define for photons in air, but the gray is independent of the primary ionizing radiation type, and can be used for both kerma and absorbed dose in a wide range of matter.\nWhen measuring absorbed dose in a human due to external exposure, the SI unit the gray, or the related non-SI rad are used. From these can be developed the dose equivalents to consider biological effects from differing radiation types and target materials. These are equivalent dose, and effective dose for which the SI unit sievert or the non-SI rem are used.\n\n\n== Radiation-related quantities ==\nThe following table shows radiation quantities in SI and non-SI units:\n\n\n== See also ==\nGray (unit) \u2013 SI unit of absorbed dose\nOrders of magnitude (radiation)\nRad (unit) \u2013 cgs unit of absorbed dose\nRoentgen equivalent man, or rem \u2013 a unit of radiation dose equivalent\nSievert (symbol: Sv) \u2013 the SI derived unit of dose equivalent\nWilhelm R\u00f6ntgen\n\n\n== References ==\n\n\n== External links ==\nNIST: Units outside the SI\nRadiation Dose Units \u2013 Health Physics Society",
        "unit": "roentgen",
        "url": "https://en.wikipedia.org/wiki/Roentgen_(unit)"
    },
    {
        "_id": "Base_pair",
        "clean": "Base pair",
        "text": "A base pair (bp) is a fundamental unit of double-stranded nucleic acids consisting of two nucleobases bound to each other by hydrogen bonds.  They form the building blocks of the DNA double helix and contribute to the folded structure of both DNA and RNA. Dictated by specific hydrogen bonding patterns, \"Watson\u2013Crick\" (or \"Watson\u2013Crick\u2013Franklin\") base pairs (guanine\u2013cytosine and adenine\u2013thymine) allow the DNA helix to maintain a regular helical structure that is subtly dependent on its nucleotide sequence. The complementary nature of this based-paired structure provides a redundant copy of the genetic information encoded within each strand of DNA. The regular structure and data redundancy provided by the DNA double helix make DNA well suited to the storage of genetic information, while base-pairing between DNA and incoming nucleotides provides the mechanism through which DNA polymerase replicates DNA and RNA polymerase transcribes DNA into RNA. Many DNA-binding proteins can recognize specific base-pairing patterns that identify particular regulatory regions of genes.\nIntramolecular base pairs can occur within single-stranded nucleic acids. This is particularly important in RNA molecules (e.g., transfer RNA), where Watson\u2013Crick base pairs (guanine\u2013cytosine and adenine\u2013uracil) permit the formation of short double-stranded helices, and a wide variety of non\u2013Watson\u2013Crick interactions (e.g., G\u2013U or A\u2013A) allow RNAs to fold into a vast range of specific three-dimensional structures. In addition, base-pairing between transfer RNA (tRNA) and messenger RNA (mRNA) forms the basis for the molecular recognition events that result in the nucleotide sequence of mRNA becoming translated into the amino acid sequence of proteins via the genetic code.\nThe size of an individual gene or an organism's entire genome is often measured in base pairs because DNA is usually double-stranded. Hence, the number of total base pairs is equal to the number of nucleotides in one of the strands (with the exception of non-coding single-stranded regions of telomeres). The haploid human genome (23 chromosomes) is estimated to be about 3.2 billion base pairs long and to contain 20,000\u201325,000 distinct protein-coding genes. A kilobase (kb) is a unit of measurement in molecular biology equal to 1000 base pairs of DNA or RNA. The total number of DNA base pairs on Earth is estimated at 5.0\u00d71037 with a weight of 50 billion tonnes. In comparison, the total mass of the biosphere has been estimated to be as much as 4 TtC (trillion tons of carbon).\n\n\n== Hydrogen bonding and stability ==\n\nHydrogen bonding is the chemical interaction that underlies the base-pairing rules described above. Appropriate geometrical correspondence of hydrogen bond donors and acceptors allows only the \"right\" pairs to form stably. DNA with high GC-content is more stable than DNA with low GC-content. Crucially, however, stacking interactions are primarily responsible for stabilising the double-helical structure; Watson-Crick base pairing's contribution to global structural stability is minimal, but its role in the specificity underlying complementarity is, by contrast, of maximal importance as this underlies the template-dependent processes of the central dogma (e.g. DNA replication).\nThe bigger nucleobases, adenine and guanine, are members of a class of double-ringed chemical structures called purines; the smaller nucleobases, cytosine and thymine (and uracil), are members of a class of single-ringed chemical structures called pyrimidines. Purines are complementary only with pyrimidines: pyrimidine\u2013pyrimidine pairings are energetically unfavorable because the molecules are too far apart for hydrogen bonding to be established; purine\u2013purine pairings are energetically unfavorable because the molecules are too close, leading to overlap repulsion. Purine\u2013pyrimidine base-pairing of AT or GC or UA (in RNA) results in proper duplex structure. The only other purine\u2013pyrimidine pairings would be AC and GT and UG (in RNA); these pairings are mismatches because the patterns of hydrogen donors and acceptors do not correspond. The GU pairing, with two hydrogen bonds, does occur fairly often in RNA (see wobble base pair).\nPaired DNA and RNA molecules are comparatively stable at room temperature, but the two nucleotide strands will separate above a melting point that is determined by the length of the molecules, the extent of mispairing (if any), and the GC content. Higher GC content results in higher melting temperatures; it is, therefore, unsurprising that the genomes of extremophile organisms such as Thermus thermophilus are particularly GC-rich. On the converse, regions of a genome that need to separate frequently \u2014 for example, the promoter regions for often-transcribed genes \u2014 are comparatively GC-poor (for example, see TATA box). GC content and melting temperature must also be taken into account when designing  primers for PCR reactions.\n\n\n=== Examples ===\nThe following DNA sequences illustrate pair double-stranded patterns. By convention, the top strand is written from the 5\u2032-end to the 3\u2032-end; thus, the bottom strand is written 3\u2032 to 5\u2032.\n\nA base-paired DNA sequence:\nATCGATTGAGCTCTAGCG\nTAGCTAACTCGAGATCGC\nThe corresponding RNA sequence, in which uracil is substituted for thymine in the RNA strand:\nAUCGAUUGAGCUCUAGCG\nUAGCUAACUCGAGAUCGC\n\n\n== Base analogs and intercalators ==\n\nChemical analogs of nucleotides can take the place of proper nucleotides and establish non-canonical base-pairing, leading to errors (mostly point mutations) in DNA replication and DNA transcription. This is due to their isosteric chemistry. One common mutagenic base analog is 5-bromouracil, which resembles thymine but can base-pair to guanine in its enol form.\nOther chemicals, known as DNA intercalators, fit into the gap between adjacent bases on a single strand and induce frameshift mutations by \"masquerading\" as a base, causing the DNA replication machinery to skip or insert additional nucleotides at the intercalated site. Most intercalators are large polyaromatic compounds and are known or suspected carcinogens. Examples include ethidium bromide and acridine.\n\n\n== Mismatch repair ==\nMismatched base pairs can be generated by errors of DNA replication and as intermediates during homologous recombination. The process of mismatch repair ordinarily must recognize and correctly repair a small number of base mispairs within a long sequence of normal DNA base pairs.  To repair mismatches formed during DNA replication, several distinctive repair processes have evolved to distinguish between the template strand and the newly formed strand so that only the newly inserted incorrect nucleotide is removed (in order to avoid generating a mutation).  The proteins employed in mismatch repair during DNA replication, and the clinical significance of defects in this process are described in the article DNA mismatch repair.  The process of mispair correction during recombination is described in the article gene conversion.\n\n\n== Length measurements ==\n\nThe following abbreviations are commonly used to describe the length of a D/RNA molecule:\n\nbp  = base pair\u2014one bp corresponds to approximately 3.4 \u00c5 (340 pm)  of length along the strand, and to roughly 618 or 643 daltons for DNA and RNA respectively.\nkb (= kbp) = kilo\u2013base-pair = 1,000 bp\nMb (= Mbp) = mega\u2013base-pair = 1,000,000 bp\nGb (= Gbp) = giga\u2013base-pair = 1,000,000,000 bp\nFor single-stranded DNA/RNA, units of nucleotides are used\u2014abbreviated nt (or knt, Mnt, Gnt)\u2014as they are not paired.\nTo distinguish between units of computer storage and bases, kbp, Mbp, Gbp, etc. may be used for base pairs.\nThe centimorgan is also often used to imply distance along a chromosome, but the number of base pairs it corresponds to varies widely. In the human genome, the centimorgan is about 1 million base pairs.\n\n\n== Unnatural base pair (UBP) ==\n\nAn unnatural base pair (UBP) is a designed subunit (or nucleobase) of DNA which is created in a laboratory and does not occur in nature.  DNA sequences have been described which use newly created nucleobases to form a third base pair, in addition to the two base pairs found in nature, A-T (adenine \u2013 thymine) and G-C (guanine \u2013 cytosine).  A few research groups have been searching for a third base pair for DNA, including teams led by Steven A. Benner, Philippe Marliere, Floyd E. Romesberg and Ichiro Hirao. Some new base pairs based on alternative hydrogen bonding, hydrophobic interactions and metal coordination have been reported.\nIn 1989 Steven Benner (then working at the Swiss Federal Institute of Technology in Zurich) and his team led with modified forms of cytosine and guanine into DNA molecules in vitro. The nucleotides, which encoded RNA and proteins, were successfully replicated in vitro. Since then, Benner's team has been trying to engineer cells that can make foreign bases from scratch, obviating the need for a feedstock.\nIn 2002, Ichiro Hirao's group in Japan developed an unnatural base pair between 2-amino-8-(2-thienyl)purine (s) and pyridine-2-one (y) that functions in transcription and translation, for the site-specific incorporation of non-standard amino acids into proteins. In 2006, they created 7-(2-thienyl)imidazo[4,5-b]pyridine (Ds) and pyrrole-2-carbaldehyde (Pa) as a third base pair for replication and transcription. Afterward, Ds and 4-[3-(6-aminohexanamido)-1-propynyl]-2-nitropyrrole (Px) was discovered as a high fidelity pair in PCR amplification. In 2013, they applied the Ds-Px pair to DNA aptamer generation by in vitro selection (SELEX) and demonstrated the genetic alphabet expansion significantly augment DNA aptamer affinities to target proteins.\nIn 2012, a group of American scientists led by Floyd Romesberg, a chemical biologist at the Scripps Research Institute in San Diego, California, published that his team designed an unnatural base pair (UBP).  The two new artificial nucleotides or Unnatural Base Pair (UBP) were named d5SICS and dNaM. More technically, these artificial nucleotides bearing hydrophobic nucleobases, feature two fused aromatic rings that form a (d5SICS\u2013dNaM) complex or base pair in DNA. His team designed a variety of in vitro or \"test tube\" templates containing the unnatural base pair and they confirmed that it was efficiently replicated with high fidelity in virtually all sequence contexts using the modern standard in vitro techniques, namely PCR amplification of DNA and PCR-based applications. Their results show that for PCR and PCR-based applications, the d5SICS\u2013dNaM unnatural base pair is functionally equivalent to a natural base pair, and when combined with the other two natural base pairs used by all organisms, A\u2013T and G\u2013C, they provide a fully functional and expanded six-letter \"genetic alphabet\".\nIn 2014 the same team from the Scripps Research Institute reported that they synthesized a stretch of circular DNA known as a plasmid containing natural T-A and C-G base pairs along with the best-performing UBP Romesberg's laboratory had designed and inserted it into cells of the common bacterium E. coli that successfully replicated the unnatural base pairs through multiple generations. The transfection did not hamper the growth of the E. coli cells and showed no sign of losing its unnatural base pairs to its natural DNA repair mechanisms. This is the first known example of a living organism passing along an expanded genetic code to subsequent generations. Romesberg said he and his colleagues created 300 variants to refine the design of nucleotides that would be stable enough and would be replicated as easily as the natural ones when the cells divide.  This was in part achieved by the addition of a supportive algal gene that expresses a nucleotide triphosphate transporter which efficiently imports the triphosphates of both d5SICSTP and dNaMTP into E. coli bacteria. Then, the natural bacterial replication pathways use them to accurately replicate a plasmid containing d5SICS\u2013dNaM. Other researchers were surprised that the bacteria replicated these human-made DNA subunits.\nThe successful incorporation of a third base pair is a significant breakthrough toward the goal of greatly expanding the number of amino acids which can be encoded by DNA, from the existing 20 amino acids to a theoretically possible 172, thereby expanding the potential for living organisms to produce novel proteins. The artificial strings of DNA do not encode for anything yet, but scientists speculate they could be designed to manufacture new proteins which could have industrial or pharmaceutical uses. Experts said the synthetic DNA incorporating the unnatural base pair raises the possibility of life forms based on a different DNA code.\n\n\n== Non-canonical base pairing ==\n\nIn addition to the canonical pairing, some conditions can also favour base-pairing with alternative base orientation, and number and geometry of hydrogen bonds. These pairings are accompanied by alterations to the local backbone shape.\nThe most common of these is the wobble base pairing that occurs between tRNAs and mRNAs at the third base position of many codons during transcription and during the charging of tRNAs by some tRNA synthetases. They have also been observed in the secondary structures of some RNA sequences.\nAdditionally, Hoogsteen base pairing (typically written as A\u2022U/T and G\u2022C) can exist in some DNA sequences (e.g. CA and TA dinucleotides) in dynamic equilibrium with standard Watson\u2013Crick pairing. They have also been observed in some protein\u2013DNA complexes.\nIn addition to these alternative base pairings, a wide range of base-base hydrogen bonding is observed in RNA secondary and tertiary structure. These bonds are often necessary for the precise, complex shape of an RNA, as well as its binding to interaction partners.\n\n\n== See also ==\nList of Y-DNA single-nucleotide polymorphisms\nNon-canonical base pairing\nChargaff's rules\n\n\n== References ==\n\n\n== Further reading ==\n\n\n== External links ==\n\nDAN\u2014webserver version of the EMBOSS tool for calculating melting temperatures",
        "unit": "mega base pair",
        "url": "https://en.wikipedia.org/wiki/Base_pair"
    },
    {
        "_id": "Viscosity",
        "clean": "Viscosity",
        "text": "The viscosity of a fluid is a measure of its resistance to deformation at a given rate. For liquids, it corresponds to the informal concept of \"thickness\": for example, syrup has a higher viscosity than water. Viscosity is defined scientifically as a force multiplied by a time divided by an area. Thus its SI units are newton-seconds per square meter, or pascal-seconds.\nViscosity quantifies the internal frictional force between adjacent layers of fluid that are in relative motion. For instance, when a viscous fluid is forced through a tube, it flows more quickly near the tube's center line than near its walls. Experiments show that some stress (such as a pressure difference between the two ends of the tube) is needed to sustain the flow. This is because a force is required to overcome the friction between the layers of the fluid which are in relative motion. For a tube with a constant rate of flow, the strength of the compensating force is proportional to the fluid's viscosity.\nIn general, viscosity depends on a fluid's state, such as its temperature, pressure, and rate of deformation. However, the dependence on some of these properties is negligible in certain cases. For example, the viscosity of a Newtonian fluid does not vary significantly with the rate of deformation. \nZero viscosity (no resistance to shear stress) is observed only at very low temperatures in superfluids; otherwise, the second law of thermodynamics requires all fluids to have positive viscosity. A fluid that has zero viscosity (non-viscous) is called ideal or inviscid.\nFor non-Newtonian fluid's viscosity, there are pseudoplastic, plastic, and dilatant flows that are time-independent, and there are thixotropic and rheopectic flows that are time-dependent.\n\n\n== Etymology ==\nThe word \"viscosity\" is derived from the Latin viscum (\"mistletoe\"). Viscum also referred to a viscous glue derived from mistletoe berries.\n\n\n== Definitions ==\n\n\n=== Dynamic viscosity ===\n\nIn materials science and engineering, there is often interest in understanding the forces or stresses involved in the deformation of a material. For instance, if the material were a simple spring, the answer would be given by Hooke's law, which says that the force experienced by a spring is proportional to the distance displaced from equilibrium. Stresses which can be attributed to the deformation of a material from some rest state are called elastic stresses. In other materials, stresses are present which can be attributed to the deformation rate over time. These are called viscous stresses. For instance, in a fluid such as water the stresses which arise from shearing the fluid do not depend on the distance the fluid has been sheared; rather, they depend on how quickly the shearing occurs.\nViscosity is the material property which relates the viscous stresses in a material to the rate of change of a deformation (the strain rate). Although it applies to general flows, it is easy to visualize and define in a simple shearing flow, such as a planar Couette flow.\nIn the Couette flow, a fluid is trapped between two infinitely large plates, one fixed and one in parallel motion at constant speed \n  \n    \n      \n        u\n      \n    \n    {\\displaystyle u}\n  \n (see illustration to the right). If the speed of the top plate is low enough (to avoid turbulence), then in steady state the fluid particles move parallel to it, and their speed varies from \n  \n    \n      \n        0\n      \n    \n    {\\displaystyle 0}\n  \n at the bottom to \n  \n    \n      \n        u\n      \n    \n    {\\displaystyle u}\n  \n at the top. Each layer of fluid moves faster than the one just below it, and friction between them gives rise to a force resisting their relative motion. In particular, the fluid applies on the top plate a force in the direction opposite to its motion, and an equal but opposite force on the bottom plate. An external force is therefore required in order to keep the top plate moving at constant speed.\nIn many fluids, the flow velocity is observed to vary linearly from zero at the bottom to \n  \n    \n      \n        u\n      \n    \n    {\\displaystyle u}\n  \n at the top. Moreover, the magnitude of the force, \n  \n    \n      \n        F\n      \n    \n    {\\displaystyle F}\n  \n, acting on the top plate is found to be proportional to the speed \n  \n    \n      \n        u\n      \n    \n    {\\displaystyle u}\n  \n and the area \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n of each plate, and inversely proportional to their separation \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  \n: \n\n  \n    \n      \n        F\n        =\n        \u03bc\n        A\n        \n          \n            u\n            y\n          \n        \n        .\n      \n    \n    {\\displaystyle F=\\mu A{\\frac {u}{y}}.}\n  \n\nThe proportionality factor is the dynamic viscosity of the fluid, often simply referred to as the viscosity. It is denoted by the Greek letter mu (\u03bc). The dynamic viscosity has the dimensions \n  \n    \n      \n        \n          (\n          m\n          a\n          s\n          s\n          \n            /\n          \n          l\n          e\n          n\n          g\n          t\n          h\n          )\n          \n            /\n          \n          t\n          i\n          m\n          e\n        \n      \n    \n    {\\displaystyle \\mathrm {(mass/length)/time} }\n  \n, therefore resulting in the SI units and the derived units:\n\n  \n    \n      \n        [\n        \u03bc\n        ]\n        =\n        \n          \n            \n              k\n              g\n            \n            \n              m\n              \n                \u22c5\n              \n              s\n            \n          \n        \n        =\n        \n          \n            \n              N\n            \n            \n              \n                m\n                \n                  2\n                \n              \n            \n          \n        \n        \n          \u22c5\n        \n        \n          \n            s\n          \n        \n        =\n        \n          \n            P\n            a\n            \n              \u22c5\n            \n            s\n          \n        \n        =\n      \n    \n    {\\displaystyle [\\mu ]={\\frac {\\rm {kg}}{\\rm {m{\\cdot }s}}}={\\frac {\\rm {N}}{\\rm {m^{2}}}}{\\cdot }{\\rm {s}}={\\rm {Pa{\\cdot }s}}=}\n  \n pressure multiplied by time \n  \n    \n      \n        =\n      \n    \n    {\\displaystyle =}\n  \n energy per unit volume multiplied by time.\nThe aforementioned ratio \n  \n    \n      \n        u\n        \n          /\n        \n        y\n      \n    \n    {\\displaystyle u/y}\n  \n is called the rate of shear deformation or shear velocity, and is the derivative of the fluid speed in the direction parallel to the normal vector of the plates (see illustrations to the right). If the velocity does not vary linearly with \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  \n, then the appropriate generalization is:\n\n  \n    \n      \n        \u03c4\n        =\n        \u03bc\n        \n          \n            \n              \u2202\n              u\n            \n            \n              \u2202\n              y\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle \\tau =\\mu {\\frac {\\partial u}{\\partial y}},}\n  \n\nwhere \n  \n    \n      \n        \u03c4\n        =\n        F\n        \n          /\n        \n        A\n      \n    \n    {\\displaystyle \\tau =F/A}\n  \n, and \n  \n    \n      \n        \u2202\n        u\n        \n          /\n        \n        \u2202\n        y\n      \n    \n    {\\displaystyle \\partial u/\\partial y}\n  \n is the local shear velocity. This expression is referred to as Newton's law of viscosity. In shearing flows with planar symmetry, it is what defines \n  \n    \n      \n        \u03bc\n      \n    \n    {\\displaystyle \\mu }\n  \n. It is a special case of the general definition of viscosity (see below), which can be expressed in coordinate-free form.\nUse of the Greek letter mu (\n  \n    \n      \n        \u03bc\n      \n    \n    {\\displaystyle \\mu }\n  \n) for the dynamic viscosity (sometimes also called the absolute viscosity) is common among mechanical and chemical engineers, as well as mathematicians and physicists. However, the Greek letter eta (\n  \n    \n      \n        \u03b7\n      \n    \n    {\\displaystyle \\eta }\n  \n) is also used by chemists, physicists, and the IUPAC. The viscosity \n  \n    \n      \n        \u03bc\n      \n    \n    {\\displaystyle \\mu }\n  \n is sometimes also called the shear viscosity. However, at least one author discourages the use of this terminology, noting that \n  \n    \n      \n        \u03bc\n      \n    \n    {\\displaystyle \\mu }\n  \n can appear in non-shearing flows in addition to shearing flows.\n\n\n=== Kinematic viscosity ===\nIn fluid dynamics, it is sometimes more appropriate to work in terms of kinematic viscosity (sometimes also called the momentum diffusivity), defined as the ratio of the dynamic viscosity (\u03bc) over the density of the fluid (\u03c1). It is usually denoted by the Greek letter nu (\u03bd):\n\n  \n    \n      \n        \u03bd\n        =\n        \n          \n            \u03bc\n            \u03c1\n          \n        \n        ,\n      \n    \n    {\\displaystyle \\nu ={\\frac {\\mu }{\\rho }},}\n  \n\nand has the dimensions \n  \n    \n      \n        \n          (\n          l\n          e\n          n\n          g\n          t\n          h\n          \n            )\n            \n              2\n            \n          \n          \n            /\n          \n          t\n          i\n          m\n          e\n        \n      \n    \n    {\\displaystyle \\mathrm {(length)^{2}/time} }\n  \n, therefore resulting in the SI units and the derived units:\n\n  \n    \n      \n        [\n        \u03bd\n        ]\n        =\n        \n          \n            \n              m\n              \n                2\n              \n            \n            s\n          \n        \n        =\n        \n          \n            \n              \n                N\n                \n                  \u22c5\n                \n                m\n              \n              \n                k\n                g\n              \n            \n          \n          \n            \u22c5\n          \n          s\n        \n        =\n        \n          \n            \n              J\n              \n                k\n                g\n              \n            \n          \n          \n            \u22c5\n          \n          s\n        \n        =\n      \n    \n    {\\displaystyle [\\nu ]=\\mathrm {\\frac {m^{2}}{s}} =\\mathrm {{\\frac {N{\\cdot }m}{kg}}{\\cdot }s} =\\mathrm {{\\frac {J}{kg}}{\\cdot }s} =}\n  \n specific energy multiplied by time \n  \n    \n      \n        =\n      \n    \n    {\\displaystyle =}\n  \n energy per unit mass multiplied by time.\n\n\n=== General definition ===\n\nIn very general terms, the viscous stresses in a fluid are defined as those resulting from the relative velocity of different fluid particles. As such, the viscous stresses must depend on spatial gradients of the flow velocity. If the velocity gradients are small, then to a first approximation the viscous stresses depend only on the first derivatives of the velocity. (For Newtonian fluids, this is also a linear dependence.) In Cartesian coordinates, the general relationship can then be written as\n\n  \n    \n      \n        \n          \u03c4\n          \n            i\n            j\n          \n        \n        =\n        \n          \u2211\n          \n            k\n          \n        \n        \n          \u2211\n          \n            \u2113\n          \n        \n        \n          \u03bc\n          \n            i\n            j\n            k\n            \u2113\n          \n        \n        \n          \n            \n              \u2202\n              \n                v\n                \n                  k\n                \n              \n            \n            \n              \u2202\n              \n                r\n                \n                  \u2113\n                \n              \n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle \\tau _{ij}=\\sum _{k}\\sum _{\\ell }\\mu _{ijk\\ell }{\\frac {\\partial v_{k}}{\\partial r_{\\ell }}},}\n  \n\nwhere \n  \n    \n      \n        \n          \u03bc\n          \n            i\n            j\n            k\n            \u2113\n          \n        \n      \n    \n    {\\displaystyle \\mu _{ijk\\ell }}\n  \n is a viscosity tensor that maps the velocity gradient tensor \n  \n    \n      \n        \u2202\n        \n          v\n          \n            k\n          \n        \n        \n          /\n        \n        \u2202\n        \n          r\n          \n            \u2113\n          \n        \n      \n    \n    {\\displaystyle \\partial v_{k}/\\partial r_{\\ell }}\n  \n onto the viscous stress tensor \n  \n    \n      \n        \n          \u03c4\n          \n            i\n            j\n          \n        \n      \n    \n    {\\displaystyle \\tau _{ij}}\n  \n. Since the indices in this expression can vary from 1 to 3, there are 81 \"viscosity coefficients\" \n  \n    \n      \n        \n          \u03bc\n          \n            i\n            j\n            k\n            l\n          \n        \n      \n    \n    {\\displaystyle \\mu _{ijkl}}\n  \n in total. However, assuming that the viscosity rank-2 tensor is isotropic reduces these 81 coefficients to three independent parameters \n  \n    \n      \n        \u03b1\n      \n    \n    {\\displaystyle \\alpha }\n  \n, \n  \n    \n      \n        \u03b2\n      \n    \n    {\\displaystyle \\beta }\n  \n, \n  \n    \n      \n        \u03b3\n      \n    \n    {\\displaystyle \\gamma }\n  \n:\n\n  \n    \n      \n        \n          \u03bc\n          \n            i\n            j\n            k\n            \u2113\n          \n        \n        =\n        \u03b1\n        \n          \u03b4\n          \n            i\n            j\n          \n        \n        \n          \u03b4\n          \n            k\n            \u2113\n          \n        \n        +\n        \u03b2\n        \n          \u03b4\n          \n            i\n            k\n          \n        \n        \n          \u03b4\n          \n            j\n            \u2113\n          \n        \n        +\n        \u03b3\n        \n          \u03b4\n          \n            i\n            \u2113\n          \n        \n        \n          \u03b4\n          \n            j\n            k\n          \n        \n        ,\n      \n    \n    {\\displaystyle \\mu _{ijk\\ell }=\\alpha \\delta _{ij}\\delta _{k\\ell }+\\beta \\delta _{ik}\\delta _{j\\ell }+\\gamma \\delta _{i\\ell }\\delta _{jk},}\n  \n\nand furthermore, it is assumed that no viscous forces may arise when the fluid is undergoing simple rigid-body rotation, thus \n  \n    \n      \n        \u03b2\n        =\n        \u03b3\n      \n    \n    {\\displaystyle \\beta =\\gamma }\n  \n, leaving only two independent parameters. The most usual decomposition is in terms of the standard (scalar) viscosity \n  \n    \n      \n        \u03bc\n      \n    \n    {\\displaystyle \\mu }\n  \n and the bulk viscosity \n  \n    \n      \n        \u03ba\n      \n    \n    {\\displaystyle \\kappa }\n  \n such that \n  \n    \n      \n        \u03b1\n        =\n        \u03ba\n        \u2212\n        \n          \n            \n              2\n              3\n            \n          \n        \n        \u03bc\n      \n    \n    {\\displaystyle \\alpha =\\kappa -{\\tfrac {2}{3}}\\mu }\n  \n and \n  \n    \n      \n        \u03b2\n        =\n        \u03b3\n        =\n        \u03bc\n      \n    \n    {\\displaystyle \\beta =\\gamma =\\mu }\n  \n. In vector notation this appears as:\n\n  \n    \n      \n        \n          \u03c4\n        \n        =\n        \u03bc\n        \n          [\n          \n            \u2207\n            \n              v\n            \n            +\n            (\n            \u2207\n            \n              v\n            \n            \n              )\n              \n                \n                  T\n                \n              \n            \n          \n          ]\n        \n        \u2212\n        \n          (\n          \n            \n              \n                2\n                3\n              \n            \n            \u03bc\n            \u2212\n            \u03ba\n          \n          )\n        \n        (\n        \u2207\n        \u22c5\n        \n          v\n        \n        )\n        \n          \u03b4\n        \n        ,\n      \n    \n    {\\displaystyle {\\boldsymbol {\\tau }}=\\mu \\left[\\nabla \\mathbf {v} +(\\nabla \\mathbf {v} )^{\\mathrm {T} }\\right]-\\left({\\frac {2}{3}}\\mu -\\kappa \\right)(\\nabla \\cdot \\mathbf {v} )\\mathbf {\\delta } ,}\n  \n\nwhere \n  \n    \n      \n        \n          \u03b4\n        \n      \n    \n    {\\displaystyle \\mathbf {\\delta } }\n  \n is the unit tensor. This equation can be thought of as a generalized form of Newton's law of viscosity.\nThe bulk viscosity (also called volume viscosity) expresses a type of internal friction that resists the shearless compression or expansion of a fluid. Knowledge of \n  \n    \n      \n        \u03ba\n      \n    \n    {\\displaystyle \\kappa }\n  \n is frequently not necessary in fluid dynamics problems. For example, an incompressible fluid satisfies \n  \n    \n      \n        \u2207\n        \u22c5\n        \n          v\n        \n        =\n        0\n      \n    \n    {\\displaystyle \\nabla \\cdot \\mathbf {v} =0}\n  \n and so the term containing \n  \n    \n      \n        \u03ba\n      \n    \n    {\\displaystyle \\kappa }\n  \n drops out. Moreover, \n  \n    \n      \n        \u03ba\n      \n    \n    {\\displaystyle \\kappa }\n  \n is often assumed to be negligible for gases since it is \n  \n    \n      \n        0\n      \n    \n    {\\displaystyle 0}\n  \n in a monatomic ideal gas. One situation in which \n  \n    \n      \n        \u03ba\n      \n    \n    {\\displaystyle \\kappa }\n  \n can be important is the calculation of energy loss in sound and shock waves, described by Stokes' law of sound attenuation, since these phenomena involve rapid expansions and compressions.\nThe defining equations for viscosity are not fundamental laws of nature, so their usefulness, as well as methods for measuring or calculating the viscosity, must be established using separate means. A potential issue is that viscosity depends, in principle, on the full microscopic state of the fluid, which encompasses the positions and momenta of every particle in the system. Such highly detailed information is typically not available in realistic systems. However, under certain conditions most of this information can be shown to be negligible. In particular, for Newtonian fluids near equilibrium and far from boundaries (bulk state), the viscosity depends only space- and time-dependent macroscopic fields (such as temperature and density) defining local equilibrium.\nNevertheless, viscosity may still carry a non-negligible dependence on several system properties, such as temperature, pressure, and the amplitude and frequency of any external forcing. Therefore, precision measurements of viscosity are only defined\nwith respect to a specific fluid state. To standardize comparisons among experiments and theoretical models, viscosity data is sometimes extrapolated to ideal limiting cases, such as the zero shear limit, or (for gases) the zero density limit.\n\n\n== Momentum transport ==\n\nTransport theory provides an alternative interpretation of viscosity in terms of momentum transport: viscosity is the material property which characterizes momentum transport within a fluid, just as thermal conductivity characterizes heat transport, and (mass) diffusivity characterizes mass transport. This perspective is implicit in Newton's law of viscosity, \n  \n    \n      \n        \u03c4\n        =\n        \u03bc\n        (\n        \u2202\n        u\n        \n          /\n        \n        \u2202\n        y\n        )\n      \n    \n    {\\displaystyle \\tau =\\mu (\\partial u/\\partial y)}\n  \n, because the shear stress \n  \n    \n      \n        \u03c4\n      \n    \n    {\\displaystyle \\tau }\n  \n has units equivalent to a momentum flux, i.e., momentum per unit time per unit area. Thus, \n  \n    \n      \n        \u03c4\n      \n    \n    {\\displaystyle \\tau }\n  \n can be interpreted as specifying the flow of momentum in the \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  \n direction from one fluid layer to the next. Per Newton's law of viscosity, this momentum flow occurs across a velocity gradient, and the magnitude of the corresponding momentum flux is determined by the viscosity.\nThe analogy with heat and mass transfer can be made explicit. Just as heat flows from high temperature to low temperature and mass flows from high density to low density, momentum flows from high velocity to low velocity. These behaviors are all described by compact expressions, called constitutive relations, whose one-dimensional forms are given here:\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  J\n                \n              \n              \n                \n                =\n                \u2212\n                D\n                \n                  \n                    \n                      \u2202\n                      \u03c1\n                    \n                    \n                      \u2202\n                      x\n                    \n                  \n                \n              \n              \n              \n                \n                  (Fick's law of diffusion)\n                \n              \n            \n            \n              \n                \n                  q\n                \n              \n              \n                \n                =\n                \u2212\n                \n                  k\n                  \n                    t\n                  \n                \n                \n                  \n                    \n                      \u2202\n                      T\n                    \n                    \n                      \u2202\n                      x\n                    \n                  \n                \n              \n              \n              \n                \n                  (Fourier's law of heat conduction)\n                \n              \n            \n            \n              \n                \u03c4\n              \n              \n                \n                =\n                \u03bc\n                \n                  \n                    \n                      \u2202\n                      u\n                    \n                    \n                      \u2202\n                      y\n                    \n                  \n                \n              \n              \n              \n                \n                  (Newton's law of viscosity)\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\mathbf {J} &=-D{\\frac {\\partial \\rho }{\\partial x}}&&{\\text{(Fick's law of diffusion)}}\\\\[5pt]\\mathbf {q} &=-k_{t}{\\frac {\\partial T}{\\partial x}}&&{\\text{(Fourier's law of heat conduction)}}\\\\[5pt]\\tau &=\\mu {\\frac {\\partial u}{\\partial y}}&&{\\text{(Newton's law of viscosity)}}\\end{aligned}}}\n  \n\nwhere \n  \n    \n      \n        \u03c1\n      \n    \n    {\\displaystyle \\rho }\n  \n is the density, \n  \n    \n      \n        \n          J\n        \n      \n    \n    {\\displaystyle \\mathbf {J} }\n  \n and \n  \n    \n      \n        \n          q\n        \n      \n    \n    {\\displaystyle \\mathbf {q} }\n  \n are the mass and heat fluxes, and \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  \n and \n  \n    \n      \n        \n          k\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle k_{t}}\n  \n are the mass diffusivity and thermal conductivity. The fact that mass, momentum, and energy (heat) transport are among the most relevant processes in continuum mechanics is not a coincidence: these are among the few physical quantities that are conserved at the microscopic level in interparticle collisions. Thus, rather than being dictated by the fast and complex microscopic interaction timescale, their dynamics occurs on macroscopic timescales, as described by the various equations of transport theory and hydrodynamics.\n\n\n== Newtonian and non-Newtonian fluids ==\n\nNewton's law of viscosity is not a fundamental law of nature, but rather a constitutive equation (like Hooke's law, Fick's law, and Ohm's law) which serves to define the viscosity \n  \n    \n      \n        \u03bc\n      \n    \n    {\\displaystyle \\mu }\n  \n. Its form is motivated by experiments which show that for a wide range of fluids, \n  \n    \n      \n        \u03bc\n      \n    \n    {\\displaystyle \\mu }\n  \n is independent of strain rate. Such fluids are called Newtonian. Gases, water, and many common liquids can be considered Newtonian in ordinary conditions and contexts. However, there are many non-Newtonian fluids that significantly deviate from this behavior. For example:\n\nShear-thickening (dilatant) liquids, whose viscosity increases with the rate of shear strain.\nShear-thinning liquids, whose viscosity decreases with the rate of shear strain.\nThixotropic liquids, that become less viscous over time when shaken, agitated, or otherwise stressed.\nRheopectic liquids, that become more viscous over time when shaken, agitated, or otherwise stressed.\nBingham plastics that behave as a solid at low stresses but flow as a viscous fluid at high stresses.\nTrouton's ratio is the ratio of extensional viscosity to shear viscosity. For a Newtonian fluid, the Trouton ratio is 3. Shear-thinning liquids are very commonly, but misleadingly, described as thixotropic.\nViscosity may also depend on the fluid's physical state (temperature and pressure) and other, external, factors. For gases and other compressible fluids, it depends on temperature and varies very slowly with pressure. The viscosity of some fluids may depend on other factors. A magnetorheological fluid, for example, becomes thicker when subjected to a magnetic field, possibly to the point of behaving like a solid.\n\n\n== In solids ==\nThe viscous forces that arise during fluid flow are distinct from the elastic forces that occur in a solid in response to shear, compression, or extension stresses. While in the latter the stress is proportional to the amount of shear deformation, in a fluid it is proportional to the rate of deformation over time. For this reason, James Clerk Maxwell used the term fugitive elasticity for fluid viscosity.\nHowever, many liquids (including water) will briefly react like elastic solids when subjected to sudden stress. Conversely, many \"solids\" (even granite) will flow like liquids, albeit very slowly, even under arbitrarily small stress. Such materials are best described as viscoelastic\u2014that is, possessing both elasticity (reaction to deformation) and viscosity (reaction to rate of deformation).\nViscoelastic solids may exhibit both shear viscosity and bulk viscosity. The extensional viscosity is a linear combination of the shear and bulk viscosities that describes the reaction of a solid elastic material to elongation. It is widely used for characterizing polymers.\nIn geology, earth materials that exhibit viscous deformation at least three orders of magnitude greater than their elastic deformation are sometimes called rheids.\n\n\n== Measurement ==\n\nViscosity is measured with various types of viscometers and rheometers. Close temperature control of the fluid is essential to obtain accurate measurements, particularly in materials like lubricants, whose viscosity can double with a change of only 5 \u00b0C. A rheometer is used for fluids that cannot be defined by a single value of viscosity and therefore require more parameters to be set and measured than is the case for a viscometer.\nFor some fluids, the viscosity is constant over a wide range of shear rates (Newtonian fluids). The fluids without a constant viscosity (non-Newtonian fluids) cannot be described by a single number. Non-Newtonian fluids exhibit a variety of different correlations between shear stress and shear rate.\nOne of the most common instruments for measuring kinematic viscosity is the glass capillary viscometer.\nIn coating industries, viscosity may be measured with a cup in which the efflux time is measured. There are several sorts of cup\u2014such as the Zahn cup and the Ford viscosity cup\u2014with the usage of each type varying mainly according to the industry.\nAlso used in coatings, a Stormer viscometer employs load-based rotation to determine viscosity. The viscosity is reported in Krebs units (KU), which are unique to Stormer viscometers.\nVibrating viscometers can also be used to measure viscosity. Resonant, or vibrational viscometers work by creating shear waves within the liquid. In this method, the sensor is submerged in the fluid and is made to resonate at a specific frequency. As the surface of the sensor shears through the liquid, energy is lost due to its viscosity. This dissipated energy is then measured and converted into a viscosity reading. A higher viscosity causes a greater loss of energy.\nExtensional viscosity can be measured with various rheometers that apply extensional stress.\nVolume viscosity can be measured with an acoustic rheometer.\nApparent viscosity is a calculation derived from tests performed on drilling fluid used in oil or gas well development. These calculations and tests help engineers develop and maintain the properties of the drilling fluid to the specifications required.\nNanoviscosity (viscosity sensed by nanoprobes) can be measured by fluorescence correlation spectroscopy.\n\n\n== Units ==\nThe SI unit of dynamic viscosity is the newton-second per square meter (N\u00b7s/m2), also frequently expressed in the equivalent forms pascal-second (Pa\u00b7s), kilogram per meter per second (kg\u00b7m\u22121\u00b7s\u22121) and poiseuille (Pl). The CGS unit is the poise (P, or g\u00b7cm\u22121\u00b7s\u22121 = 0.1 Pa\u00b7s), named after Jean L\u00e9onard Marie Poiseuille. It is commonly expressed, particularly in ASTM standards, as centipoise (cP). The centipoise is convenient because the viscosity of water at 20 \u00b0C is about 1 cP, and one centipoise is equal to the SI millipascal second (mPa\u00b7s).\nThe SI unit of kinematic viscosity is square meter per second (m2/s), whereas the CGS unit for kinematic viscosity is the stokes (St, or cm2\u00b7s\u22121 = 0.0001 m2\u00b7s\u22121), named after Sir George Gabriel Stokes. In U.S. usage, stoke is sometimes used as the singular form. The submultiple centistokes (cSt) is often used instead, 1 cSt = 1 mm2\u00b7s\u22121 = 10\u22126 m2\u00b7s\u22121. 1 cSt is 1 cP divided by 1000 kg/m^3, close to the density of water. The kinematic viscosity of water at 20 \u00b0C is about 1 cSt.\nThe most frequently used systems of US customary, or Imperial, units are the British Gravitational (BG) and English Engineering (EE). In the BG system, dynamic viscosity has units of pound-seconds per square foot (lb\u00b7s/ft2), and in the EE system it has units of pound-force-seconds per square foot (lbf\u00b7s/ft2). The pound and pound-force are equivalent; the two systems differ only in how force and mass are defined. In the BG system the pound is a basic unit from which the unit of mass (the slug) is defined by Newton's Second Law, whereas in the EE system the units of force and mass (the pound-force and pound-mass respectively) are defined independently through the Second Law using the proportionality constant gc.\nKinematic viscosity has units of square feet per second (ft2/s) in both the BG and EE systems.\nNonstandard units include the reyn (lbf\u00b7s/in2), a British unit of dynamic viscosity. In the automotive industry the viscosity index is used to describe the change of viscosity with temperature.\nThe reciprocal of viscosity is fluidity, usually symbolized by \n  \n    \n      \n        \u03d5\n        =\n        1\n        \n          /\n        \n        \u03bc\n      \n    \n    {\\displaystyle \\phi =1/\\mu }\n  \n or \n  \n    \n      \n        F\n        =\n        1\n        \n          /\n        \n        \u03bc\n      \n    \n    {\\displaystyle F=1/\\mu }\n  \n, depending on the convention used, measured in reciprocal poise (P\u22121, or cm\u00b7s\u00b7g\u22121), sometimes called the rhe. Fluidity is seldom used in engineering practice.\nAt one time the petroleum industry relied on measuring kinematic viscosity by means of the Saybolt viscometer, and expressing kinematic viscosity in units of Saybolt universal seconds (SUS). Other abbreviations such as SSU (Saybolt seconds universal) or SUV (Saybolt universal viscosity) are sometimes used. Kinematic viscosity in centistokes can be converted from SUS according to the arithmetic and the reference table provided in ASTM D 2161.\n\n\n== Molecular origins ==\nMomentum transport in gases is mediated by discrete molecular collisions, and in liquids by attractive forces that bind molecules close together. Because of this, the dynamic viscosities of liquids are typically much larger than those of gases. In addition, viscosity tends to increase with temperature in gases and decrease with temperature in liquids.\nAbove the liquid-gas critical point, the liquid and gas phases are replaced by a single supercritical phase. In this regime, the mechanisms of momentum transport interpolate between liquid-like and gas-like behavior.\nFor example, along a supercritical isobar (constant-pressure surface), the kinematic viscosity decreases at low temperature and increases at high temperature, with a minimum in between. A rough estimate for the value\nat the minimum is\n\n  \n    \n      \n        \n          \u03bd\n          \n            min\n          \n        \n        =\n        \n          \n            1\n            \n              4\n              \u03c0\n            \n          \n        \n        \n          \n            \u210f\n            \n              \n                m\n                \n                  e\n                \n              \n              m\n            \n          \n        \n      \n    \n    {\\displaystyle \\nu _{\\text{min}}={\\frac {1}{4\\pi }}{\\frac {\\hbar }{\\sqrt {m_{\\text{e}}m}}}}\n  \n\nwhere \n  \n    \n      \n        \u210f\n      \n    \n    {\\displaystyle \\hbar }\n  \n is the Planck constant, \n  \n    \n      \n        \n          m\n          \n            e\n          \n        \n      \n    \n    {\\displaystyle m_{\\text{e}}}\n  \n is the electron mass, and \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  \n is the molecular mass.\nIn general, however, the viscosity of a system depends in detail on how the molecules constituting the system interact, and there are no simple but correct formulas for it. The simplest exact expressions are the Green\u2013Kubo relations for the linear shear viscosity or the transient time correlation function expressions derived by Evans and Morriss in 1988. Although these expressions are each exact, calculating the viscosity of a dense fluid using these relations currently requires the use of molecular dynamics computer simulations. Somewhat more progress can be made for a dilute gas, as elementary assumptions about how gas molecules move and interact lead to a basic understanding of the molecular origins of viscosity. More sophisticated treatments can be constructed by systematically coarse-graining the equations of motion of the gas molecules. An example of such a treatment is Chapman\u2013Enskog theory, which derives expressions for the viscosity of a dilute gas from the Boltzmann equation.\n\n\n=== Pure gases ===\n\nViscosity in gases arises principally from the molecular diffusion that transports momentum between layers of flow. An elementary calculation for a dilute gas at temperature \n  \n    \n      \n        T\n      \n    \n    {\\displaystyle T}\n  \n and density \n  \n    \n      \n        \u03c1\n      \n    \n    {\\displaystyle \\rho }\n  \n gives\n\n  \n    \n      \n        \u03bc\n        =\n        \u03b1\n        \u03c1\n        \u03bb\n        \n          \n            \n              \n                2\n                \n                  k\n                  \n                    B\n                  \n                \n                T\n              \n              \n                \u03c0\n                m\n              \n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle \\mu =\\alpha \\rho \\lambda {\\sqrt {\\frac {2k_{\\text{B}}T}{\\pi m}}},}\n  \n\nwhere \n  \n    \n      \n        \n          k\n          \n            B\n          \n        \n      \n    \n    {\\displaystyle k_{\\text{B}}}\n  \n is the Boltzmann constant, \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  \n the molecular mass, and \n  \n    \n      \n        \u03b1\n      \n    \n    {\\displaystyle \\alpha }\n  \n a numerical constant on the order of \n  \n    \n      \n        1\n      \n    \n    {\\displaystyle 1}\n  \n. The quantity \n  \n    \n      \n        \u03bb\n      \n    \n    {\\displaystyle \\lambda }\n  \n, the mean free path, measures the average distance a molecule travels between collisions. Even without a priori knowledge of \n  \n    \n      \n        \u03b1\n      \n    \n    {\\displaystyle \\alpha }\n  \n, this expression has nontrivial implications. In particular, since \n  \n    \n      \n        \u03bb\n      \n    \n    {\\displaystyle \\lambda }\n  \n is typically inversely proportional to density and increases with temperature, \n  \n    \n      \n        \u03bc\n      \n    \n    {\\displaystyle \\mu }\n  \n itself should increase with temperature and be independent of density at fixed temperature. In fact, both of these predictions persist in more sophisticated treatments, and accurately describe experimental observations. By contrast, liquid viscosity typically decreases with temperature.\nFor rigid elastic spheres of diameter \n  \n    \n      \n        \u03c3\n      \n    \n    {\\displaystyle \\sigma }\n  \n, \n  \n    \n      \n        \u03bb\n      \n    \n    {\\displaystyle \\lambda }\n  \n can be computed, giving\n\n  \n    \n      \n        \u03bc\n        =\n        \n          \n            \u03b1\n            \n              \u03c0\n              \n                3\n                \n                  /\n                \n                2\n              \n            \n          \n        \n        \n          \n            \n              \n                k\n                \n                  B\n                \n              \n              m\n              T\n            \n            \n              \u03c3\n              \n                2\n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\mu ={\\frac {\\alpha }{\\pi ^{3/2}}}{\\frac {\\sqrt {k_{\\text{B}}mT}}{\\sigma ^{2}}}.}\n  \n\nIn this case \n  \n    \n      \n        \u03bb\n      \n    \n    {\\displaystyle \\lambda }\n  \n is independent of temperature, so \n  \n    \n      \n        \u03bc\n        \u221d\n        \n          T\n          \n            1\n            \n              /\n            \n            2\n          \n        \n      \n    \n    {\\displaystyle \\mu \\propto T^{1/2}}\n  \n. For more complicated molecular models, however, \n  \n    \n      \n        \u03bb\n      \n    \n    {\\displaystyle \\lambda }\n  \n depends on temperature in a non-trivial way, and simple kinetic arguments as used here are inadequate. More fundamentally, the notion of a mean free path becomes imprecise for particles that interact over a finite range, which limits the usefulness of the concept for describing real-world gases.\n\n\n==== Chapman\u2013Enskog theory ====\n\nA technique developed by Sydney Chapman and David Enskog in the early 1900s allows a more refined calculation of \n  \n    \n      \n        \u03bc\n      \n    \n    {\\displaystyle \\mu }\n  \n. It is based on the Boltzmann equation, which provides a statistical description of a dilute gas in terms of intermolecular interactions. The technique allows accurate calculation of \n  \n    \n      \n        \u03bc\n      \n    \n    {\\displaystyle \\mu }\n  \n for molecular models that are more realistic than rigid elastic spheres, such as those incorporating intermolecular attractions. Doing so is necessary to reproduce the correct temperature dependence of \n  \n    \n      \n        \u03bc\n      \n    \n    {\\displaystyle \\mu }\n  \n, which experiments show increases more rapidly than the \n  \n    \n      \n        \n          T\n          \n            1\n            \n              /\n            \n            2\n          \n        \n      \n    \n    {\\displaystyle T^{1/2}}\n  \n trend predicted for rigid elastic spheres. Indeed, the Chapman\u2013Enskog analysis shows that the predicted temperature dependence can be tuned by varying the parameters in various molecular models. A simple example is the Sutherland model, which describes rigid elastic spheres with weak mutual attraction. In such a case, the attractive force can be treated perturbatively, which leads to a simple expression for \n  \n    \n      \n        \u03bc\n      \n    \n    {\\displaystyle \\mu }\n  \n:\n\n  \n    \n      \n        \u03bc\n        =\n        \n          \n            5\n            \n              16\n              \n                \u03c3\n                \n                  2\n                \n              \n            \n          \n        \n        \n          \n            (\n            \n              \n                \n                  \n                    k\n                    \n                      B\n                    \n                  \n                  m\n                  T\n                \n                \u03c0\n              \n            \n            )\n          \n          \n            \n            \n            1\n            \n              /\n            \n            2\n          \n        \n         \n        \n          \n            (\n            \n              1\n              +\n              \n                \n                  S\n                  T\n                \n              \n            \n            )\n          \n          \n            \n            \n            \u2212\n            1\n          \n        \n        ,\n      \n    \n    {\\displaystyle \\mu ={\\frac {5}{16\\sigma ^{2}}}\\left({\\frac {k_{\\text{B}}mT}{\\pi }}\\right)^{\\!\\!1/2}\\ \\left(1+{\\frac {S}{T}}\\right)^{\\!\\!-1},}\n  \n\nwhere \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n is independent of temperature, being determined only by the parameters of the intermolecular attraction. To connect with experiment, it is convenient to rewrite as\n\n  \n    \n      \n        \u03bc\n        =\n        \n          \u03bc\n          \n            0\n          \n        \n        \n          \n            (\n            \n              \n                T\n                \n                  T\n                  \n                    0\n                  \n                \n              \n            \n            )\n          \n          \n            \n            \n            3\n            \n              /\n            \n            2\n          \n        \n         \n        \n          \n            \n              \n                T\n                \n                  0\n                \n              \n              +\n              S\n            \n            \n              T\n              +\n              S\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle \\mu =\\mu _{0}\\left({\\frac {T}{T_{0}}}\\right)^{\\!\\!3/2}\\ {\\frac {T_{0}+S}{T+S}},}\n  \n\nwhere \n  \n    \n      \n        \n          \u03bc\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\mu _{0}}\n  \n is the viscosity at temperature \n  \n    \n      \n        \n          T\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle T_{0}}\n  \n. This expression is usually named Sutherland's formula. If \n  \n    \n      \n        \u03bc\n      \n    \n    {\\displaystyle \\mu }\n  \n is known from experiments at \n  \n    \n      \n        T\n        =\n        \n          T\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle T=T_{0}}\n  \n and at least one other temperature, then \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n can be calculated. Expressions for \n  \n    \n      \n        \u03bc\n      \n    \n    {\\displaystyle \\mu }\n  \n obtained in this way are qualitatively accurate for a number of simple gases. Slightly more sophisticated models, such as the Lennard-Jones potential, or the more flexible Mie potential, may provide better agreement with experiments, but only at the cost of a more opaque dependence on temperature. A further advantage of these more complex interaction potentials is that they can be used to develop accurate models for a wide variety of properties using the same potential parameters. In situations where little experimental data is available, this makes it possible to obtain model parameters from fitting to properties such as pure-fluid vapour-liquid equilibria, before using the parameters thus obtained to predict the viscosities of interest with reasonable accuracy.\nIn some systems, the assumption of spherical symmetry must be abandoned, as is the case for vapors with highly polar molecules like H2O. In these cases, the Chapman\u2013Enskog analysis is significantly more complicated.\n\n\n==== Bulk viscosity ====\n\nIn the kinetic-molecular picture, a non-zero bulk viscosity arises in gases whenever there are non-negligible relaxational timescales governing the exchange of energy between the translational energy of molecules and their internal energy, e.g. rotational and vibrational. As such, the bulk viscosity is \n  \n    \n      \n        0\n      \n    \n    {\\displaystyle 0}\n  \n for a monatomic ideal gas, in which the internal energy of molecules is negligible, but is nonzero for a gas like carbon dioxide, whose molecules possess both rotational and vibrational energy.\n\n\n=== Pure liquids ===\n\nIn contrast with gases, there is no simple yet accurate picture for the molecular origins of viscosity in liquids.\nAt the simplest level of description, the relative motion of adjacent layers in a liquid is opposed primarily by attractive molecular forces\nacting across the layer boundary. In this picture, one (correctly) expects viscosity to decrease with increasing temperature. This is because\nincreasing temperature increases the random thermal motion of the molecules, which makes it easier for them to overcome their attractive interactions.\nBuilding on this visualization, a simple theory can be constructed in analogy with the discrete structure of a solid: groups of molecules in a liquid \nare visualized as forming \"cages\" which surround and enclose single molecules. These cages can be occupied or unoccupied, and\nstronger molecular attraction corresponds to stronger cages.\nDue to random thermal motion, a molecule \"hops\" between cages at a rate which varies inversely with the strength of molecular attractions. In equilibrium these \"hops\" are not biased in any direction.\nOn the other hand, in order for two adjacent layers to move relative to each other, the \"hops\" must be biased in the direction\nof the relative motion. The force required to sustain this directed motion can be estimated for a given shear rate, leading to\n\nwhere \n  \n    \n      \n        \n          N\n          \n            A\n          \n        \n      \n    \n    {\\displaystyle N_{\\text{A}}}\n  \n is the Avogadro constant, \n  \n    \n      \n        h\n      \n    \n    {\\displaystyle h}\n  \n is the Planck constant, \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n  \n is the volume of a mole of liquid, and \n  \n    \n      \n        \n          T\n          \n            b\n          \n        \n      \n    \n    {\\displaystyle T_{\\text{b}}}\n  \n is the normal boiling point. This result has the same form as the well-known empirical relation\n\nwhere \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n and \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n are constants fit from data. On the other hand, several authors express caution with respect to this model.\nErrors as large as 30% can be encountered using equation (1), compared with fitting equation (2) to experimental data. More fundamentally, the physical assumptions underlying equation (1) have been criticized. It has also been argued that the exponential dependence in equation (1) does not necessarily describe experimental observations more accurately than simpler, non-exponential expressions.\nIn light of these shortcomings, the development of a less ad hoc model is a matter of practical interest. Foregoing simplicity in favor of precision, it is possible to write rigorous expressions for viscosity starting from the fundamental equations of motion for molecules. A classic example of this approach is Irving\u2013Kirkwood theory. On the other hand, such expressions are given as averages over multiparticle correlation functions and are therefore difficult to apply in practice.\nIn general, empirically derived expressions (based on existing viscosity measurements) appear to be the only consistently reliable means of calculating viscosity in liquids.\nLocal atomic structure changes observed in undercooled liquids on cooling below the equilibrium melting temperature either in terms of radial distribution function g(r) or structure factor S(Q) are found to be directly responsible for the liquid fragility: deviation of the temperature dependence of viscosity of the undercooled liquid from the Arrhenius equation (2) through modification of the activation energy for viscous flow. At the same time equilibrium liquids follow the Arrhenius equation.\n\n\n=== Mixtures and blends ===\n\n\n==== Gaseous mixtures ====\nThe same molecular-kinetic picture of a single component gas can also be applied to a gaseous mixture. For instance, in the Chapman\u2013Enskog approach the viscosity \n  \n    \n      \n        \n          \u03bc\n          \n            mix\n          \n        \n      \n    \n    {\\displaystyle \\mu _{\\text{mix}}}\n  \n of a binary mixture of gases can be written in terms of the individual component viscosities \n  \n    \n      \n        \n          \u03bc\n          \n            1\n            ,\n            2\n          \n        \n      \n    \n    {\\displaystyle \\mu _{1,2}}\n  \n, their respective volume fractions, and the intermolecular interactions. \nAs for the single-component gas, the dependence of \n  \n    \n      \n        \n          \u03bc\n          \n            mix\n          \n        \n      \n    \n    {\\displaystyle \\mu _{\\text{mix}}}\n  \n on the parameters of the intermolecular interactions enters through various collisional integrals which may not be expressible in closed form. To obtain usable expressions for \n  \n    \n      \n        \n          \u03bc\n          \n            mix\n          \n        \n      \n    \n    {\\displaystyle \\mu _{\\text{mix}}}\n  \n which reasonably match experimental data, the collisional integrals may be computed numerically or from correlations. In some cases, the collision integrals are regarded as fitting parameters, and are fitted directly to experimental data. This is a common approach in the development of reference equations for gas-phase viscosities. An example of such a procedure is the Sutherland approach for the single-component gas, discussed above.\nFor gas mixtures consisting of simple molecules, Revised Enskog Theory has been shown to accurately represent both the density- and temperature dependence of the viscosity over a wide range of conditions.\n\n\n==== Blends of liquids ====\nAs for pure liquids, the viscosity of a blend of liquids is difficult to predict from molecular principles. One method is to extend the molecular \"cage\" theory presented above for a pure liquid. This can be done with varying levels of sophistication. One expression resulting from such an analysis is the Lederer\u2013Roegiers equation for a binary mixture:\n\n  \n    \n      \n        ln\n        \u2061\n        \n          \u03bc\n          \n            blend\n          \n        \n        =\n        \n          \n            \n              x\n              \n                1\n              \n            \n            \n              \n                x\n                \n                  1\n                \n              \n              +\n              \u03b1\n              \n                x\n                \n                  2\n                \n              \n            \n          \n        \n        ln\n        \u2061\n        \n          \u03bc\n          \n            1\n          \n        \n        +\n        \n          \n            \n              \u03b1\n              \n                x\n                \n                  2\n                \n              \n            \n            \n              \n                x\n                \n                  1\n                \n              \n              +\n              \u03b1\n              \n                x\n                \n                  2\n                \n              \n            \n          \n        \n        ln\n        \u2061\n        \n          \u03bc\n          \n            2\n          \n        \n        ,\n      \n    \n    {\\displaystyle \\ln \\mu _{\\text{blend}}={\\frac {x_{1}}{x_{1}+\\alpha x_{2}}}\\ln \\mu _{1}+{\\frac {\\alpha x_{2}}{x_{1}+\\alpha x_{2}}}\\ln \\mu _{2},}\n  \n\nwhere \n  \n    \n      \n        \u03b1\n      \n    \n    {\\displaystyle \\alpha }\n  \n is an empirical parameter, and \n  \n    \n      \n        \n          x\n          \n            1\n            ,\n            2\n          \n        \n      \n    \n    {\\displaystyle x_{1,2}}\n  \n and \n  \n    \n      \n        \n          \u03bc\n          \n            1\n            ,\n            2\n          \n        \n      \n    \n    {\\displaystyle \\mu _{1,2}}\n  \n are the respective mole fractions and viscosities of the component liquids.\nSince blending is an important process in the lubricating and oil industries, a variety of empirical and proprietary equations exist for predicting the viscosity of a blend.\n\n\n=== Solutions and suspensions ===\n\n\n==== Aqueous solutions ====\n\nDepending on the solute and range of concentration, an aqueous electrolyte solution can have either a larger or smaller viscosity compared with pure water at the same temperature and pressure. For instance, a 20% saline (sodium chloride) solution has viscosity over 1.5 times that of pure water, whereas a 20% potassium iodide solution has viscosity about 0.91 times that of pure water.\nAn idealized model of dilute electrolytic solutions leads to the following prediction for the viscosity \n  \n    \n      \n        \n          \u03bc\n          \n            s\n          \n        \n      \n    \n    {\\displaystyle \\mu _{s}}\n  \n of a solution:\n\n  \n    \n      \n        \n          \n            \n              \u03bc\n              \n                s\n              \n            \n            \n              \u03bc\n              \n                0\n              \n            \n          \n        \n        =\n        1\n        +\n        A\n        \n          \n            c\n          \n        \n        ,\n      \n    \n    {\\displaystyle {\\frac {\\mu _{s}}{\\mu _{0}}}=1+A{\\sqrt {c}},}\n  \n\nwhere \n  \n    \n      \n        \n          \u03bc\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\mu _{0}}\n  \n is the viscosity of the solvent, \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  \n is the concentration, and \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n is a positive constant which depends on both solvent and solute properties. However, this expression is only valid for very dilute solutions, having \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  \n less than 0.1 mol/L. For higher concentrations, additional terms are necessary which account for higher-order molecular correlations:\n\n  \n    \n      \n        \n          \n            \n              \u03bc\n              \n                s\n              \n            \n            \n              \u03bc\n              \n                0\n              \n            \n          \n        \n        =\n        1\n        +\n        A\n        \n          \n            c\n          \n        \n        +\n        B\n        c\n        +\n        C\n        \n          c\n          \n            2\n          \n        \n        ,\n      \n    \n    {\\displaystyle {\\frac {\\mu _{s}}{\\mu _{0}}}=1+A{\\sqrt {c}}+Bc+Cc^{2},}\n  \n\nwhere \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n and \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n are fit from data. In particular, a negative value of \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n is able to account for the decrease in viscosity observed in some solutions. Estimated values of these constants are shown below for sodium chloride and potassium iodide at temperature 25 \u00b0C (mol = mole, L = liter).\n\n\n==== Suspensions ====\nIn a suspension of solid particles (e.g. micron-size spheres suspended in oil), an effective viscosity \n  \n    \n      \n        \n          \u03bc\n          \n            eff\n          \n        \n      \n    \n    {\\displaystyle \\mu _{\\text{eff}}}\n  \n can be defined in terms of stress and strain components which are averaged over a volume large compared with the distance between the suspended particles, but small with respect to macroscopic dimensions. Such suspensions generally exhibit non-Newtonian behavior. However, for dilute systems in steady flows, the behavior is Newtonian and expressions for \n  \n    \n      \n        \n          \u03bc\n          \n            eff\n          \n        \n      \n    \n    {\\displaystyle \\mu _{\\text{eff}}}\n  \n can be derived directly from the particle dynamics. In a very dilute system, with volume fraction \n  \n    \n      \n        \u03d5\n        \u2272\n        0.02\n      \n    \n    {\\displaystyle \\phi \\lesssim 0.02}\n  \n, interactions between the suspended particles can be ignored. In such a case one can explicitly calculate the flow field around each particle independently, and combine the results to obtain \n  \n    \n      \n        \n          \u03bc\n          \n            eff\n          \n        \n      \n    \n    {\\displaystyle \\mu _{\\text{eff}}}\n  \n. For spheres, this results in the Einstein's effective viscosity formula:\n\n  \n    \n      \n        \n          \u03bc\n          \n            eff\n          \n        \n        =\n        \n          \u03bc\n          \n            0\n          \n        \n        \n          (\n          \n            1\n            +\n            \n              \n                5\n                2\n              \n            \n            \u03d5\n          \n          )\n        \n        ,\n      \n    \n    {\\displaystyle \\mu _{\\text{eff}}=\\mu _{0}\\left(1+{\\frac {5}{2}}\\phi \\right),}\n  \n\nwhere \n  \n    \n      \n        \n          \u03bc\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\mu _{0}}\n  \n is the viscosity of the suspending liquid. The linear dependence on \n  \n    \n      \n        \u03d5\n      \n    \n    {\\displaystyle \\phi }\n  \n is a consequence of neglecting interparticle interactions. For dilute systems in general, one expects \n  \n    \n      \n        \n          \u03bc\n          \n            eff\n          \n        \n      \n    \n    {\\displaystyle \\mu _{\\text{eff}}}\n  \n to take the form\n\n  \n    \n      \n        \n          \u03bc\n          \n            eff\n          \n        \n        =\n        \n          \u03bc\n          \n            0\n          \n        \n        \n          (\n          \n            1\n            +\n            B\n            \u03d5\n          \n          )\n        \n        ,\n      \n    \n    {\\displaystyle \\mu _{\\text{eff}}=\\mu _{0}\\left(1+B\\phi \\right),}\n  \n\nwhere the coefficient \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n may depend on the particle shape (e.g. spheres, rods, disks). Experimental determination of the precise value of \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n is difficult, however: even the prediction \n  \n    \n      \n        B\n        =\n        5\n        \n          /\n        \n        2\n      \n    \n    {\\displaystyle B=5/2}\n  \n for spheres has not been conclusively validated, with various experiments finding values in the range \n  \n    \n      \n        1.5\n        \u2272\n        B\n        \u2272\n        5\n      \n    \n    {\\displaystyle 1.5\\lesssim B\\lesssim 5}\n  \n. This deficiency has been attributed to difficulty in controlling experimental conditions.\nIn denser suspensions, \n  \n    \n      \n        \n          \u03bc\n          \n            eff\n          \n        \n      \n    \n    {\\displaystyle \\mu _{\\text{eff}}}\n  \n acquires a nonlinear dependence on \n  \n    \n      \n        \u03d5\n      \n    \n    {\\displaystyle \\phi }\n  \n, which indicates the importance of interparticle interactions. Various analytical and semi-empirical schemes exist for capturing this regime. At the most basic level, a term quadratic in \n  \n    \n      \n        \u03d5\n      \n    \n    {\\displaystyle \\phi }\n  \n is added to \n  \n    \n      \n        \n          \u03bc\n          \n            eff\n          \n        \n      \n    \n    {\\displaystyle \\mu _{\\text{eff}}}\n  \n:\n\n  \n    \n      \n        \n          \u03bc\n          \n            eff\n          \n        \n        =\n        \n          \u03bc\n          \n            0\n          \n        \n        \n          (\n          \n            1\n            +\n            B\n            \u03d5\n            +\n            \n              B\n              \n                1\n              \n            \n            \n              \u03d5\n              \n                2\n              \n            \n          \n          )\n        \n        ,\n      \n    \n    {\\displaystyle \\mu _{\\text{eff}}=\\mu _{0}\\left(1+B\\phi +B_{1}\\phi ^{2}\\right),}\n  \n\nand the coefficient \n  \n    \n      \n        \n          B\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle B_{1}}\n  \n is fit from experimental data or approximated from the microscopic theory. However, some authors advise caution in applying such simple formulas since non-Newtonian behavior appears in dense suspensions (\n  \n    \n      \n        \u03d5\n        \u2273\n        0.25\n      \n    \n    {\\displaystyle \\phi \\gtrsim 0.25}\n  \n for spheres), or in suspensions of elongated or flexible particles.\nThere is a distinction between a suspension of solid particles, described above, and an emulsion. The latter is a suspension of tiny droplets, which themselves may exhibit internal circulation. The presence of internal circulation can decrease the observed effective viscosity, and different theoretical or semi-empirical models must be used.\n\n\n=== Amorphous materials ===\n\nIn the high and low temperature limits, viscous flow in amorphous materials (e.g. in glasses and melts) has the Arrhenius form:\n\n  \n    \n      \n        \u03bc\n        =\n        A\n        \n          e\n          \n            Q\n            \n              /\n            \n            (\n            R\n            T\n            )\n          \n        \n        ,\n      \n    \n    {\\displaystyle \\mu =Ae^{Q/(RT)},}\n  \n\nwhere Q is a relevant activation energy, given in terms of molecular parameters; T is temperature; R is the molar gas constant; and A is approximately a constant. The activation energy Q takes a different value depending on whether the high or low temperature limit is being considered: it changes from a high value QH at low temperatures (in the glassy state) to a low value QL at high temperatures (in the liquid state).\n\nFor intermediate temperatures, \n  \n    \n      \n        Q\n      \n    \n    {\\displaystyle Q}\n  \n varies nontrivially with temperature and the simple Arrhenius form fails. On the other hand, the two-exponential equation\n\n  \n    \n      \n        \u03bc\n        =\n        A\n        T\n        exp\n        \u2061\n        \n          (\n          \n            \n              B\n              \n                R\n                T\n              \n            \n          \n          )\n        \n        \n          [\n          \n            1\n            +\n            C\n            exp\n            \u2061\n            \n              (\n              \n                \n                  D\n                  \n                    R\n                    T\n                  \n                \n              \n              )\n            \n          \n          ]\n        \n        ,\n      \n    \n    {\\displaystyle \\mu =AT\\exp \\left({\\frac {B}{RT}}\\right)\\left[1+C\\exp \\left({\\frac {D}{RT}}\\right)\\right],}\n  \n\nwhere \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n, \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n, \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n, \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  \n are all constants, provides a good fit to experimental data over the entire range of temperatures, while at the same time reducing to the correct Arrhenius form in the low and high temperature limits. This expression, also known as Duouglas-Doremus-Ojovan model , can be motivated from various theoretical models of amorphous materials at the atomic level.\nA two-exponential equation for the viscosity can be derived within the Dyre shoving model of supercooled liquids, where the Arrhenius energy barrier is identified with the high-frequency shear modulus times a characteristic shoving volume. Upon specifying the temperature dependence of the shear modulus via thermal expansion and via the repulsive part of the intermolecular potential, another two-exponential equation is retrieved:\n\n  \n    \n      \n        \u03bc\n        =\n        exp\n        \u2061\n        \n          \n            {\n            \n              \n                \n                  \n                    \n                      V\n                      \n                        c\n                      \n                    \n                    \n                      C\n                      \n                        G\n                      \n                    \n                  \n                  \n                    \n                      k\n                      \n                        B\n                      \n                    \n                    T\n                  \n                \n              \n              exp\n              \u2061\n              \n                \n                  [\n                  \n                    (\n                    2\n                    +\n                    \u03bb\n                    )\n                    \n                      \u03b1\n                      \n                        T\n                      \n                    \n                    \n                      T\n                      \n                        g\n                      \n                    \n                    \n                      (\n                      \n                        1\n                        \u2212\n                        \n                          \n                            T\n                            \n                              T\n                              \n                                g\n                              \n                            \n                          \n                        \n                      \n                      )\n                    \n                  \n                  ]\n                \n              \n            \n            }\n          \n        \n      \n    \n    {\\displaystyle \\mu =\\exp {\\left\\{{\\frac {V_{c}C_{G}}{k_{B}T}}\\exp {\\left[(2+\\lambda )\\alpha _{T}T_{g}\\left(1-{\\frac {T}{T_{g}}}\\right)\\right]}\\right\\}}}\n  \n\nwhere \n  \n    \n      \n        \n          C\n          \n            G\n          \n        \n      \n    \n    {\\displaystyle C_{G}}\n  \n denotes the high-frequency shear modulus of the material evaluated at a temperature equal to the glass transition temperature \n  \n    \n      \n        \n          T\n          \n            g\n          \n        \n      \n    \n    {\\displaystyle T_{g}}\n  \n, \n  \n    \n      \n        \n          V\n          \n            c\n          \n        \n      \n    \n    {\\displaystyle V_{c}}\n  \n is the so-called shoving volume, i.e. it is the characteristic volume of the group of atoms involved in the shoving event by which an atom/molecule escapes from the cage of nearest-neighbours, typically on the order of the volume occupied by few atoms. Furthermore, \n  \n    \n      \n        \n          \u03b1\n          \n            T\n          \n        \n      \n    \n    {\\displaystyle \\alpha _{T}}\n  \n is the thermal expansion coefficient of the material, \n  \n    \n      \n        \u03bb\n      \n    \n    {\\displaystyle \\lambda }\n  \n is a parameter which measures the steepness of the power-law rise of the ascending flank of the first peak of the radial distribution function, and is quantitatively related to the repulsive part of the interatomic potential. Finally, \n  \n    \n      \n        \n          k\n          \n            B\n          \n        \n      \n    \n    {\\displaystyle k_{B}}\n  \n denotes the Boltzmann constant.\n\n\n=== Eddy viscosity ===\nIn the study of turbulence in fluids, a common practical strategy is to ignore the small-scale vortices (or eddies) in the motion and to calculate a large-scale motion with an effective viscosity, called the \"eddy viscosity\", which characterizes the transport and dissipation of energy in the smaller-scale flow (see large eddy simulation). In contrast to the viscosity of the fluid itself, which must be positive by the second law of thermodynamics, the eddy viscosity can be negative.\n\n\n== Prediction ==\nBecause viscosity depends continuously on temperature and pressure, it cannot be fully characterized by a finite number of experimental measurements. Predictive formulas become necessary if experimental values are not available at the temperatures and pressures of interest. This capability is important for thermophysical simulations, \nin which the temperature and pressure of a fluid can vary continuously with space and time. A similar situation is encountered for mixtures of pure fluids, where the viscosity depends continuously on the concentration ratios of the constituent fluids\nFor the simplest fluids, such as dilute monatomic gases and their mixtures, ab initio quantum mechanical computations can accurately predict viscosity in terms of fundamental atomic constants, i.e., without reference to existing viscosity measurements. For the special case of dilute helium, uncertainties in the ab initio calculated viscosity are two order of magnitudes smaller than uncertainties in experimental values.\nFor slightly more complex fluids and mixtures at moderate densities (i.e. sub-critical densities) Revised Enskog Theory can be used to predict viscosities with some accuracy. Revised Enskog Theory is predictive in the sense that predictions for viscosity can be obtained using parameters fitted to other, pure-fluid thermodynamic properties or transport properties, thus requiring no a priori experimental viscosity measurements.\nFor most fluids, high-accuracy, first-principles computations are not feasible. Rather, theoretical or empirical expressions must be fit to existing viscosity measurements. If such an expression is fit to high-fidelity data over a large range of temperatures and pressures, then it is called a \"reference correlation\" for that fluid. Reference correlations have been published for many pure fluids; a few examples are water, carbon dioxide, ammonia, benzene, and xenon. Many of these cover temperature and pressure ranges that encompass gas, liquid, and supercritical phases.\nThermophysical modeling software often relies on reference correlations for predicting viscosity at user-specified temperature and pressure.\nThese correlations may be proprietary. Examples are REFPROP (proprietary) and CoolProp\n(open-source).\nViscosity can also be computed using formulas that express it in terms of the statistics of individual particle\ntrajectories. These formulas include the Green\u2013Kubo relations for the linear shear viscosity and the transient time correlation function expressions derived by Evans and Morriss in 1988. \nThe advantage of these expressions is that they are formally exact and valid for general systems. The disadvantage is that they require detailed knowledge of particle trajectories, available only in computationally expensive simulations such as molecular dynamics. \nAn accurate model for interparticle interactions is also required, which may be difficult to obtain for complex molecules.\n\n\n== Selected substances ==\n\nObserved values of viscosity vary over several orders of magnitude, even for common substances (see the order of magnitude table below). For instance, a 70% sucrose (sugar) solution has a viscosity over 400 times that of water, and 26,000 times that of air. More dramatically, pitch has been estimated to have a viscosity 230 billion times that of water.\n\n\n=== Water ===\nThe dynamic viscosity \n  \n    \n      \n        \u03bc\n      \n    \n    {\\displaystyle \\mu }\n  \n of water is about 0.89 mPa\u00b7s at room temperature (25 \u00b0C). As a function of temperature in kelvins, the viscosity can be estimated using the semi-empirical Vogel-Fulcher-Tammann equation:\n\n  \n    \n      \n        \u03bc\n        =\n        A\n        exp\n        \u2061\n        \n          (\n          \n            \n              B\n              \n                T\n                \u2212\n                C\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\mu =A\\exp \\left({\\frac {B}{T-C}}\\right)}\n  \n\nwhere A = 0.02939 mPa\u00b7s, B = 507.88 K, and C = 149.3 K. Experimentally determined values of the viscosity are also given in the table below. The values at 20 \u00b0C are a useful reference: there, the dynamic viscosity is about 1 cP and the kinematic viscosity is about 1 cSt.\n\n\n=== Air ===\nUnder standard atmospheric conditions (25 \u00b0C and pressure of 1 bar), the dynamic viscosity of air is 18.5 \u03bcPa\u00b7s, roughly 50 times smaller than the viscosity of water at the same temperature. Except at very high pressure, the viscosity of air depends mostly on the temperature. Among the many possible approximate formulas for the temperature dependence (see Temperature dependence of viscosity), one is:\n\n  \n    \n      \n        \n          \u03b7\n          \n            air\n          \n        \n        =\n        2.791\n        \u00d7\n        \n          10\n          \n            \u2212\n            7\n          \n        \n        \u00d7\n        \n          T\n          \n            0.7355\n          \n        \n      \n    \n    {\\displaystyle \\eta _{\\text{air}}=2.791\\times 10^{-7}\\times T^{0.7355}}\n  \n\nwhich is accurate in the range \u221220 \u00b0C to 400 \u00b0C. For this formula to be valid, the temperature must be given in kelvins; \n  \n    \n      \n        \n          \u03b7\n          \n            air\n          \n        \n      \n    \n    {\\displaystyle \\eta _{\\text{air}}}\n  \n then corresponds to the viscosity in Pa\u00b7s.\n\n\n=== Other common substances ===\n\n\n=== Order of magnitude estimates ===\nThe following table illustrates the range of viscosity values observed in common substances. Unless otherwise noted, a temperature of 25 \u00b0C and a pressure of 1 atmosphere are assumed.\nThe values listed are representative estimates only, as they do not account for measurement uncertainties, variability in material definitions, or non-Newtonian behavior.\n\n\n== See also ==\n\n\n== References ==\n\n\n=== Footnotes ===\n\n\n=== Citations ===\n\n\n=== Sources ===\n\n\n== External links ==\n\nViscosity - The Feynman Lectures on Physics\nFluid properties \u2013 high accuracy calculation of viscosity for frequently encountered pure liquids and gases\nFluid Characteristics Chart \u2013 a table of viscosities and vapor pressures for various fluids\nGas Dynamics Toolbox \u2013 calculate coefficient of viscosity for mixtures of gases\nGlass Viscosity Measurement \u2013 viscosity measurement, viscosity units and fixpoints, glass viscosity calculation\nKinematic Viscosity \u2013 conversion between kinematic and dynamic viscosity\nPhysical Characteristics of Water \u2013 a table of water viscosity as a function of temperature\nCalculation of temperature-dependent dynamic viscosities for some common components\nArtificial viscosity\nViscosity of Air, Dynamic and Kinematic, Engineers Edge",
        "unit": "fluidity",
        "url": "https://en.wikipedia.org/wiki/Viscosity"
    },
    {
        "_id": "Pound_sterling",
        "clean": "Pound sterling",
        "text": "Sterling  (ISO code: GBP) is the currency of the United Kingdom and nine of its associated territories. The pound (sign: \u00a3) is the main unit of sterling, and the word pound is also used to refer to the British currency generally, often qualified in international contexts as the British pound or the pound sterling.\nSterling is the world's oldest currency in continuous use since its inception. In 2022, it was the fourth-most-traded currency in the foreign exchange market, after the United States dollar, the euro, and the Japanese yen. Together with those three currencies and the renminbi, it forms the basket of currencies that calculate the value of IMF special drawing rights. As of late 2022, sterling is also the fourth  most-held reserve currency in global reserves.\nThe Bank of England is the central bank for sterling, issuing its own banknotes and regulating issuance of banknotes by private banks in Scotland and Northern Ireland. Sterling banknotes issued by other jurisdictions are not regulated by the Bank of England; their governments guarantee convertibility at par. Historically, sterling was also used to varying degrees by the colonies and territories of the British Empire.\n\n\n== Names ==\n\n\n=== Etymology ===\n\nThere are various theories regarding the origin of the word \"sterling\". The Oxford English Dictionary states that the \"most plausible\" etymology is a derivation from the Old English steorra for \"star\" with the added diminutive suffix -ling, to yield \"little star\". The reference is to the silver penny used in Norman England in the twelfth century, which bore a small star.\nAnother theory holds that the Hanseatic League was the origin of its definition, manufacture, and name: the German name for the Baltic is Ostsee ('East Sea') and from this the Baltic merchants were called Osterlings ('Easterlings'). In 1260, Henry III granted them a charter of protection and land for their kontor, the Steelyard of London, which by the 1340s was also called Esterlingeshalle ('Easterlings Hall'). Because the League's money was not frequently debased like that of England, English traders stipulated to be paid in pounds of the \"Easterlings\", which was contracted to \"'sterling\". The OED dismisses this theory as unlikely, since the stressed first syllable would not have been elided.\nEncyclop\u00e6dia Britannica states that the (pre-Norman) Anglo-Saxon kingdoms had silver coins called sterlings and that the compound noun pound sterling was derived from a pound (weight) of these sterlings.\nThe English word pound derives from the Latin expression l\u012bbra pond\u014d, in which l\u012bbra is a noun meaning 'pound' and pond\u014d is a noun, in the ablative case, meaning 'by weight'.\n\n\n=== Symbol ===\n\nThe currency sign for the pound unit of sterling is \u00a3, which (depending on typeface) may be drawn with one or two bars: the Bank of England has exclusively used the single bar variant since 1975. Historically, a simple capital L (in the historic black-letter typeface, \n  \n    \n      \n        \n          \n            L\n          \n        \n      \n    \n    {\\displaystyle {\\mathfrak {L}}}\n  \n) placed before the numerals, or an italic l. after them, was used in newspapers, books and letters. The Royal Mint was still using this style of notation as late as 1939. The glyphs \u0141 and \u2c60 may occasionally be encountered. Use of the letter \u27e8L\u27e9 for pound derives from medieval Latin documents: \"L\" was the abbreviation for libra, the Roman pound (weight), which in time became an English unit of weight defined as the tower pound. A \"pound sterling\" was literally a tower pound (weight) of sterling silver. In the British pre-decimal (duodecimal) currency system, the term \u00a3sd (or Lsd) for pounds, shillings and pence referred to the Roman libra, solidus, and denarius.\nNotable style guides recommend that the pound sign be used without any abbreviation or qualification to indicate sterling (e.g., \u00a312,000). The ISO 4217 code \"GBP\" (e.g., GBP 12,000) may also be seen should disambiguation become necessary.\n\n\n=== Currency code ===\nThe ISO 4217 currency code for sterling is \"GBP\", formed from the ISO 3166-1 alpha-2 code for the United Kingdom (\"GB\") and the first letter of \"pound\".  \nIn historical sources and some specialist banking uses, the abbreviation stg (in various styles) has been used to indicate sterling. Many stocks on the London Stock Exchange are quoted in penny sterling, using the unofficial code \"GBX\".\n\n\n=== Cable ===\n\nThe exchange rate of sterling against the US dollar is referred to as \"cable\" in the wholesale foreign exchange markets. The origins of this term are attributed to the fact that from the mid-19th century, the sterling/dollar exchange rate was transmitted via transatlantic cable.\n\n\n=== Slang terms ===\nHistorically almost every British coin had a widely recognised nickname, such as \"tanner\" for the sixpence and \"bob\" for the shilling. Since decimalisation these have mostly fallen out of use except as parts of proverbs.\nA common slang term for the pound unit is \"quid\" (singular and plural, except in the common phrase \"quids in\"). Its origin is unknown: possible derivations include scudo, the name for a number of currency units used in Italy until the 19th century, introduced by Italian immigrants; or from Latin quid via the common phrase quid pro quo, literally, \"what for what\", or, figuratively, \"An equal exchange or substitution\". The term \"nicker\" (also both singular and plural) may also refer to the pound.\n\n\n== Crown Dependencies and British Overseas Territories ==\nThe currency of all the Crown Dependencies (Guernsey, Jersey, Isle of Man) and a third of British Overseas Territories (British Antarctic Territory;  Falkland Islands and South Georgia and the South Sandwich Islands; Gibraltar; and Saint Helena, Ascension and Tristan da Cunha) is either sterling or pegged to sterling at par.\nThe other British Overseas Territories have a local currency that is pegged to the U.S. dollar or the New Zealand dollar. The Sovereign Base Areas of Akrotiri and Dhekelia (in Cyprus) use the euro.\n\n\n== Subdivisions and other units ==\n\n\n=== Decimal coinage ===\nSince decimalisation on Decimal Day in 1971, the pound has been divided into 100 pence (denoted on coinage, until 1981, as \"new pence\"). The symbol for the penny is \"p\"; hence an amount such as 50p (\u00a30.50) properly pronounced \"fifty pence\" is often pronounced \"fifty pee\" /f\u026afti pi\u02d0/. The old sign d was not reused for the new penny in order to avoid confusion between the two units. A decimal halfpenny (\u20601/2\u2060p, worth 1.2 old pennies) was issued until 1984 but was withdrawn due to inflation.\n\n\n=== Pre-decimal ===\n\nBefore decimalisation in 1971, the pound was divided into 20 shillings, and each shilling into 12 pence, making 240 pence to the pound. The symbol for the shilling was \"s.\" \u2013  not from the first letter of \"shilling\", but from the Latin solidus. The symbol for the penny was \"d.\", from the French denier, from the Latin denarius (the solidus and denarius were Roman coins). A mixed sum of shillings and pence, such as 3 shillings and 6 pence, was written as \"3/6\" or \"3s. 6d.\" and spoken as \"three and six\" or \"three and sixpence\" except for \"1/1\", \"2/1\" etc., which were spoken as \"one and a penny\", \"two and a penny\", etc. 5 shillings, for example, was written as \"5s.\" or, more commonly, \"5/\u2013\" (five shillings, no pence).\nVarious coin denominations had, and in some cases continue to have, special names, such as florin (2/\u2013), crown (5/\u2013), half crown (2/6d), farthing (1\u20444d), sovereign (\u00a31) and guinea (21s, 21/\u2013, \u00a31\u20131\u20130 or \u00a31.05 in decimal notation).\nBy the 1950s, coins of Kings George III, George IV and William IV had disappeared from circulation, but coins (at least the penny) bearing the head of every British monarch from Queen Victoria onwards could be found in circulation. Silver coins were replaced by those in cupro-nickel in 1947, and by the 1960s the silver coins were rarely seen. Silver/cupro-nickel sixpences, shillings (from any period after 1816) and florins (2 shillings) remained legal tender after decimalisation (as 2\u00bdp, 5p and 10p respectively) until 1980, 1990 and 1993 respectively, but are now officially demonetised.\n\n\n== History (600\u20131945) ==\n \n\nThe pound sterling emerged after the adoption of the Carolingian monetary system in England c.\u2009800. Here is a summary of changes to its value in terms of silver or gold until 1816.\n\n\n=== Anglo-Saxon ===\n\nThe pound was a unit of account in Anglo-Saxon England. By the ninth century it was equal to 240 silver pence.\nThe accounting system of dividing one pound into twenty shillings, a shilling into twelve pence, and a penny into four farthings was adopted from the livre carolingienne system introduced by Charlemagne to the Frankish Empire. The penny was abbreviated to \"d\", from denarius, the Roman equivalent of the penny; the shilling to \"s\" from solidus (written with a long s, \u017f, later evolving into a simple slash, /); and the pound to \"L\" (subsequently \u00a3) from Libra or Livre.\nThe origins of sterling lie in the reign of King Offa of Mercia (757\u2013796), who introduced a \"sterling\" coin made by physically dividing a Tower pound (5,400 grains, 349.9 grams) of silver into 240 parts. In practice, the weights of the coins were not consistent, 240 of them seldom added up to a full pound; there were no shilling or pound coins and these units were used only as an accounting convenience.\nHalfpennies and farthings worth 1\u20442 and 1\u20444 penny respectively were also minted, but small change was more commonly produced by cutting up a whole penny.\n\n\n=== Medieval, 1158 ===\n\nThe early pennies were struck from fine silver (as pure as was available). In 1158, a new coinage was introduced by King Henry II (known as the Tealby penny), with a Tower Pound (5,400 grains, 349.9 g) of 92.5% silver minted into 240 pennies, each penny containing 20.82 grains (1.349 g) of fine silver. Called sterling silver, the alloy is harder than the 99.9% fine silver that was traditionally used, and sterling silver coins did not wear down as rapidly as fine silver ones.\nThe introduction of the larger French gros tournois coins in 1266, and their subsequent popularity, led to additional denominations in the form of groats worth four pence and half groats worth two pence. A gold penny weighing twice the silver penny and valued at 20 silver pence was also issued in 1257 but was not successful.\nThe English penny remained nearly unchanged from 800 and was a prominent exception in the progressive debasements of coinage which occurred in the rest of Europe. The Tower Pound, originally divided into 240 pence, devalued to 243 pence by 1279.\n\n\n=== Edward III, 1351 ===\n\nDuring the reign of King Edward III, the introduction of gold coins received from Flanders as payment for English wool provided substantial economic and trade opportunities but also unsettled the currency for the next 200 years.:\u200a41\u200a The first monetary changes in 1344 consisted of\n\nEnglish pennies reduced to 20+1\u20444 grains (1.312 g; 0.042 ozt) of sterling silver (or 20.25gr @ 0.925 fine = 18.73 gr pure silver) and\nGold double florins weighing 108 gr (6.998 g; 0.225 ozt) and valued at 6 shillings (or 72 pence). (or 108gr @ 0.9948 fine = 107.44 gr pure gold).\nThe resulting gold-silver ratio of 1:12.55 was much higher than the ratio of 1:11 prevailing in the Continent, draining England of its silver coinage and requiring a more permanent remedy in 1351 in the form of\n\nPennies reduced further to 18 gr (1.2 g; 0.038 ozt) of sterling silver (or 18 @ 0.925 fine = 15.73 gr pure silver) and\nNew gold nobles weighing 120 grains (7.776 grams; 0.250 troy ounces) of the finest gold possible at the time (191/192 or 99.48% fine), (meaning 120gr @ 0.9948 fine = 119.38 gr pure gold) and valued at 6 shillings and 8 pence (80 pence, or 1\u20443rd of a pound). The pure gold-silver ratio was thus 1:(80 \u00d7 15.73 / 119.38) = 1:10.5.\nThese gold nobles, together with half-nobles (40 pence) and farthings or quarter-nobles (20 pence), became the first English gold coins produced in quantity.\n\n\n=== Henry IV, 1412 ===\nThe exigencies of the Hundred Years' War during the reign of King Henry IV resulted in further debasements toward the end of his reign, with the English penny reduced to 15 grains sterling silver (0.899 g fine silver) and the half-noble reduced to 54 grains (3.481 g fine gold). The gold-silver ratio went down to 40 \u00d7 0.899 / 3.481 = 10.3.\nAfter the French monetary reform of 1425, the gold half-noble (1\u20446th pound, 40 pence) was worth close to one Livre Parisis (French pound) or 20 sols, while the silver half-groat (2 pence, fine silver 1.798 g) was worth close to 1 sol parisis (1.912 g). Also, after the Flemish monetary reform of 1434, the new Dutch florin was valued close to 40 pence while the Dutch stuiver (shilling) of 1.63 g fine silver was valued close to 2 pence sterling at 1.8 g. This approximate pairing of English half-nobles and half-groats to Continental livres and sols persisted up to the 1560s.\n\n\n=== Great slump, 1464 ===\nThe Great Bullion Famine and the Great Slump of the mid-15th century resulted in another reduction in the English penny to 12 grains sterling silver (0.719 g fine silver) and the introduction of a new half-angel gold coin of 40 grains (2.578 g), worth 1\u20446th pound or 40 pence. The gold-silver ratio rose again to 40 \u00d7 0.719\u20442.578 = 11.2. The reduction in the English penny approximately matched those with the French sol Parisis and the Flemish stuiver; furthermore, from 1469 to 1475 an agreement between England and the Burgundian Netherlands made the English groat (4-pence) mutually exchangeable with the Burgundian double patard (or 2-stuiver) minted under Charles the Rash.\n40 pence or 1\u20446th pound sterling made one Troy Ounce (480 grains, 31.1035 g) of sterling silver. It was approximately on a par with France's livre parisis of one French ounce (30.594 g), and in 1524 it would also be the model for a standardised German currency in the form of the Guldengroschen, which also weighed 1 German ounce of silver or 29.232 g (0.9398 ozt).:\u200a361\u200a\n\n\n=== Tudor, 1551 ===\n\nThe last significant depreciation in sterling's silver standard occurred amidst the 16th century influx of precious metals from the Americas arriving through the Habsburg Netherlands. Enforcement of monetary standards amongst its constituent provinces was loose, spending under King Henry VIII was extravagant, and England loosened the importation of cheaper continental coins for exchange into full-valued English coins. All these contributed to the Great Debasement which resulted in a significant 1\u20443rd reduction in the bullion content of each pound sterling in 1551.\nThe troy ounce of sterling silver was henceforth raised in price by 50% from 40 to 60 silver pennies (each penny weighing 8 grains sterling silver and containing 0.4795 g (0.01542 ozt) fine silver). The gold half-angel of 40 grains (2.578 g (0.0829 ozt) fine gold) was raised in price from 40 pence to 60 pence (5 shillings or 1\u20444 pound) and was henceforth known as the Crown.\nPrior to 1551, English coin denominations closely matched with corresponding sol (2d) and livre (40d) denominations in the Continent, namely:\n\nSilver; see farthing (1\u20444d), halfpenny (1\u20442d), penny (1d), half-groat (2d), and groat (4d)\nGold; see 1351: 1\u20444 noble (20d), 1\u20442 noble (40d) and noble or angel (80d).\nAfter 1551 new denominations were introduced, weighing similarly to 1464-issued coins but increased in value 1+1\u20442 times, namely:\n\nIn silver: the threepence (3d), replacing the half-groat; the sixpence (6d), replacing the groat; and a new shilling or testoon (1s or 1/\u2013).\nIn silver or gold: the crown (5/- (5s) or 60d), replacing the 1\u20442 angel of 40d; and the half crown (2/6d or 30d), replacing the 1\u20444 angel of 20d\nAnd in gold: the new half sovereign (10/\u2013) and sovereign (\u00a31 or 20/\u2013)\n\n\n=== 1601 to 1816 ===\n\nThe silver basis of sterling remained essentially unchanged until the 1816 introduction of the Gold Standard, save for the increase in the number of pennies in a troy ounce from 60 to 62 (hence, 0.464 g fine silver in a penny). Its gold basis remained unsettled, however, until the gold guinea was fixed at 21 shillings in 1717.\nThe guinea was introduced in 1663 with 44+1\u20442 guineas minted out of 12 troy ounces of 22-karat gold (hence, 7.6885 g fine gold) and initially worth \u00a31 or 20 shillings. While its price in shillings was not legally fixed at first, its persistent trade value above 21 shillings reflected the poor state of clipped underweight silver coins tolerated for payment. Milled shillings of full weight were hoarded and exported to the Continent, while clipped, hand-hammered shillings stayed in circulation (as Gresham's law describes).\nIn the 17th century, English merchants tended to pay for imports in silver but were generally paid for exports in gold. This effect was notably driven by trade with the Far East, as the Chinese insisted on payments for their exports being settled in silver. From the mid-17th century, around 28,000 metric tons (27,600 long tons) of silver were received by China, principally from European powers, in exchange for Chinese tea and other goods. In order to be able to purchase Chinese exports in this period, England initially had to export to other European nations and request payment in silver, until the British East India Company was able to foster the indirect sale of opium to the Chinese.\nDomestic demand for silver bullion in Britain further reduced silver coinage in circulation, as the improving fortunes of the merchant class led to increased demand for tableware. Silversmiths had always regarded coinage as a source of raw material, already verified for fineness by the government. As a result, sterling silver coins were being melted and fashioned into \"sterling silverware\" at an accelerating rate. An Act of the Parliament of England in 1697 tried to stem this tide by raising the minimum acceptable fineness on wrought plate from sterling's 92.5% to a new Britannia silver standard of 95.83%. Silverware made purely from melted coins would be found wanting when the silversmith took his wares to the assay office, thus discouraging the melting of coins.\nDuring the time of Sir Isaac Newton, Master of the Mint, the gold guinea was fixed at 21 shillings (\u00a31/1/-) in 1717. But without addressing the problem of underweight silver coins, and with the high resulting gold-silver ratio of 15.2, it gave sterling a firmer footing in gold guineas rather than silver shillings, resulting in a de facto gold standard. Silver and copper tokens issued by private entities partly relieved the problem of small change until the Great Recoinage of 1816.\n\n\n=== Establishment of modern currency ===\nThe Bank of England was founded in 1694, followed by the Bank of Scotland a year later.} The Bank of England began to issue banknotes from \"the late 1600s\".\n\n\n=== Currency of Great Britain (1707) and the United Kingdom (1801) ===\nIn the 17th century Scots currency was pegged to sterling at a value of \u00a312 Scots = \u00a31 sterling.\nIn 1707, the kingdoms of England and Scotland merged into the Kingdom of Great Britain. In accordance with the Treaty of Union, the currency of Great Britain was sterling, with the pound Scots soon being replaced by sterling at the pegged value.\nIn 1801, Great Britain and the Kingdom of Ireland were united to form the United Kingdom of Great Britain and Ireland. However, the Irish pound was not replaced by sterling until January 1826. The conversion rate had long been \u00a313 Irish to \u00a312 sterling. In 1928, six years after the Anglo-Irish Treaty restored Irish autonomy within the British Empire, the Irish Free State established a new Irish pound, initially pegged at par to sterling.\n\n\n=== Use in the Empire ===\n\nSterling circulated in much of the British Empire. In some areas it was used alongside local currencies. For example, the gold sovereign was legal tender in Canada despite the use of the Canadian dollar. Several colonies and dominions adopted the pound as their own currency. These included Australia, Barbados, British West Africa, Cyprus, Fiji, British India, the Irish Free State, Jamaica, New Zealand, South Africa and Southern Rhodesia. Some of these retained parity with sterling throughout their existence (e.g. the South African pound), while others deviated from parity after the end of the gold standard (e.g. the Australian pound). These currencies and others tied to sterling constituted the core of the sterling area.\nThe original English colonies on mainland North America were not party to the sterling area because the above-mentioned silver shortage in England coincided with these colonies' formative years. As a result of equitable trade (and rather less equitable piracy), the Spanish milled dollar became the most common coin within the English colonies.\n\n\n=== Gold standard ===\n\nDuring the American War of Independence and the Napoleonic wars, Bank of England notes were legal tender, and their value floated relative to gold. The Bank also issued silver tokens to alleviate the shortage of silver coins. In 1816, the gold standard was adopted officially, with silver coins minted at a rate of 66 shillings to a troy pound (weight) of sterling silver, thus rendering them as \"token\" issues (i.e. not containing their value in precious metal). In 1817, the sovereign was introduced, valued at 20/\u2013. Struck in 22\u2011carat gold, it contained 113 grains or 7.32238 g (0.235420 ozt) of fine gold and replaced the guinea as the standard British gold coin without changing the gold standard.\nBy the 19th century, sterling notes were widely accepted outside Britain. The American journalist Nellie Bly carried Bank of England notes on her 1889\u20131890 trip around the world in 72 days. During the late 19th and early 20th centuries, many other countries adopted the gold standard. As a consequence, conversion rates between different currencies could be determined simply from the respective gold standards. \u00a31 sterling was equal to US$4.87 in the United States, Can$4.87 in Canada, \u019212.11 in Dutch territories, F 25.22 in French territories (or equivalent currencies of the Latin Monetary Union), 20\u2133 43\u20b0 in Germany, Rbls 9.46 in Russia or K 24.02 in Austria-Hungary. After the International Monetary Conference of 1867 in Paris, the possibility of the UK joining the Latin Monetary Union was discussed, and a Royal Commission on International Coinage examined the issues, resulting in a decision against joining it.\n\n\n=== First world war: suspension of the gold standard ===\nThe gold standard was suspended at the outbreak of First World War in 1914, with Bank of England and Treasury notes becoming legal tender. Before that war, the United Kingdom had one of the world's strongest economies, holding 40% of the world's overseas investments. But after the end of the war, the country was highly indebted: Britain owed \u00a3850 million (about \u00a352.3 billion today) with interest costing the country some 40% of all government spending. The British government under Prime Minister David Lloyd George and Chancellor of the Exchequer Austen Chamberlain tried to make up for the deficit with a deflationary policy, but this only led to the Depression of 1920\u201321.\nBy 1917, production of gold sovereigns had almost halted (the remaining production was for collector's sets and other very specific occasions), and by 1920, the silver coinage was debased from its original .925 fine to just .500 fine. That was due to a drastic increase in silver prices from an average 27/6d. [\u00a31.375] per troy pound in the period between 1894 and 1913, to 89/6d. [\u00a34.475] in August 1920.\n\n\n=== Interwar period: gold standard reinstated ===\nTo try to resume stability, a version of the gold standard was reintroduced in 1925, under which the currency was fixed to gold at its pre-war peg, but one could only exchange currency for gold bullion, not for coins. On 21 September 1931, this was abandoned during the Great Depression, and sterling suffered an initial devaluation of some 25%.\nSince the suspension of the gold standard in 1931, sterling has been a fiat currency, with its value determined by its continued acceptance in the national and international economy.\n\n\n=== World War II ===\nIn 1940, an agreement with the US pegged sterling to the US dollar at a rate of \u00a31 = US$4.03. (Only the year before, it had been US$4.86.) This rate was maintained through the Second World War and became part of the Bretton Woods system which governed post-war exchange rates.\n\n\n== History (1946\u2013present) ==\n\n\n=== Bretton Woods ===\n\nUnder continuing economic pressure, and despite months of denials that it would do so, on 19 September 1949 the government devalued the pound by 30.5% to US$2.80. The 1949 sterling devaluation prompted several other currencies to be devalued against the dollar.\nIn 1961, 1964, and 1966, sterling came under renewed pressure, as speculators were selling pounds for dollars. In summer 1966, with the value of the pound falling in the currency markets, exchange controls were tightened by the Wilson government. Among the measures, tourists were banned from taking more than \u00a350 out of the country in travellers' cheques and remittances, plus \u00a315 in cash; this restriction was not lifted until 1979. Sterling was devalued by 14.3% to \u00a31 = US$2.40 on 18 November 1967.\n\n\n=== Decimalisation ===\n\nUntil decimalisation, amounts in sterling were expressed in pounds, shillings, and pence, with various widely understood notations. The same amount could be stated as 32s. 6d., 32/6, \u00a31. 12s. 6d., or \u00a31/12/6. It was customary to specify some prices (for example professional fees and auction prices for works of art) in guineas (abbr: gn. or gns.), although guinea coins were no longer in use.\nFormal parliamentary proposals to decimalise sterling were first made in 1824 when Sir John Wrottesley, MP for Staffordshire, asked in the House of Commons whether consideration had been given to decimalising the currency. Wrottesley raised the issue in the House of Commons again in 1833, and it was again raised by John Bowring, MP for Kilmarnock Burghs, in 1847 whose efforts led to the introduction in 1848 of what was in effect the first decimal coin in the United Kingdom, the florin, valued at one-tenth of a pound. However, full decimalisation was resisted, although the florin coin, re-designated as ten new pence, survived the transfer to a full decimal system in 1971, with examples surviving in British coinage until 1993.\nJohn Benjamin Smith, MP for Stirling Burghs, raised the issue of full decimalisation again in Parliament in 1853, resulting in the Chancellor of the Exchequer, William Gladstone, announcing soon afterwards that \"the great question of a decimal coinage\" was \"now under serious consideration\". A full proposal for the decimalisation of sterling was then tabled in the House of Commons in June 1855, by William Brown, MP for Lancashire Southern, with the suggestion that the pound sterling be divided into one thousand parts, each called a \"mil\", or alternatively a farthing, as the pound was then equivalent to 960 farthings which could easily be rounded up to one thousand farthings in the new system. This did not result in the conversion of sterling into a decimal system, but it was agreed to establish a Royal Commission to look into the issue. However, largely due to the hostility to decimalisation of two of the appointed commissioners, Lord Overstone (a banker) and John Hubbard (Governor of the Bank of England), decimalisation in Britain was effectively quashed for over a hundred years.\nHowever, sterling was decimalised in various British colonial territories before the United Kingdom (and in several cases in line with William Brown's proposal that the pound be divided into 1,000 parts, called mils). These included Hong Kong from 1863 to 1866; Cyprus from 1955 until 1960 (and continued on the island as the division of the Cypriot pound until 1983); and the Palestine Mandate from 1926 until 1948.\nLater, in 1966, the UK Government decided to include in the Queen's Speech a plan to convert sterling into a decimal currency. As a result of this, on 15 February 1971, the UK decimalised sterling, replacing the shilling and the penny with a single subdivision, the new penny, which was worth 2.4d. For example, a price tag of \u00a31/12/6. became \u00a31.62+1\u20442. The word \"new\" was omitted from coins minted after 1981.\n\n\n=== Free-floating pound ===\n\nWith the breakdown of the Bretton Woods system, sterling floated from August 1971 onwards. At first, it appreciated a little, rising to almost US$2.65 in March 1972 from US$2.42, the upper bound of the band in which it had been fixed. The sterling area effectively ended at this time, when the majority of its members also chose to float freely against sterling and the dollar.\n\n\n=== 1976 sterling crisis ===\n\nJames Callaghan became Prime Minister in 1976. He was immediately told the economy was facing huge problems, according to documents released in 2006 by the National Archives. The effects of the failed Barber Boom and the 1973 oil crisis were still being felt, with inflation rising to nearly 27% in 1975. Financial markets were beginning to believe the pound was overvalued, and in April that year The Wall Street Journal advised the sale of sterling investments in the face of high taxes, in a story that ended with \"goodbye, Great Britain. It was nice knowing you\". At the time the UK Government was running a budget deficit, and the Labour government's strategy emphasised high public spending. Callaghan was told there were three possible outcomes: a disastrous free fall in sterling, an internationally unacceptable siege economy, or a deal with key allies to prop up the pound while painful economic reforms were put in place. The US Government feared the crisis could endanger NATO and the European Economic Community (EEC), and in light of this, the US Treasury set out to force domestic policy changes. In November 1976, the International Monetary Fund (IMF) announced the conditions for a loan, including deep cuts in public expenditure.\n\n\n=== 1979\u20131989 ===\nThe Conservative Party was elected to office in 1979, on a programme of fiscal austerity. Initially, sterling rocketed, moving above \u00a31 to US$2.40, as interest rates rose in response to the monetarist policy of targeting money supply. The high exchange rate was widely blamed for the deep recession of 1981. Sterling fell sharply after 1980; at its lowest, \u00a31 stood at just US$1.03 in March 1985, before rising to US$1.70 in December 1989.\n\n\n=== Following the Deutsche Mark ===\nIn 1988, the Chancellor of the Exchequer, Nigel Lawson, decided that sterling should \"shadow\" the Deutsche Mark (DM), with the unintended result of a rapid rise in inflation as the economy boomed due to low interest rates.\nFollowing German reunification in 1990, the reverse held true, as high German borrowing costs to fund Eastern reconstruction, exacerbated by the political decision to convert the Ostmark to the D\u2013Mark on a 1:1 basis, meant that interest rates in other countries shadowing the D\u2013Mark, especially the UK, were far too high relative to domestic circumstances, leading to a housing decline and recession.\n\n\n=== Following the European Currency Unit ===\nOn 8 October 1990 the Conservative government (Third Thatcher ministry) decided to join the European Exchange Rate Mechanism (ERM), with \u00a31 set at DM 2.95. However, the country was forced to withdraw from the system on \"Black Wednesday\" (16 September 1992) as Britain's economic performance made the exchange rate unsustainable. The event was also triggered by comments by Bundesbank president Helmut Schlesinger who suggested the pound would eventually have to be devalued.\n\"Black Wednesday\" saw interest rates jump from 10% to 15% in an unsuccessful attempt to stop the pound from falling below the ERM limits. The exchange rate fell to DM 2.20. Those who had argued for a lower GBP/DM exchange rate were vindicated since the cheaper pound encouraged exports and contributed to the economic prosperity of the 1990s.\n\n\n=== Following inflation targets ===\nIn 1997, the newly elected Labour government handed over day-to-day control of interest rates to the Bank of England (a policy that had originally been advocated by the Liberal Democrats). The Bank is now responsible for setting its base rate of interest so as to keep inflation (as measured by the Consumer Price Index (CPI)) very close to 2% per annum. Should CPI inflation be more than one percentage point above or below the target, the Governor of the Bank of England is required to write an open letter to the Chancellor of the Exchequer explaining the reasons for this and the measures which will be taken to bring this measure of inflation back in line with the 2% target. On 17 April 2007, annual CPI inflation was reported at 3.1% (inflation of the Retail Prices Index was 4.8%). Accordingly, and for the first time, the Governor had to write publicly to the UK Government explaining why inflation was more than one percentage point higher than its target.\n\n\n=== Euro ===\n\nIn 2007, Gordon Brown, then Chancellor of the Exchequer, ruled out membership in the eurozone for the foreseeable future, saying that the decision not to join had been right for Britain and for Europe.\nOn 1 January 2008, with the Republic of Cyprus switching its currency from the Cypriot pound to the euro, the British sovereign bases on Cyprus (Akrotiri and Dhekelia) followed suit, making the Sovereign Base Areas the only territory under British sovereignty to officially use the euro.\nThe government of former Prime Minister Tony Blair had pledged to hold a public referendum to decide on the adoption of the Euro should \"five economic tests\" be met, to increase the likelihood that any adoption of the euro would be in the national interest. In addition to these internal (national) criteria, the UK would have to meet the European Union's economic convergence criteria (Maastricht criteria) before being allowed to adopt the euro. The Conservative and Liberal Democrat coalition government (2010\u20132015) ruled out joining the euro for that parliamentary term.\nThe idea of replacing sterling with the euro was always controversial with the British public, partly because of sterling's identity as a symbol of British sovereignty and because it would, according to some critics, have led to suboptimal interest rates, harming the British economy. In December 2008, the results of a BBC poll of 1,000 people suggested that 71% would vote no to the euro, 23% would vote yes, while 6% said they were unsure. Sterling did not join the Second European Exchange Rate Mechanism (ERM II) after the euro was created. Denmark and the UK had opt-outs from entry to the euro. Theoretically, every EU nation but Denmark must eventually sign up.\nAs a member of the European Union, the United Kingdom could have adopted the euro as its currency. However, the subject was always politically controversial, and the UK negotiated an opt-out on this issue. Following the UK's withdrawal from the EU, on 31 January 2020, the Bank of England ended its membership of the European System of Central Banks, and shares in the European Central Bank were reallocated to other EU banks.\n\n\n=== Recent exchange rates ===\n\nSterling and the euro fluctuate in value against one another, although there may be correlation between movements in their respective exchange rates with other currencies such as the US dollar. Inflation concerns in the UK led the Bank of England to raise interest rates in late 2006 and 2007. This caused sterling to appreciate against other major currencies and, with the US dollar depreciating at the same time, sterling hit a 15-year high against the US dollar on 18 April 2007, with \u00a31 reaching US$2 the day before, for the first time since 1992. Sterling and many other currencies continued to appreciate against the dollar; sterling hit a 26-year high of \u00a31 to US$2.1161 on 7 November 2007 as the dollar fell worldwide. From mid-2003 to mid-2007, the pound/euro rate remained within a narrow range (\u20ac1.45 \u00b1 5%).\nFollowing the 2007\u20132008 financial crisis, sterling depreciated sharply, declining to \u00a31 to US$1.38 on 23 January 2009 and falling below \u00a31 to \u20ac1.25 against the euro in April 2008. There was a further decline during the remainder of 2008, most dramatically on 29 December when its euro rate hit an all-time low at \u20ac1.0219, while its US dollar rate depreciated. Sterling appreciated in early 2009, reaching a peak against the euro of \u00a31 to \u20ac1.17 in mid-July. In the following months sterling remained broadly steady against the euro, with \u00a31 valued on 27 May 2011 at \u20ac1.15 and US$1.65.\nOn 5 March 2009, the Bank of England announced that it would pump \u00a375 billion of new capital into the British economy, through a process known as quantitative easing (QE). This was the first time in the United Kingdom's history that this measure had been used, although the Bank's Governor Mervyn King suggested it was not an experiment.\nThe process saw the Bank of England creating new money for itself, which it then used to purchase assets such as government bonds, secured commercial paper, or corporate bonds. The initial amount stated to be created through this method was \u00a375 billion, although Chancellor of the Exchequer Alistair Darling had given permission for up to \u00a3150 billion to be created if necessary. It was expected that the process would continue for three months, with results only likely in the long term. By 5 November 2009, some \u00a3175 billion had been injected using QE, and the process remained less effective in the long term. In July 2012, the final increase in QE meant it had peaked at \u00a3375 billion, then holding solely UK Government bonds, representing one third of the UK national debt.\nThe result of the 2016 UK referendum on EU membership caused a major decline in sterling against other world currencies as the future of international trade relationships and domestic political leadership became unclear. The referendum result weakened sterling against the euro by 5% overnight. The night before the vote, sterling was trading at \u00a31 to \u20ac1.30; the next day, this had fallen to \u00a31 to \u20ac1.23. By October 2016, the exchange rate was \u00a31 to \u20ac1.12, a fall of 14% since the referendum. By the end of August 2017 sterling was even lower, at \u00a31 to \u20ac1.08. Against the US dollar, meanwhile, sterling fell from \u00a31 to $1.466 to \u00a31 to $1.3694 when the referendum result was first revealed, and down to \u00a31 to $1.2232 by October 2016, a fall of 16%.\nIn September 2022, under the influence of inflation and tax cuts funded by borrowing, sterling's value reached an all-time low of just over $1.03.\n\n\n=== Annual inflation rate ===\n\nThe Bank of England had stated in 2009 that the decision had been taken to prevent the rate of inflation falling below the 2% target rate. Mervyn King, the Governor of the Bank of England, had also suggested there were no other monetary options left, as interest rates had already been cut to their lowest level ever (0.5%) and it was unlikely that they would be cut further.\nThe inflation rate rose in following years, reaching 5.2% per year (based on the Consumer Price Index) in September 2011, then decreased to around 2.5% the following year. After a number of years when inflation remained near or below the Bank's 2% target, 2021 saw a significant and sustained increase on all indices: as of November 2021, RPI had reached 7.1%, CPI 5.1% and CPIH 4.6%.\n\n\n== Coins ==\n\n\n=== Pre-decimal coins ===\nThe silver penny (plural: pence; abbreviation: d) was the principal and often the only coin in circulation from the 8th century until the 13th century. Although some fractions of the penny were struck (see farthing and halfpenny), it was more common to find pennies cut into halves and quarters to provide smaller change. Very few gold coins were struck, with the gold penny (equal in value to 20 silver pennies) a rare example. However, in 1279, the groat, worth 4d, was introduced, with the half groat following in 1344. 1344 also saw the establishment of a gold coinage with the introduction (after the failed gold florin) of the noble worth six shillings and eight pence (6/8d) (i.e. 3 nobles to the pound), together with the half and quarter noble. Reforms in 1464 saw a reduction in value of the coinage in both silver and gold, with the noble renamed the ryal and worth 10/\u2013 (i.e. 2 to the pound) and the angel introduced at the noble's old value of 6/8d.\nThe reign of Henry VII saw the introduction of two important coins: the shilling (abbr.: s; known as the testoon, equivalent to twelve pence) in 1487 and the pound (known as the sovereign, abbr.: \u00a3 before numerals or \"l.\" (lower-case L) after them, equivalent to twenty shillings) in 1489. In 1526, several new denominations of gold coins were added, including the crown and half crown, worth five shillings (5/\u2013) and two shillings and six pence (2/6, two and six) respectively. Henry VIII's reign (1509\u20131547) saw a high level of debasement which continued into the reign of Edward VI (1547\u20131553). This debasement was halted in 1552, and new silver coinage was introduced, including coins for 1d, 2d, 3d, 4d and 6d, 1/\u2013, 2/6d and 5/\u2013. In the reign of Elizabeth I (1558\u20131603), silver 3\u20444d and 1+1\u20442d coins were added, but these denominations did not last. Gold coins included the half-crown, crown, angel, half-sovereign (10/\u2013) and sovereign (\u00a31). Elizabeth's reign also saw the introduction of the horse-drawn screw press to produce the first \"milled\" coins.\nFollowing the succession of the Scottish King James VI to the English throne, a new gold coinage was introduced, including the spur ryal (15/\u2013), the unite (20/\u2013) and the rose ryal (30/\u2013). The laurel, worth 20/\u2013, followed in 1619. The first base metal coins were also introduced: tin and copper farthings. Copper halfpenny coins followed in the reign of Charles I. During the English Civil War, a number of siege coinages were produced, often in unusual denominations.\nFollowing the restoration of the monarchy in 1660, the coinage was reformed, with the ending of production of hammered coins in 1662. The guinea was introduced in 1663, soon followed by the 1\u20442, 2 and 5 guinea coins. The silver coinage consisted of denominations of 1d, 2d, 3d, 4d and 6d, 1/\u2013, 2/6d and 5/\u2013. Due to the widespread export of silver in the 18th century, the production of silver coins gradually came to a halt, with the half crown and crown not issued after the 1750s, and the 6d and 1/\u2013 stopping production in the 1780s. In response, copper 1d and 2d coins and a gold 1\u20443 guinea (7/\u2013) were introduced in 1797. The copper penny was the only one of these coins to survive long.\nTo alleviate the shortage of silver coins, between 1797 and 1804, the Bank of England counterstamped Spanish dollars (8 reales) and other Spanish and Spanish colonial coins for circulation. A small counterstamp of the King's head was used. Until 1800, these circulated at a rate of 4/9d for 8 reales. After 1800, a rate of 5/\u2013 for 8 reales was used. The Bank then issued silver tokens for 5/\u2013 (struck over Spanish dollars) in 1804, followed by tokens for 1/6d and 3/\u2013 between 1811 and 1816.\nIn 1816, a new silver coinage was introduced in denominations of 6d, 1/\u2013, 2/6d (half-crown) and 5/\u2013 (crown). The crown was only issued intermittently until 1900. It was followed by a new gold coinage in 1817 consisting of 10/\u2013 and \u00a31 coins, known as the half sovereign and sovereign. The silver 4d coin was reintroduced in 1836, followed by the 3d in 1838, with the 4d coin issued only for colonial use after 1855. In 1848, the 2/\u2013 florin was introduced, followed by the short-lived double florin in 1887. In 1860, copper was replaced by bronze in the farthing (quarter penny, 1\u20444d), halfpenny and penny.\nDuring the First World War, production of the sovereign and half-sovereign was suspended, and although the gold standard was later restored, the coins saw little circulation thereafter. In 1920, the silver standard, maintained at .925 since 1552, was reduced to .500. In 1937, a nickel-brass 3d coin was introduced; the last silver 3d coins were issued seven years later. In 1947, the remaining silver coins were replaced with cupro-nickel, with the exception of Maundy coinage which was then restored to .925. Inflation caused the farthing to cease production in 1956 and be demonetised in 1960. In the run-up to decimalisation, the halfpenny and half-crown were demonetised in 1969.\n\n\n=== Decimal coins ===\n\nBritish coinage timeline:\n\n1968: The first decimal coins were introduced. These were cupro-nickel 5p and 10p coins which were the same size as, equivalent in value to, and circulated alongside, the one shilling coin and the florin (two shilling coin) respectively.\n1969: The curved equilateral heptagonal cupro-nickel 50p coin replaced the ten shilling note (10/\u2013).\n1970: The half crown (2/6d, 12.5p) was demonetised.\n1971: The decimal coinage was completed when decimalisation came into effect in 1971 with the introduction of the bronze half new penny (1\u20442p), new penny (1p), and two new pence (2p) coins and the withdrawal of the (old) penny (1d) and (old) threepence (3d) coins.\n1980: Withdrawal of the sixpence (6d) coin, which had continued in circulation at a value of 2+1\u20442p.\n1982: The word \"new\" was dropped from the coinage and a 20p coin was introduced.\n1983: A (round, brass) \u00a31 coin was introduced.\n1983: The 1\u20442p coin was last produced.\n1984: The 1\u20442p coin was withdrawn from circulation.\n1990: The crown, historically valued at five shillings (25p), was re-tariffed for future issues as a commemorative coin at \u00a35.\n1990: A new, smaller 5p coin was introduced, replacing the original size that had been the same as the shilling coins of the same value that it had in turn replaced. These first generation 5p coins and any remaining old shilling coins were withdrawn from circulation in 1991.\n1992: A new, smaller 10p coin was introduced, replacing the original size that had been the same as the florin or two shilling coins of the same value that it had in turn replaced. These first generation 10p coins and any remaining old florin coins were withdrawn from circulation over the following two years.\n1992: 1p and 2p coins began to be minted in copper-plated steel (the original bronze coins continued in circulation).\n1997: A new 50p coin was introduced, replacing the original size that had been in use since 1969, and the first generation 50p coins were withdrawn from circulation.\n1998: The bi-metallic \u00a32 coin was introduced.\n2007: By now the value of copper in the pre-1992 1p and 2p coins (which are 97% copper) exceeded those coins' face value to such an extent that melting down the coins by entrepreneurs was becoming worthwhile (with a premium of up to 11%, with smelting costs reducing this to around 4%)\u2014although this is illegal, and the market value of copper has subsequently fallen dramatically from these earlier peaks.\nIn April 2008, an extensive redesign of the coinage was unveiled. The 1p, 2p, 5p, 10p, 20p, and 50p coins feature parts of the Royal Shield on their reverse; and the reverse of the pound coin showed the whole shield. The coins were issued gradually into circulation, starting in mid-2008. They have the same sizes, shapes and weights as those with the old designs which, apart from the round pound coin which was withdrawn in 2017, continue to circulate.\n2012: The 5p and 10p coins were changed from cupro-nickel to nickel-plated steel.\n2017: A more secure twelve-sided bi-metallic \u00a31 coin was introduced to reduce forgery. The old round \u00a31 coin ceased to be legal tender on 15 October 2017.\nAs of 2020, the oldest circulating coins in the UK are the 1p and 2p copper coins introduced in 1971. No other coins from before 1982 are in circulation. Prior to the withdrawal from circulation in 1992, the oldest circulating coins usually dated from 1947: although older coins were still legal tender, inflation meant that their silver content was worth more than their face value, so they tended to be removed from circulation and hoarded. Before decimalisation in 1971, a handful of change might have contained coins over 100 years old, bearing any of five monarchs' heads, especially in the copper coins.\n\n\n== Banknotes ==\n\nThe first sterling notes were issued by the Bank of England shortly after its foundation in 1694. Denominations were initially handwritten on the notes at the time of issue. From 1745, the notes were printed in denominations between \u00a320 and \u00a31,000, with any odd shillings added by hand. \u00a310 notes were added in 1759, followed by \u00a35 in 1793 and \u00a31 and \u00a32 in 1797. The lowest two denominations were withdrawn after the end of the Napoleonic wars. In 1855, the notes were converted to being entirely printed, with denominations of \u00a35, \u00a310, \u00a320, \u00a350, \u00a3100, \u00a3200, \u00a3300, \u00a3500 and \u00a31,000 issued.\nThe Bank of Scotland began issuing notes in 1695. Although the pound Scots was still the currency of Scotland, these notes were denominated in sterling in values up to \u00a3100. From 1727, the Royal Bank of Scotland also issued notes. Both banks issued some notes denominated in guineas as well as pounds. In the 19th century, regulations limited the smallest note issued by Scottish banks to be the \u00a31 denomination, a note not permitted in England.\nWith the extension of sterling to Ireland in 1825, the Bank of Ireland began issuing sterling notes, later followed by other Irish banks. These notes included the unusual denominations of 30/\u2013 and \u00a33. The highest denomination issued by the Irish banks was \u00a3100.\nIn 1826, banks at least 65 miles (105 km) from London were given permission to issue their own paper money. From 1844, new banks were excluded from issuing notes in England and Wales but not in Scotland and Ireland. Consequently, the number of private banknotes dwindled in England and Wales but proliferated in Scotland and Ireland. The last English private banknotes were issued in 1921.\nIn 1914, the Treasury introduced notes for 10/\u2013 and \u00a31 to replace gold coins. These circulated until 1928 when they were replaced by Bank of England notes. Irish independence reduced the number of Irish banks issuing sterling notes to five operating in Northern Ireland. The Second World War had a drastic effect on the note production of the Bank of England. Fearful of mass forgery by the Nazis (see Operation Bernhard), all notes for \u00a310 and above ceased production, leaving the bank to issue only 10/\u2013, \u00a31 and \u00a35 notes. Scottish and Northern Irish issues were unaffected, with issues in denominations of \u00a31, \u00a35, \u00a310, \u00a320, \u00a350 and \u00a3100.\nDue to repeated devaluations and spiralling inflation the Bank of England reintroduced \u00a310 notes in 1964. In 1969, the 10/\u2013 note was replaced by the 50p coin, again due to inflation. \u00a320 Bank of England notes were reintroduced in 1970, followed by \u00a350 in 1981. A \u00a31 coin was introduced in 1983, and Bank of England \u00a31 notes were withdrawn in 1988. Scottish and Northern Irish banks followed, with only the Royal Bank of Scotland continuing to issue this denomination.\nUK notes include raised print (e.g. on the words \"Bank of England\"); watermarks; embedded metallic thread; holograms; and fluorescent ink visible only under UV lamps. Three printing techniques are involved: offset litho, intaglio and letterpress; and the notes incorporate a total of 85 specialized inks.\nThe Bank of England produces notes named \"giant\" and \"titan\". A giant is a one million pound note, and a titan is a one hundred million pound bank note. Giants and titans are used only within the banking system.\n\n\n=== Polymer banknotes ===\nThe Northern Bank \u00a35 note, issued by Northern Ireland's Northern Bank (now Danske Bank) in 2000, was the only polymer banknote in circulation until 2016. The Bank of England introduced \u00a35 polymer banknotes in September 2016, and the paper \u00a35 notes were withdrawn on 5 May 2017. A polymer \u00a310 banknote was introduced on 14 September 2017, and the paper note was withdrawn on 1 March 2018. A polymer \u00a320 banknote was introduced on 20 February 2020, followed by a polymer \u00a350 in 2021.\n\n\n== Monetary policy ==\nAs the central bank of the United Kingdom which has been delegated authority by the government, the Bank of England sets the monetary policy for the British pound by controlling the amount of money in circulation. It has a monopoly on the issuance of banknotes in England and Wales and regulates the amount of banknotes issued by seven authorized banks in Scotland and Northern Ireland. HM Treasury has reserve powers to give orders to the committee \"if they are required in the public interest and by extreme economic circumstances\" but such orders must be endorsed by Parliament within 28 days.\nUnlike banknotes which have separate issuers in Scotland and Northern Ireland, all British coins are issued by the Royal Mint, an independent enterprise (wholly owned by the Treasury) which also mints coins for other countries.\n\n\n== Legal tender and national issues ==\n\nLegal tender in the United Kingdom is defined such that \"a debtor cannot successfully be sued for non-payment if he pays into court in legal tender.\" Parties can alternatively settle a debt by other means with mutual consent. Strictly speaking, it is necessary for the debtor to offer the exact amount due as there is no obligation for the other party to provide change.\nThroughout the UK, \u00a31 and \u00a32 coins are legal tender for any amount, with the other coins being legal tender only for limited amounts. Bank of England notes are legal tender for any amount in England and Wales, but not in Scotland or Northern Ireland. (Bank of England 10/\u2013 and \u00a31 notes were legal tender, as were Scottish banknotes, during World War II under the Currency (Defence) Act 1939, which was repealed on 1 January 1946.) Channel Islands and Manx banknotes are legal tender only in their respective jurisdictions.\nBank of England, Scottish, Northern Irish, Channel Islands, Isle of Man, Gibraltar, and Falkland banknotes may be offered anywhere in the UK, although there is no obligation to accept them as a means of payment, and acceptance varies. For example, merchants in England generally accept Scottish and Northern Irish notes, but some unfamiliar with them may reject them. However, Scottish and Northern Irish notes both tend to be accepted in Scotland and Northern Ireland, respectively. Merchants in England generally do not accept Jersey, Guernsey, Manx, Gibraltarian, and Falkland notes but Manx notes are generally accepted in Northern Ireland. Bank of England notes are generally accepted in the Falklands and Gibraltar, but for example, Scottish and Northern Irish notes are not. Since all of the notes are denominated in sterling, banks will exchange them for locally issued notes at face value, though some in the UK have had trouble exchanging Falkland Islands notes.\nCommemorative \u00a35 and 25p (crown) coins, and decimal sixpences (6p, not the pre-decimalisation 6d, equivalent to 2+1\u20442p) made for traditional wedding ceremonies and Christmas gifts, although rarely if ever seen in circulation, are formally legal tender, as are the bullion coins issued by the Mint.\n\n\n=== Pegged currencies ===\nIn Britain's Crown Dependencies, the Manx pound, Jersey pound, and Guernsey pound are unregulated by the Bank of England and are issued independently. However, they are maintained at a fixed exchange rate by their respective governments, and Bank of England notes have been made legal tender on the islands, forming a sort of one-way de facto currency union. Internationally they are considered local issues of sterling so do not have ISO 4217 codes. \"GBP\" is usually used to represent all of them; informal abbreviations resembling ISO codes are used where the distinction is important.\nBritish Overseas Territories are responsible for the monetary policy of their own currencies (where they exist), and have their own ISO 4217 codes. The Falkland Islands pound, Gibraltar pound, and Saint Helena pound are set at a fixed 1:1 exchange rate with the British pound by local governments.\n\n\n== Value ==\nIn 2006, the House of Commons Library published a research paper which included an index of prices for each year between 1750 and 2005, where 1974 was indexed at 100.\nRegarding the period 1750\u20131914 the document states: \"Although there was considerable year on year fluctuation in price levels prior to 1914 (reflecting the quality of the harvest, wars, etc.) there was not the long-term steady increase in prices associated with the period since 1945\". It goes on to say that \"Since 1945 prices have risen in every year with an aggregate rise of over 27 times\".\nThe value of the index in 1751 was 5.1, increasing to a peak of 16.3 in 1813 before declining very soon after the end of the Napoleonic Wars to around 10.0 and remaining in the range 8.5\u201310.0 at the end of the 19th century. The index was 9.8 in 1914 and peaked at 25.3 in 1920, before declining to 15.8 in 1933 and 1934\u2014prices were only about three times as high as they had been 180 years earlier.\nInflation has had a dramatic effect during and after World War II: the index was 20.2 in 1940, 33.0 in 1950, 49.1 in 1960, 73.1 in 1970, 263.7 in 1980, 497.5 in 1990, 671.8 in 2000 and 757.3 in 2005. The smallest coin in 1971 was the 1\u20442p, worth about 6.4p in 2015 prices.\nThe following table shows the equivalent amount of goods and services that, in a particular year, could be purchased with \u00a31.\nThe table shows that from 1971 to 2023, the buying power of a pound fell by 94.4%.\n\nFor example, the purchasing power of a pound in 2006 was slightly more than that of 10p in 1971; conversely, the purchasing power of a pound in 1971 was slightly less than that of \u00a310 in 2006. The hypothetical \"shopping basket\" of goods and services that cost \u00a310 in 1971 would cost \u00a398.04 in 2006 (and  \u00a3163.40 in 2022).\n\n\n== Exchange rate ==\nSterling is freely bought and sold on the foreign exchange markets around the world, and its value relative to other currencies therefore fluctuates.\n\n\n== Reserve ==\nSterling is used as a reserve currency around the world. As of 2020, it is ranked fourth in value held as reserves.\n\n\n== See also ==\n\nCommonwealth banknote-issuing institutions\nList of British currencies\nList of currencies in Europe\nList of the largest trading partners of United Kingdom\nPound (currency) \u2013 other currencies with a \"pound\" unit of account.\n\n\n== Footnotes ==\n\n\n== References ==\n\n\n== Further reading ==\n\n\n== External links ==\n\nOfficial website of the Royal Mint\nPound Sterling \u2013 BBC News (Foreign exchange market news)",
        "unit": "pound sterling",
        "url": "https://en.wikipedia.org/wiki/Pound_sterling"
    },
    {
        "_id": "Base_pair",
        "clean": "Base pair",
        "text": "A base pair (bp) is a fundamental unit of double-stranded nucleic acids consisting of two nucleobases bound to each other by hydrogen bonds.  They form the building blocks of the DNA double helix and contribute to the folded structure of both DNA and RNA. Dictated by specific hydrogen bonding patterns, \"Watson\u2013Crick\" (or \"Watson\u2013Crick\u2013Franklin\") base pairs (guanine\u2013cytosine and adenine\u2013thymine) allow the DNA helix to maintain a regular helical structure that is subtly dependent on its nucleotide sequence. The complementary nature of this based-paired structure provides a redundant copy of the genetic information encoded within each strand of DNA. The regular structure and data redundancy provided by the DNA double helix make DNA well suited to the storage of genetic information, while base-pairing between DNA and incoming nucleotides provides the mechanism through which DNA polymerase replicates DNA and RNA polymerase transcribes DNA into RNA. Many DNA-binding proteins can recognize specific base-pairing patterns that identify particular regulatory regions of genes.\nIntramolecular base pairs can occur within single-stranded nucleic acids. This is particularly important in RNA molecules (e.g., transfer RNA), where Watson\u2013Crick base pairs (guanine\u2013cytosine and adenine\u2013uracil) permit the formation of short double-stranded helices, and a wide variety of non\u2013Watson\u2013Crick interactions (e.g., G\u2013U or A\u2013A) allow RNAs to fold into a vast range of specific three-dimensional structures. In addition, base-pairing between transfer RNA (tRNA) and messenger RNA (mRNA) forms the basis for the molecular recognition events that result in the nucleotide sequence of mRNA becoming translated into the amino acid sequence of proteins via the genetic code.\nThe size of an individual gene or an organism's entire genome is often measured in base pairs because DNA is usually double-stranded. Hence, the number of total base pairs is equal to the number of nucleotides in one of the strands (with the exception of non-coding single-stranded regions of telomeres). The haploid human genome (23 chromosomes) is estimated to be about 3.2 billion base pairs long and to contain 20,000\u201325,000 distinct protein-coding genes. A kilobase (kb) is a unit of measurement in molecular biology equal to 1000 base pairs of DNA or RNA. The total number of DNA base pairs on Earth is estimated at 5.0\u00d71037 with a weight of 50 billion tonnes. In comparison, the total mass of the biosphere has been estimated to be as much as 4 TtC (trillion tons of carbon).\n\n\n== Hydrogen bonding and stability ==\n\nHydrogen bonding is the chemical interaction that underlies the base-pairing rules described above. Appropriate geometrical correspondence of hydrogen bond donors and acceptors allows only the \"right\" pairs to form stably. DNA with high GC-content is more stable than DNA with low GC-content. Crucially, however, stacking interactions are primarily responsible for stabilising the double-helical structure; Watson-Crick base pairing's contribution to global structural stability is minimal, but its role in the specificity underlying complementarity is, by contrast, of maximal importance as this underlies the template-dependent processes of the central dogma (e.g. DNA replication).\nThe bigger nucleobases, adenine and guanine, are members of a class of double-ringed chemical structures called purines; the smaller nucleobases, cytosine and thymine (and uracil), are members of a class of single-ringed chemical structures called pyrimidines. Purines are complementary only with pyrimidines: pyrimidine\u2013pyrimidine pairings are energetically unfavorable because the molecules are too far apart for hydrogen bonding to be established; purine\u2013purine pairings are energetically unfavorable because the molecules are too close, leading to overlap repulsion. Purine\u2013pyrimidine base-pairing of AT or GC or UA (in RNA) results in proper duplex structure. The only other purine\u2013pyrimidine pairings would be AC and GT and UG (in RNA); these pairings are mismatches because the patterns of hydrogen donors and acceptors do not correspond. The GU pairing, with two hydrogen bonds, does occur fairly often in RNA (see wobble base pair).\nPaired DNA and RNA molecules are comparatively stable at room temperature, but the two nucleotide strands will separate above a melting point that is determined by the length of the molecules, the extent of mispairing (if any), and the GC content. Higher GC content results in higher melting temperatures; it is, therefore, unsurprising that the genomes of extremophile organisms such as Thermus thermophilus are particularly GC-rich. On the converse, regions of a genome that need to separate frequently \u2014 for example, the promoter regions for often-transcribed genes \u2014 are comparatively GC-poor (for example, see TATA box). GC content and melting temperature must also be taken into account when designing  primers for PCR reactions.\n\n\n=== Examples ===\nThe following DNA sequences illustrate pair double-stranded patterns. By convention, the top strand is written from the 5\u2032-end to the 3\u2032-end; thus, the bottom strand is written 3\u2032 to 5\u2032.\n\nA base-paired DNA sequence:\nATCGATTGAGCTCTAGCG\nTAGCTAACTCGAGATCGC\nThe corresponding RNA sequence, in which uracil is substituted for thymine in the RNA strand:\nAUCGAUUGAGCUCUAGCG\nUAGCUAACUCGAGAUCGC\n\n\n== Base analogs and intercalators ==\n\nChemical analogs of nucleotides can take the place of proper nucleotides and establish non-canonical base-pairing, leading to errors (mostly point mutations) in DNA replication and DNA transcription. This is due to their isosteric chemistry. One common mutagenic base analog is 5-bromouracil, which resembles thymine but can base-pair to guanine in its enol form.\nOther chemicals, known as DNA intercalators, fit into the gap between adjacent bases on a single strand and induce frameshift mutations by \"masquerading\" as a base, causing the DNA replication machinery to skip or insert additional nucleotides at the intercalated site. Most intercalators are large polyaromatic compounds and are known or suspected carcinogens. Examples include ethidium bromide and acridine.\n\n\n== Mismatch repair ==\nMismatched base pairs can be generated by errors of DNA replication and as intermediates during homologous recombination. The process of mismatch repair ordinarily must recognize and correctly repair a small number of base mispairs within a long sequence of normal DNA base pairs.  To repair mismatches formed during DNA replication, several distinctive repair processes have evolved to distinguish between the template strand and the newly formed strand so that only the newly inserted incorrect nucleotide is removed (in order to avoid generating a mutation).  The proteins employed in mismatch repair during DNA replication, and the clinical significance of defects in this process are described in the article DNA mismatch repair.  The process of mispair correction during recombination is described in the article gene conversion.\n\n\n== Length measurements ==\n\nThe following abbreviations are commonly used to describe the length of a D/RNA molecule:\n\nbp  = base pair\u2014one bp corresponds to approximately 3.4 \u00c5 (340 pm)  of length along the strand, and to roughly 618 or 643 daltons for DNA and RNA respectively.\nkb (= kbp) = kilo\u2013base-pair = 1,000 bp\nMb (= Mbp) = mega\u2013base-pair = 1,000,000 bp\nGb (= Gbp) = giga\u2013base-pair = 1,000,000,000 bp\nFor single-stranded DNA/RNA, units of nucleotides are used\u2014abbreviated nt (or knt, Mnt, Gnt)\u2014as they are not paired.\nTo distinguish between units of computer storage and bases, kbp, Mbp, Gbp, etc. may be used for base pairs.\nThe centimorgan is also often used to imply distance along a chromosome, but the number of base pairs it corresponds to varies widely. In the human genome, the centimorgan is about 1 million base pairs.\n\n\n== Unnatural base pair (UBP) ==\n\nAn unnatural base pair (UBP) is a designed subunit (or nucleobase) of DNA which is created in a laboratory and does not occur in nature.  DNA sequences have been described which use newly created nucleobases to form a third base pair, in addition to the two base pairs found in nature, A-T (adenine \u2013 thymine) and G-C (guanine \u2013 cytosine).  A few research groups have been searching for a third base pair for DNA, including teams led by Steven A. Benner, Philippe Marliere, Floyd E. Romesberg and Ichiro Hirao. Some new base pairs based on alternative hydrogen bonding, hydrophobic interactions and metal coordination have been reported.\nIn 1989 Steven Benner (then working at the Swiss Federal Institute of Technology in Zurich) and his team led with modified forms of cytosine and guanine into DNA molecules in vitro. The nucleotides, which encoded RNA and proteins, were successfully replicated in vitro. Since then, Benner's team has been trying to engineer cells that can make foreign bases from scratch, obviating the need for a feedstock.\nIn 2002, Ichiro Hirao's group in Japan developed an unnatural base pair between 2-amino-8-(2-thienyl)purine (s) and pyridine-2-one (y) that functions in transcription and translation, for the site-specific incorporation of non-standard amino acids into proteins. In 2006, they created 7-(2-thienyl)imidazo[4,5-b]pyridine (Ds) and pyrrole-2-carbaldehyde (Pa) as a third base pair for replication and transcription. Afterward, Ds and 4-[3-(6-aminohexanamido)-1-propynyl]-2-nitropyrrole (Px) was discovered as a high fidelity pair in PCR amplification. In 2013, they applied the Ds-Px pair to DNA aptamer generation by in vitro selection (SELEX) and demonstrated the genetic alphabet expansion significantly augment DNA aptamer affinities to target proteins.\nIn 2012, a group of American scientists led by Floyd Romesberg, a chemical biologist at the Scripps Research Institute in San Diego, California, published that his team designed an unnatural base pair (UBP).  The two new artificial nucleotides or Unnatural Base Pair (UBP) were named d5SICS and dNaM. More technically, these artificial nucleotides bearing hydrophobic nucleobases, feature two fused aromatic rings that form a (d5SICS\u2013dNaM) complex or base pair in DNA. His team designed a variety of in vitro or \"test tube\" templates containing the unnatural base pair and they confirmed that it was efficiently replicated with high fidelity in virtually all sequence contexts using the modern standard in vitro techniques, namely PCR amplification of DNA and PCR-based applications. Their results show that for PCR and PCR-based applications, the d5SICS\u2013dNaM unnatural base pair is functionally equivalent to a natural base pair, and when combined with the other two natural base pairs used by all organisms, A\u2013T and G\u2013C, they provide a fully functional and expanded six-letter \"genetic alphabet\".\nIn 2014 the same team from the Scripps Research Institute reported that they synthesized a stretch of circular DNA known as a plasmid containing natural T-A and C-G base pairs along with the best-performing UBP Romesberg's laboratory had designed and inserted it into cells of the common bacterium E. coli that successfully replicated the unnatural base pairs through multiple generations. The transfection did not hamper the growth of the E. coli cells and showed no sign of losing its unnatural base pairs to its natural DNA repair mechanisms. This is the first known example of a living organism passing along an expanded genetic code to subsequent generations. Romesberg said he and his colleagues created 300 variants to refine the design of nucleotides that would be stable enough and would be replicated as easily as the natural ones when the cells divide.  This was in part achieved by the addition of a supportive algal gene that expresses a nucleotide triphosphate transporter which efficiently imports the triphosphates of both d5SICSTP and dNaMTP into E. coli bacteria. Then, the natural bacterial replication pathways use them to accurately replicate a plasmid containing d5SICS\u2013dNaM. Other researchers were surprised that the bacteria replicated these human-made DNA subunits.\nThe successful incorporation of a third base pair is a significant breakthrough toward the goal of greatly expanding the number of amino acids which can be encoded by DNA, from the existing 20 amino acids to a theoretically possible 172, thereby expanding the potential for living organisms to produce novel proteins. The artificial strings of DNA do not encode for anything yet, but scientists speculate they could be designed to manufacture new proteins which could have industrial or pharmaceutical uses. Experts said the synthetic DNA incorporating the unnatural base pair raises the possibility of life forms based on a different DNA code.\n\n\n== Non-canonical base pairing ==\n\nIn addition to the canonical pairing, some conditions can also favour base-pairing with alternative base orientation, and number and geometry of hydrogen bonds. These pairings are accompanied by alterations to the local backbone shape.\nThe most common of these is the wobble base pairing that occurs between tRNAs and mRNAs at the third base position of many codons during transcription and during the charging of tRNAs by some tRNA synthetases. They have also been observed in the secondary structures of some RNA sequences.\nAdditionally, Hoogsteen base pairing (typically written as A\u2022U/T and G\u2022C) can exist in some DNA sequences (e.g. CA and TA dinucleotides) in dynamic equilibrium with standard Watson\u2013Crick pairing. They have also been observed in some protein\u2013DNA complexes.\nIn addition to these alternative base pairings, a wide range of base-base hydrogen bonding is observed in RNA secondary and tertiary structure. These bonds are often necessary for the precise, complex shape of an RNA, as well as its binding to interaction partners.\n\n\n== See also ==\nList of Y-DNA single-nucleotide polymorphisms\nNon-canonical base pairing\nChargaff's rules\n\n\n== References ==\n\n\n== Further reading ==\n\n\n== External links ==\n\nDAN\u2014webserver version of the EMBOSS tool for calculating melting temperatures",
        "unit": "base pair",
        "url": "https://en.wikipedia.org/wiki/Base_pair"
    },
    {
        "_id": "South_African_rand",
        "clean": "South African rand",
        "text": "The South African rand, or simply the rand, (sign: R; code: ZAR) is the official currency of South Africa. It is subdivided into 100 cents (sign: \"c\"), and a comma separates the rand and cents.\nThe South African rand is legal tender in the Common Monetary Area member states of Namibia, Lesotho, and Eswatini, with these three countries also having national currencies: (the dollar, the loti and the lilangeni respectively) pegged with the rand at parity and still widely accepted as substitutes. The rand was also legal tender in Botswana until 1976 when the pula replaced the rand at par.\n\n\n== Etymology ==\nThe rand takes its name from the Witwatersrand (\"white waters' ridge\" in English, rand being the Afrikaans (and Dutch) word for 'ridge'), the ridge upon which Johannesburg is built and where most of South Africa's gold deposits were found. In English and Afrikaans (and Dutch), the singular and plural forms of the unit (\"rand\") are the same: one rand, ten rand, and two million rand.\n\n\n== History ==\n\nThe rand was introduced in the Union of South Africa in 1961, three months before the country declared itself a republic. A Decimal Coinage Commission had been set up in 1956 to consider a move away from the denominations of pounds, shillings, and pence; it submitted its recommendations on 8 August 1958. It replaced the South African pound as legal tender, at the rate of 2 rand to 1 pound, or 10 shillings to the rand. The government introduced a mascot, Decimal Dan, \"the rand-cent man\" (known in Afrikaans as Daan Desimaal). This was accompanied by a radio jingle to inform the public about the new currency. Although pronounced in the Afrikaans style as  in the jingles when introduced, the contemporary pronunciation in South African English is .\n\n\n=== Brief exchange rate history ===\n\n\n==== 1961\u20132000 ====\n\nOne rand was worth US$1.40 (R0.72 per dollar) from the time of its inception in 1961 until late 1971, and the U.S. dollar became stronger than South African currency for the first time on 15 March 1982. Its value thereafter fluctuated as various exchange rate dispensations were implemented by the South African authorities. By the early 1980s, high inflation and mounting political pressure combined with sanctions placed against the country due to international opposition to the apartheid system had started to erode its value. The currency broke above parity with the dollar for the first time in March 1982. It continued to trade between R1 and R1.30 to the dollar until June 1984, when the currency's depreciation gained momentum. By February 1985, it was trading at over R2 per dollar, and in July of that year, all foreign exchange trading was suspended for three days to try to stop the depreciation.\nBy the time that State President P. W. Botha made his Rubicon speech on 15 August 1985, it had weakened to R2.40 per dollar. The currency recovered somewhat between 1986 and 1988, trading near the R2 level most of the time and breaking beneath it sporadically. The recovery was short-lived; by the end of 1989, the rand was trading at more than R2.50 per dollar.\nAs it became clear in the early 1990s that the country was destined for Black majority rule and one reform after the other was announced, uncertainty about the country's future hastened the depreciation until the level of R3 to the dollar was breached in November 1992. A host of local and international events influenced the currency after that, most notably the 1994 general election, which had it weaken to over R3.60 to the dollar, the election of Tito Mboweni as the governor of the South African Reserve Bank, and the inauguration of President Thabo Mbeki in 1999, which had it quickly slide to over R6 to the dollar. The controversial land reform programme that was initiated in Zimbabwe, followed by the September 11, 2001 attacks, propelled it to its weakest historical level of R13.84 to the dollar in December 2001.\n\n\n==== 2001\u20132011 ====\n\nThis sudden depreciation in 2001 led to a formal investigation and a dramatic recovery. By the end of 2002, the currency was trading under R9 to the dollar again, and by the end of 2004, it was trading under R5.70. The currency softened somewhat in 2005, trading around R6.35 to the dollar at the end of the year. At the start of 2006, however, the currency resumed its rally and, as of 19 January 2006, was trading under R6 to the dollar again. However, the rand weakened significantly during the second and third quarters of 2006 (i.e., April through September).\nIn sterling terms, it fell from around 9.5% to just over 7%, losing some 25% of its international trade-weighted value in six months. In late 2007, the rand rallied modestly to just over 8%, only to experience a precipitous slide during the first quarter of 2008.\nThis downward slide could be attributed to a range of factors: South Africa's worsening current account deficit, which widened to a 36\u2011year high of 7.3% of gross domestic product (GDP) in 2007; inflation at a five-year high of just under 9%; escalating global risk aversion as investors' concerns over the spreading impact of the sub-prime crisis grew; and a general flight to \"safe havens\", away from the perceived risks of emerging markets. The rand depreciation was exacerbated by the Eskom electricity crisis, which arose from the utility's inability to meet the country's rapidly growing energy demands.\n\n\n==== 2012\u2013present ====\nA stalled mining industry in late 2012 led to new lows in early 2013. In late January 2014, the rand slid to R11.25 to the dollar, with analysts attributing the shift to \"word from the US Federal Reserve that it would trim back stimulus spending, which led to a massive sell-off in emerging economies.\" In 2014, South Africa experienced its worst year against the US dollar since 2009, and in March 2015, the rand traded at its worst since 2002. At the time, Trading Economics released data that the rand \"averaged R4.97 to the dollar between 1972\u20132015, reaching an all time high of R12.45 in December 2001 and a record low of R0.67 in June of 1973.\" By the end of 2014, the rand had weakened to R15.05 per dollar, partly due to South Africa's consistent trade account deficit with the rest of the world.\nFrom 9\u201313 December 2015, over four days, the rand dropped over 10% due to what some suspected was President Jacob Zuma's surprise announcement that he would be replacing the Finance Minister Nhlanhla Nene with the little-known David van Rooyen. The rapid drop in value stemmed when Zuma backtracked and announced that the better-known previous Minister of Finance, Pravin Gordhan, would instead be appointed to the post. Zuma's surprise sacking of Nene damaged international confidence in the rand, and the exchange rate was volatile throughout much of January 2016 and reached an all-time low of R17.9169 to the US dollar on 9 January 2016 before rebounding to R16.57 later the same day.\nThe January drop in value was also partly caused by Japanese retail investors cutting their losses in the currency to look for higher-yield investments elsewhere and due to concerns over the impact of the economic slowdown in China, South Africa's largest export market.  By mid-January, economists were speculating that the rand could expect to see further volatility for the rest of 2016. By 29 April, it reached its highest performance over the previous five months, exchanging at a rate of R14.16 to the United States dollar.\nFollowing the United Kingdom voting to leave the European Union, the rand dropped in value over 8% against the US$ on 24 June 2016, the currency's largest single-day decline since the 2008 economic crash. This was partly due to a general global financial retreat from currencies seen as risky to the US dollar and partly due to concerns over how British withdrawal from the EU would impact the South African economy and trade relations.\nIn April 2017, a Reuters poll estimated that the rand would remain relatively stable for the rest of the year, as two polls found that analysts had already factored in a possible downgrade to \"junk\" status. At the time, Moody's rated South Africa two notches above junk status. When President Jacob Zuma narrowly won a motion of no confidence in South Africa in August 2017, the rand continued to slide, dropping 1.7% that day. In September 2017, Goldman Sachs said that the debt and corruption of Eskom Holdings was the biggest risk to South Africa's economy and the exchange rate of the rand. At the time, it had no permanent CEO, and Colin Coleman of Goldman Sachs in Africa said the company was \"having discussions on solutions\" on finding credible management. In October 2017, the rand firmed against the US dollar as it recovered from a six-month low. Reuters noted, \"South Africa is highly susceptible to global investor sentiment as the country relies on foreign money to cover its large budget and current account deficits.\" On 13 November 2017, the rand fell by over 1% when the budget chief, Michael Sachs, stood down from his position in Zuma's administration.\nIn October 2022, the rand sank to its lowest point in two years, reaching R18.46 to the US dollar on 25 October 2022.\n\n\n== Coins ==\n\nCoins were introduced in 1961 in denominations of 1\u20442, 1, 2+1\u20442, 5, 10, 20, and 50 cents. In 1965, 2-cent coins replaced the 2+1\u20442 cent coins. The 1\u20442 cent coin was last struck for circulation in 1973. The 1 rand coin for circulation was introduced in 1967, followed by 2 rand coins in 1989 and 5 rand coins in 1994. Production of the 1 and 2-cent coins was discontinued in 2002, followed by 5-cent coins in 2012, primarily due to inflation having devalued them, but they remain legal tender. Shops normally round the total purchase price of goods to the nearest 10 cents.\nTo curb counterfeiting, a new 5-rand coin was released in August 2004. Security features introduced on the coin include a bimetal design (similar to the \u20ac1 and \u20ac2 coins, the Thai \u0e3f10 coin, the pre-2018 Philippine \u20b110 coin, the British \u00a32 coin, and the Canadian $2 coin), a specially serrated security groove along the rim and microlettering.\nOn 3 May 2023, the South African Reserve Bank announced that a new series of coins would be released. These will have the same denominations as the previous series. The 10c will feature an image of the Cape Honey Bee, the 20c the Bitter Aloe, the 50c the Knysna Turaco, the R1 the Springbok, the R2 the King Protea, and the R5 the Southern Right Whale.\n\n\n== Banknotes ==\nThe first series of rand banknotes was introduced in 1961 in denominations of 1, 2, 10, and 20 rand, with similar designs and colours to the preceding pound notes to ease the transition. They bore the image of what was believed at the time to be Jan van Riebeeck, the first VOC administrator of Cape Town. It was later discovered that the original portrait was not, in fact, Van Riebeeck at all, but a portrait of Bartholomew Vermuyden had been mistaken for Van Riebeeck.\nIn 1966, a second series with designs that moved away from the previous pound notes was released. Notes with 1, 5, and 10 rand denominations were produced with predominantly one colour per note. A smaller 1 rand note with the same design was introduced in 1973, and a 2 rand note was introduced in 1974. The 20 rand denomination from the first series was dropped. The practice of having an English and an Afrikaans version of each note was continued in this series.\nThe 1978 series began with denominations of 2, 5, 10, and 20 rand, with a 50 rand introduced in 1984. This series had only one language variant for each denomination of note. Afrikaans was the first language on the 2, 10, and 50 rand, while English was the first on the 5 and 20 rand. A coin replaced the 1 rand note.\n\nIn the 1990s, the notes were redesigned with images of the Big Five wildlife species. 10, 20, and 50 rand notes were introduced in 1992 & 1993, retaining the colour scheme of the previous issue. Coins were introduced for the 2 and 5 rand, replacing the notes of the previous series, mainly because of the severe wear and tear experienced with low-denomination notes in circulation. In 1994, 100 and 200 rand notes were introduced.\nThe 2005 series has the same principal design but with additional security features, such as colour-shifting ink on the 50 rand and higher and the EURion constellation. The obverses of all denominations were printed in English, while two other official languages were printed on the reverse, thus using all 11 official languages of South Africa.\nIn 2010, the South African Reserve Bank and commercial banks withdrew all 1994 series 200-rand banknotes due to relatively high-quality counterfeit notes in circulation.\nIn 2011, the South African Reserve Bank issued defective 100 rand banknotes which lacked fluorescent printing visible under UV light. In June, the printing of this denomination was moved from the South African Bank Note Company to Crane Currency's Swedish division (Tumba Bruk), which reportedly produced 80 million 100 rand notes. The South African Reserve Bank shredded 3.6 million 100-rand banknotes printed by Crane Currency because they had the same serial numbers as a batch printed by the South African Bank Note Company. In addition, the notes printed in Sweden were not the correct colour and were 1mm short.\nOn 11 February 2012, President Jacob Zuma announced that the country would be issuing a new set of banknotes bearing Nelson Mandela's image. They were entered into circulation on 6 November 2012. These contained the same denominations of 10, 20, 50, 100, and 200 rand.\nIn 2013, the 2012 series was updated with the addition of the EURion constellation to all five denominations. They were entered into circulation on 6 November 2013.\nOn 18 July 2018, a special commemorative series of banknotes was released in commemoration of the 100th anniversary of Nelson Mandela's birth. This series includes notes of all denominations, 10, 20, 50, 100, and 200 rand. These notes will circulate alongside the existing notes. The notes depict the standard face of Nelson Mandela on the obverse. Still, instead of the Big Five animals on the reverse, they show a younger Mandela with different iconic scenes relating to his legacy. These scenes comprise the rolling hills of the Eastern Cape, featuring Mandela's humble birthplace of Mvezo (10 rand); the home of Mandela in Soweto, where he defined his political life alongside other struggle icons (20 rand); the site where Mandela was captured near Howick, following 17 months in hiding, where a monument to him has been erected (50 rand); the place of Mandela's 27-year imprisonment at Robben Island, showing a pile of quarried limestone (100 rand); the statue of Mandela at the Union Buildings in remembrance of when he was inaugurated there in 1994 (200 rand).\nOn 3 May 2023, the South African Reserve Bank announced that a new series of banknotes would retain the image of Nelson Mandela on the obverse while showing the Big 5 in a family depiction on the reverse. This series contains the same denominations of 10, 20, 50, 100, and 200 rand.\n\n\n=== First series ===\n\n\n=== Second series ===\n\n\n=== Third series ===\n\n\n=== Fourth series ===\n\n\n=== Fifth series ===\n\n\n=== Sixth series ===\n\n\n=== Seventh series ===\n\n\n=== Eighth series ===\n\n\n=== Exchange rate ===\n\n\n== See also ==\nFinancial rand\nWitwatersrand\nKrugerrand\nCoins of the South African rand\nSouth African pound\nEconomy of South Africa\n\n\n== Note ==\n\n\n== References ==\n\n\n== Further reading ==\n\n\n== External links ==\n\nDecimal Coinage (1962): Newsreel of South Africa's conversion to the Rand, British Path\u00e9\nSouth African Reserve Bank Currency Page Archived 13 April 2020 at the Wayback Machine\nUS Federal Reserve Bank historical exchange rate data\nSouth African Currency Page, with a short description of each note.\nSouth African Currency Page (old rand), a short description of pre-1994 (apartheid-era) notes.\nHistorical banknotes of South Africa (in English and German)\nBank of England exchange rate ZAR vs GBP since 2001\u2013present",
        "unit": "south african rand",
        "url": "https://en.wikipedia.org/wiki/South_African_rand"
    },
    {
        "_id": "Kibibyte",
        "clean": "Kibibyte",
        "text": "The byte is a unit of digital information that most commonly consists of eight bits. Historically, the byte was the number of bits used to encode a single character of text in a computer and for this reason it is the smallest addressable unit of memory in many computer architectures. To disambiguate arbitrarily sized bytes from the common 8-bit definition, network protocol documents such as the Internet Protocol (RFC 791) refer to an 8-bit byte as an octet. Those bits in an octet are usually counted with numbering from 0 to 7 or 7 to 0 depending on the bit endianness.\nThe size of the byte has historically been hardware-dependent and no definitive standards existed that mandated the size. Sizes from 1 to 48 bits have been used. The six-bit character code was an often-used implementation in early encoding systems, and computers using six-bit and nine-bit bytes were common in the 1960s. These systems often had memory words of 12, 18, 24, 30, 36, 48, or 60 bits, corresponding to 2, 3, 4, 5, 6, 8, or 10 six-bit bytes, and persisted, in legacy systems, into the twenty-first century.  In this era, bit groupings in the instruction stream were often referred to as syllables or slab, before the term byte became common.\nThe modern de facto standard of eight bits, as documented in ISO/IEC 2382-1:1993, is a convenient power of two permitting the binary-encoded values 0 through 255 for one byte, as 2 to the power of 8 is 256. The international standard IEC 80000-13 codified this common meaning. Many types of applications use information representable in eight or fewer bits and processor designers commonly optimize for this usage. The popularity of major commercial computing architectures has aided in the ubiquitous acceptance of the 8-bit byte. Modern architectures typically use 32- or 64-bit words, built of four or eight bytes, respectively.\nThe unit symbol for the byte was designated as the upper-case letter B by the International Electrotechnical Commission (IEC) and Institute of Electrical and Electronics Engineers (IEEE). Internationally, the unit octet explicitly defines a sequence of eight bits, eliminating the potential ambiguity of the term \"byte\". The symbol for octet, 'o', also conveniently eliminates the ambiguity in the symbol 'B' between byte and bel.\n\n\n== Etymology and history ==\nThe term byte was coined by Werner Buchholz in June 1956, during the early design phase for the IBM Stretch computer, which had addressing to the bit and variable field length (VFL) instructions with a byte size encoded in the instruction. It is a deliberate respelling of bite to avoid accidental mutation to bit.\nAnother origin of byte for bit groups smaller than a computer's word size, and in particular groups of four bits, is on record by Louis G. Dooley, who claimed he coined the term while working with Jules Schwartz and Dick Beeler on an air defense system called SAGE at MIT Lincoln Laboratory in 1956 or 1957, which was jointly developed by Rand, MIT, and IBM. Later on, Schwartz's language JOVIAL actually used the term, but the author recalled vaguely that it was derived from AN/FSQ-31.\nEarly computers used a variety of four-bit binary-coded decimal (BCD) representations and the six-bit codes for printable graphic patterns common in the U.S. Army (FIELDATA) and Navy. These representations included alphanumeric characters and special graphical symbols. These sets were expanded in 1963 to seven bits of coding, called the American Standard Code for Information Interchange (ASCII) as the Federal Information Processing Standard, which replaced the incompatible teleprinter codes in use by different branches of the U.S. government and universities during the 1960s. ASCII included the distinction of upper- and lowercase alphabets and a set of control characters to facilitate the transmission of written language as well as printing device functions, such as page advance and line feed, and the physical or logical control of data flow over the transmission media. During the early 1960s, while also active in ASCII standardization, IBM simultaneously introduced in its product line of System/360 the eight-bit Extended Binary Coded Decimal Interchange Code (EBCDIC), an expansion of their six-bit binary-coded decimal (BCDIC) representations used in earlier card punches.\nThe prominence of the System/360 led to the ubiquitous adoption of the eight-bit storage size, while in detail the EBCDIC and ASCII encoding schemes are different.\nIn the early 1960s, AT&T introduced digital telephony on long-distance trunk lines. These used the eight-bit \u03bc-law encoding. This large investment promised to reduce transmission costs for eight-bit data.\nIn Volume 1 of The Art of Computer Programming (first published in 1968), Donald Knuth uses byte in his hypothetical MIX computer to denote a unit which \"contains an unspecified amount of information ... capable of holding at least 64 distinct values ... at most 100 distinct values. On a binary computer a byte must therefore be composed of six bits\". He notes that \"Since 1975 or so, the word byte has come to mean a sequence of precisely eight binary digits...When we speak of bytes in connection with MIX we shall confine ourselves to the former sense of the word, harking back to the days when bytes were not yet standardized.\"\nThe development of eight-bit microprocessors in the 1970s popularized this storage size. Microprocessors such as the Intel 8080, the direct predecessor of the 8086, could also perform a small number of operations on the four-bit pairs in a byte, such as the decimal-add-adjust (DAA) instruction. A four-bit quantity is often called a nibble, also nybble, which is conveniently represented by a single hexadecimal digit.\nThe term octet unambiguously specifies a size of eight bits. It is used extensively in protocol definitions.\nHistorically, the term octad or octade was used to denote eight bits as well at least in Western Europe; however, this usage is no longer common. The exact origin of the term is unclear, but it can be found in British, Dutch, and German sources of the 1960s and 1970s, and throughout the documentation of Philips mainframe computers.\n\n\n== Unit symbol ==\nThe unit symbol for the byte is specified in IEC 80000-13, IEEE 1541 and the Metric Interchange Format as the upper-case character B.\nIn the International System of Quantities (ISQ), B is also the symbol of the bel, a unit of logarithmic power ratio named after Alexander Graham Bell, creating a conflict with the IEC specification. However, little danger of confusion exists, because the bel is a rarely used unit. It is used primarily in its decadic fraction, the decibel (dB), for signal strength and sound pressure level measurements, while a unit for one-tenth of a byte, the decibyte, and other fractions, are only used in derived units, such as transmission rates.\nThe lowercase letter o for octet is defined as the symbol for octet in IEC 80000-13 and is commonly used in languages such as French and Romanian, and is also combined with metric prefixes for multiples, for example ko and Mo.\n\n\n== Multiple-byte units ==\n\nMore than one system exists to define unit multiples based on the byte. Some systems are based on powers of 10, following the International System of Units (SI), which defines for example the prefix kilo as 1000 (103); other systems are based on powers of 2. Nomenclature for these systems has led to confusion. Systems based on powers of 10 use standard SI prefixes (kilo, mega, giga, ...) and their corresponding symbols (k, M, G, ...). Systems based on powers of 2, however, might use binary prefixes (kibi, mebi, gibi, ...) and their corresponding symbols (Ki, Mi, Gi, ...) or they might use the prefixes K, M, and G, creating ambiguity when the prefixes M or G are used.\nWhile the difference between the decimal and binary interpretations is relatively small for the kilobyte (about 2% smaller than the kibibyte), the systems deviate increasingly as units grow larger (the relative deviation grows by 2.4% for each three orders of magnitude). For example, a power-of-10-based terabyte is about 9% smaller than power-of-2-based tebibyte.\n\n\n=== Units based on powers of 10 ===\nDefinition of prefixes using powers of 10\u2014in which 1 kilobyte (symbol kB) is defined to equal 1,000 bytes\u2014is recommended by the International Electrotechnical Commission (IEC). The IEC standard defines eight such multiples, up to 1 yottabyte (YB), equal to 10008 bytes. The additional prefixes ronna- for 10009 and quetta- for 100010 were adopted by the International Bureau of Weights and Measures (BIPM) in 2022.\nThis definition is most commonly used for data-rate units in computer networks, internal bus, hard drive and flash media transfer speeds, and for the capacities of most storage media, particularly hard drives, flash-based storage, and DVDs. Operating systems that use this definition include macOS, iOS, Ubuntu, and Debian. It is also consistent with the other uses of the SI prefixes in computing, such as CPU clock speeds or measures of performance.\n\n\n=== Units based on powers of 2 ===\nA system of units based on powers of 2 in which 1 kibibyte (KiB) is equal to 1,024 (i.e., 210) bytes is defined by international standard IEC 80000-13 and is supported by national and international standards bodies (BIPM, IEC, NIST). The IEC standard defines eight such multiples, up to 1 yobibyte (YiB), equal to 10248 bytes. The natural binary counterparts to ronna- and quetta- were given in a consultation paper of the International Committee for Weights and Measures' Consultative Committee for Units (CCU) as robi- (Ri, 10249) and quebi- (Qi, 102410), but have not yet been adopted by the IEC and ISO.\nAn alternative system of nomenclature for the same units (referred to here as the customary convention), in which 1 kilobyte (KB) is equal to 1,024 bytes, 1 megabyte (MB) is equal to 10242 bytes and  1 gigabyte (GB) is equal to 10243 bytes is mentioned by a 1990s JEDEC standard. Only the first three multiples (up to GB) are mentioned by the JEDEC standard, which makes no mention of TB and larger. While confusing and incorrect, the customary convention is used by the Microsoft Windows operating system and random-access memory capacity, such as main memory and CPU cache size, and in marketing and billing by telecommunication companies, such as Vodafone, AT&T, Orange and Telstra.\nFor storage capacity, the customary convention was used by macOS and iOS through Mac OS X 10.6 Snow Leopard and iOS 10, after which they switched to units based on powers of 10.\n\n\n=== Parochial units ===\nVarious computer vendors have coined terms for data of various sizes, sometimes with different sizes for the same term even within a single vendor. These terms include double word, half word, long word, quad word, slab, superword and syllable. There are also informal terms. e.g., half byte and nybble for 4 bits, octal K for 10008.\n\n\n=== History of the conflicting definitions ===\n\nContemporary computer memory has a binary architecture making a definition of memory units based on powers of 2 most practical. The use of the metric prefix kilo for binary multiples arose as a convenience, because 1024 is approximately 1000. This definition was popular in early decades of personal computing, with products like the Tandon 51\u20444-inch DD floppy format (holding 368640 bytes) being advertised as \"360 KB\", following the 1024-byte convention. It was not universal, however. The Shugart SA-400 51\u20444-inch floppy disk held 109,375 bytes unformatted, and was advertised as \"110 Kbyte\", using the 1000 convention. Likewise, the 8-inch DEC RX01 floppy (1975) held 256256 bytes formatted, and was advertised as \"256k\". Some devices were advertised using a mixture of the two definitions: most notably, floppy disks advertised as \"1.44 MB\" have an actual capacity of 1440 KiB, the equivalent of 1.47 MB or 1.41 MiB.\nIn 1995, the International Union of Pure and Applied Chemistry's (IUPAC) Interdivisional Committee on Nomenclature and Symbols attempted to resolve this ambiguity by proposing a set of binary prefixes for the powers of 1024, including kibi (kilobinary), mebi (megabinary), and gibi (gigabinary).\nIn December 1998, the IEC addressed such multiple usages and definitions by adopting the IUPAC's proposed prefixes (kibi, mebi, gibi, etc.) to unambiguously denote powers of 1024. Thus one kibibyte (1 KiB) is 10241 bytes = 1024 bytes, one mebibyte (1 MiB) is 10242 bytes = 1048576 bytes, and so on.\nIn 1999, Donald Knuth suggested calling the kibibyte a \"large kilobyte\" (KKB).\n\n\n=== Modern standard definitions ===\nThe IEC adopted the IUPAC proposal and published the standard in January 1999. The IEC prefixes are part of the International System of Quantities. The IEC further specified that the kilobyte should only be used to refer to 1000 bytes.\n\n\n=== Lawsuits over definition ===\nLawsuits arising from alleged consumer confusion over the binary and decimal definitions of multiples of the byte have generally ended in favor of the manufacturers, with courts holding that the legal definition of gigabyte or GB is 1 GB = 1000000000 (109) bytes (the decimal definition), rather than the binary definition (230, i.e., 1073741824). Specifically, the United States District Court for the Northern District of California held that \"the U.S. Congress has deemed the decimal definition of gigabyte to be the 'preferred' one for the purposes of 'U.S. trade and commerce' [...] The California Legislature has likewise adopted the decimal system for all 'transactions in this state.'\"\nEarlier lawsuits had ended in settlement with no court ruling on the question, such as a lawsuit against drive manufacturer Western Digital. Western Digital settled the challenge and added explicit disclaimers to products that the usable capacity may differ from the advertised capacity. Seagate was sued on similar grounds and also settled.\n\n\n=== Practical examples ===\n\n\n== Common uses ==\nMany programming languages define the data type byte.\nThe C and C++ programming languages define byte as an \"addressable unit of data storage large enough to hold any member of the basic character set of the execution environment\" (clause 3.6 of the C standard). The C standard requires that the integral data type unsigned char must hold at least 256 different values, and is represented by at least eight bits (clause 5.2.4.2.1). Various implementations of C and C++ reserve 8, 9, 16, 32, or 36 bits for the storage of a byte. In addition, the C and C++ standards require that there are no gaps between two bytes. This means every bit in memory is part of a byte.\nJava's primitive data type byte is defined as eight bits. It is a signed data type, holding values from \u2212128 to 127.\n.NET programming languages, such as C#, define byte as an unsigned type, and the sbyte as a signed data type, holding values from 0 to 255, and \u2212128 to 127, respectively.\nIn data transmission systems, the byte is used as a contiguous sequence of bits in a serial data stream, representing the smallest distinguished unit of data. For asynchronous communication a full transmission unit usually additionally includes a start bit, 1 or 2 stop bits, and possibly a parity bit, and thus its size may vary from seven to twelve bits for five to eight bits of actual data. For synchronous communication the error checking usually uses bytes at the end of a frame.\n\n\n== See also ==\nData\nData hierarchy\nNibble\nOctet (computing)\nPrimitive data type\nTryte\nWord (computer architecture)\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Further reading ==\n\"2.5 Byte manipulation\" (PDF). Programming with the PDP-10 Instruction Set (PDF). PDP-10 System Reference Manual. Vol. 1. Digital Equipment Corporation (DEC). August 1969. pp. 2-15\u20132-17. Archived (PDF) from the original on 2017-04-05. Retrieved 2017-04-05.\nAshley Taylor. \"Bits and Bytes\". Stanford. https://web.stanford.edu/class/cs101/bits-bytes.html",
        "unit": "kibibyte",
        "url": "https://en.wikipedia.org/wiki/Kibibyte"
    },
    {
        "_id": "Day",
        "clean": "Day",
        "text": "A day is the time period of a full rotation of the Earth with respect to the Sun.  On average, this is 24 hours (86,400 seconds). As a day passes at a given location it experiences morning, noon, afternoon, evening, and night. This daily cycle drives circadian rhythms in many organisms, which are vital to many life processes.\nA collection of sequential days is organized into calendars as dates, almost always into weeks, months and years. A solar calendar organizes dates based on the Sun's annual cycle, giving consistent start dates for the four seasons from year to year. A lunar calendar organizes dates based on the Moon's lunar phase.\nIn common usage, a day starts at midnight, written as 00:00 or 12:00 am in 24- or 12-hour clocks, respectively. Because the time of midnight varies between locations, time zones are set up to facilitate the use of a uniform standard time.  Other conventions are sometimes used, for example the Jewish religious calendar counts days from sunset to sunset, so the Jewish Sabbath begins at sundown on Friday. In astronomy, a day begins at noon so that observations throughout a single night are recorded as happening on the same day.\nIn specific applications, the definition of a day is slightly modified, such as in the SI day (exactly 86,400 seconds) used for computers and standards keeping, local mean time accounting of the Earth's natural fluctuation of a solar day, and stellar day and sidereal day (using the celestial sphere) used for astronomy.  In most countries outside of the tropics, daylight saving time is practiced, and each year there will be one 23-hour civil day and one 25-hour civil day.  Due to slight variations in the rotation of the Earth, there are rare times when a leap second will get inserted at the end of a UTC day, and so while almost all days have a duration of 86,400 seconds, there are these exceptional cases of a day with 86,401 seconds (in the half-century spanning 1972 through 2022, there have been a total of 27 leap seconds that have been inserted, so roughly once every other year).\n\n\n== Etymology ==\nThe term comes from the Old English term d\u00e6\u0121 (/d\u00e6j/), with its cognates such as dagur in Icelandic, Tag in German, and dag in Norwegian, Danish, Swedish and Dutch \u2013 all stemming from a Proto-Germanic root *dagaz.\n\n\n== Definitions ==\n\n\n=== Apparent and mean solar day ===\n\nSeveral definitions of this universal human concept are used according to context, need, and convenience. Besides the day of 24 hours (86,400 seconds), the word day is used for several different spans of time based on the rotation of the Earth around its axis. An important one is the solar day, the time it takes for the Sun to return to its culmination point (its highest point in the sky). Due to an orbit's eccentricity, the Sun resides in one of the orbit's foci instead of the middle. Consequently, due to Kepler's second law, the planet travels at different speeds at various positions in its orbit, and thus a solar day is not the same length of time throughout the orbital year. Because the Earth moves along an eccentric orbit around the Sun while the Earth spins on an inclined axis, this period can be up to 7.9 seconds more than (or less than) 24 hours. In recent decades, the average length of a solar day on Earth has been about 86,400.002 seconds (24.000 000 6 hours). There are currently about 365.2421875 solar days in one mean tropical year.\nAncient custom has a new day starting at either the rising or setting of the Sun on the local horizon (Italian reckoning, for example, being 24 hours from sunset, old style). The exact moment of, and the interval between, two sunrises or sunsets depends on the geographical position (longitude and latitude, as well as altitude), and the time of year (as indicated by ancient hemispherical sundials).\nA more constant day can be defined by the Sun passing through the local meridian, which happens at local noon (upper culmination) or midnight (lower culmination). The exact moment is dependent on the geographical longitude, and to a lesser extent on the time of the year. The length of such a day is nearly constant (24 hours \u00b1 30 seconds). This is the time as indicated by modern sundials.\nA further improvement defines a fictitious mean Sun that moves with constant speed along the celestial equator; the speed is the same as the average speed of the real Sun, but this removes the variation over a year as the Earth moves along its orbit around the Sun (due to both its velocity and its axial tilt).\nIn terms of Earth's rotation, the average day length is about 360.9856\u00b0. A day lasts for more than 360\u00b0 of rotation because of the Earth's revolution around the Sun.  With a full year being slightly more than 360 days, the Earth's daily orbit around the Sun is slightly less than 1\u00b0, so the day is slightly less than 361\u00b0 of rotation.\nElsewhere in the Solar System or other parts of the universe, a day is a full rotation of other large astronomical objects with respect to its star.\n\n\n==== Civil day ====\nFor civil purposes, a common clock time is typically defined for an entire region based on the local mean solar time at a central meridian. Such  time zones began to be adopted about the middle of the 19th century when railroads with regularly occurring schedules came into use, with most major countries having adopted them by 1929. As of 2015, throughout the world, 40 such zones are now in use: the central zone, from which all others are defined as offsets, is known as UTC+00, which uses Coordinated Universal Time (UTC).\nThe most common convention starts the civil day at midnight: this is near the time of the lower culmination of the Sun on the central meridian of the time zone. Such a day may be called a calendar day.\nA day is commonly divided into 24 hours, with each hour being made up of 60 minutes, and each minute composed of 60 seconds.\n\n\n=== Sidereal day ===\n\nA sidereal day or stellar day is the span of time it takes for the Earth to make one entire rotation with respect to the celestial background or a distant star (assumed to be fixed). Measuring a day as such is used in astronomy. A sidereal day is about 4 minutes less than a solar day of 24 hours (23 hours 56 minutes and 4.09 seconds), or 0.99726968 of a solar day of 24 hours. There are about 366.2422 stellar days in one mean tropical year (one stellar day more than the number of solar days).\nBesides a stellar day on Earth, other bodies in the Solar System have day times, the durations of these being:\n\n\n=== In the International System of Units ===\n\nIn the International System of Units (SI), a day not an official unit, but is accepted for use with SI. A day, with symbol d, is defined using SI units as 86,400 seconds; the second is the base unit of time in SI units. In 1967\u201368, during the 13th CGPM (Resolution 1), the International Bureau of Weights and Measures (BIPM) redefined a second as \"the duration of 9,192,631,770 periods of the radiation corresponding to the transition between two hyperfine levels of the ground state of the caesium-133 atom\". This makes the SI-based day last exactly 794,243,384,928,000 of those periods.\n\n\n=== In decimal and metric time ===\n\nVarious decimal or metric time proposals have been made, but do not redefine the day, and use the day or sidereal day as a base unit. Metric time uses metric prefixes to keep time. It uses the day as the base unit, and smaller units being fractions of a day: a metric hour (deci) is 1\u204410 of a day; a metric minute (milli) is 1\u20441000 of a day; etc. Similarly, in decimal time, the length of a day is static to normal time. A day is also split into 10 hours, and 10 days comprise a d\u00e9cade \u2013 the equivalent of a week. 3 d\u00e9cades make a month.:\u200a35\u200a Various decimal time proposals which do not redefine the day: Henri de Sarrauton's proposal kept days, and subdivided hours into 100 minutes;:\u200a42\u200a in Mendiz\u00e1bal y Tamborel's proposal, the sidereal day was the basic unit, with subdivisions made upon it;:\u200a42\u201343\u200a and Rey-Pailhade's proposal divided the day 100 c\u00e9s.:\u200a42\u200a\n\n\n=== Other definitions ===\nThe word refers to various similarly defined ideas, such as:\n\nFull day\n24 hours (exactly) (a nychthemeron)\nA day counting approximation, for example \"See you in three days.\" or \"the following day\"\nThe full day covering both the dark and light periods, beginning from the start of the dark period or from a point near the middle of the dark period\nA full dark and light period, sometimes called a nychthemeron in English, from the Greek for night-day; or more colloquially the term 24 hours. In other languages, 24 hours is also often used. Other languages also have a separate word for a full day.\nPart of a date: the day of the year (doy) in ordinal dates, day of the month (dom) in calendar dates or day of the week (dow) in week dates.\nTime regularly spend at paid work on a single work day, cf. man-day and workweek.\nDaytime\nThe period of light when the Sun is above the local horizon (that is, the time period from sunrise to sunset)\nThe time period from 06:00\u201318:00 (6:00 am \u2013 6:00 pm) or 21:00 (9:00 pm) or another fixed clock period overlapping or offset from other time periods such as \"morning\", \"afternoon\", or \"evening\".\nThe time period from first-light \"dawn\" to last-light \"dusk\".\nOther\nA specific period of the day, which may vary by context, such as \"the school day\" or \"the work day\".\n\n\n== Variations in length ==\n\nMainly due to tidal deceleration \u2013 the Moon's gravitational pull slowing down the Earth's rotation \u2013 the Earth's rotational period is slowing. Because of the way the second is defined, the mean length of a solar day is now about 86,400.002 seconds, and is increasing by about 2 milliseconds per century.\nSince the rotation rate of the Earth is slowing, the length of a SI second fell out of sync with a second derived from the rotational period. This arose the need for leap seconds, which insert extra seconds into Coordinated Universal Time (UTC). Although typically 86,400 SI seconds in duration, a civil day can be either 86,401 or 86,399 SI seconds long on such a day. Other than the two-millisecond variation from tidal deceleration, other factors minutely affect the day's length, which creates an irregularity in the placement of leap seconds. Leap seconds are announced in advance by the International Earth Rotation and Reference Systems Service (IERS), which measures the Earth's rotation and determines whether a leap second is necessary.\n\n\n=== Geological day lengths ===\nDiscovered by paleontologist John W. Wells, the day lengths of geological periods have been estimated by measuring sedimentation rings in coral fossils, due to some biological systems being affected by the tide. The length of a day at the Earth's formation is estimated at 6 hours. Arbab I. Arbab plotted day lengths over time and found a curved line. Arbab attributed this to the change of water volume present affecting Earth's rotation.\n\n\n== Boundaries ==\n\nFor most diurnal animals, the day naturally begins at dawn and ends at sunset. Humans, with their cultural norms and scientific knowledge, have employed several different conceptions of the day's boundaries.\nIn the Hebrew Bible, Genesis 1:5 defines a day in terms of \"evening\" and \"morning\" before recounting the creation of the Sun to illuminate it: \"And God called the light Day, and the darkness he called Night. And the evening and the morning were the first day.\"\nThe Jewish day begins at either sunset or nightfall (when three second-magnitude stars appear).\nMedieval Europe also followed this tradition, known as Florentine reckoning: In this system, a reference like \"two hours into the day\" meant two hours after sunset and thus times during the evening need to be shifted back one calendar day in modern reckoning.\nDays such as Christmas Eve, Halloween (\u201cAll Hallows\u2019 Eve\u201d), and the Eve of Saint Agnes are remnants of the older pattern when holidays began during the prior evening.\nThe common convention among the ancient Romans, ancient Chinese and in modern times is for the civil day to begin at midnight, i.e. 00:00, and to last a full 24 hours until 24:00, i.e. 00:00 of the next day. The International Meridian Conference of 1884 resolved\n\nThat the Conference expresses the hope that as soon as may be practicable the astronomical and nautical days will be arranged everywhere to begin at midnight.\nIn ancient Egypt the day was reckoned from sunrise to sunrise.\nPrior to 1926, Turkey had two time systems: Turkish, counting the hours from sunset, and French, counting the hours from midnight.\n\n\n== Parts ==\n\nHumans have divided the day in rough periods, which can have cultural implications, and other effects on humans' biological processes. The parts of the day do not have set times; they can vary by lifestyle or hours of daylight in a given place.\n\n\n=== Daytime ===\n\nDaytime is the part of the day during which sunlight directly reaches the ground, assuming that there are no obstacles. The length of daytime averages slightly more than half of the 24-hour day. Two effects make daytime on average longer than night. The Sun is not a point but has an apparent size of about 32 minutes of arc. Additionally, the atmosphere refracts sunlight in such a way that some of it reaches the ground even when the Sun is below the horizon by about 34 minutes of arc. So the first light reaches the ground when the centre of the Sun is still below the horizon by about 50 minutes of arc. Thus, daytime is on average around 7 minutes longer than 12 hours.\nDaytime is further divided into morning, afternoon, and evening. Morning occurs between sunrise and noon. Afternoon occurs between noon and sunset, or between noon and the start of evening. This period of time sees human's highest body temperature, an increase of traffic collisions, and a decrease of productivity. Evening begins around 5 or 6 pm, or when the sun sets, and ends when one goes to bed.\n\n\n=== Twilight ===\n\nTwilight is the period before sunrise and after sunset in which there is natural light but no direct sunlight. The morning twilight begins at dawn and ends at sunrise, while the evening twilight begins at sunset and ends at dusk. Both periods of twilight can be divided into civil twilight, nautical twilight, and astronomical twilight. Civil twilight is when the sun is up to 6 degrees below the horizon; nautical when it is up to 12 degrees below, and astronomical when it is up to 18 degrees below.\n\n\n=== Night ===\n\nNight is the period in which the sky is dark; the period between dusk and dawn when no light from the sun is visible. Light pollution during night can impact human and animal life, for example by disrupting sleep.\n\n\n== See also ==\n\n\n== References ==\n\n\n== External links ==\n Media related to Day at Wikimedia Commons\n The dictionary definition of day at Wiktionary\n Quotations related to Day at Wikiquote",
        "unit": "day",
        "url": "https://en.wikipedia.org/wiki/Day"
    },
    {
        "_id": "Barn_(unit)",
        "clean": "Barn (unit)",
        "text": "A barn (symbol: b) is a metric unit of area equal to 10\u221228 m2 (100 fm2). Originally used in nuclear physics for expressing the cross sectional area of nuclei and nuclear reactions, today it is also used in all fields of high-energy physics to express the cross sections of any scattering process, and is best understood as a measure of the probability of interaction between small particles. A barn is approximately the cross-sectional area of a uranium nucleus. The barn is also the unit of area used in nuclear quadrupole resonance and nuclear magnetic resonance to quantify the interaction of a nucleus with an electric field gradient. While the barn never was an SI unit, the SI standards body acknowledged it in the 8th SI Brochure (superseded in 2019) due to its use in particle physics.\n\n\n== Etymology ==\nDuring Manhattan Project research on the atomic bomb during World War II, American physicists Marshall Holloway and Charles P. Baker were working at Purdue University on a project using a particle accelerator to measure the cross sections of certain nuclear reactions. According to an account of theirs from a couple years later, they were dining in a cafeteria in December 1942 and discussing their work. They \"lamented\" that there was no name for the unit of cross section and challenged themselves to develop one. They initially tried to find the name of \"some great man closely associated with the field\" that they could name the unit after, but struggled to find one that was appropriate. They considered \"Oppenheimer\" too long (in retrospect, they considered an \"Oppy\" to perhaps have been allowable), and considered \"Bethe\" to be too easily confused with the commonly-used Greek letter beta. They then considered naming it after John Manley, another scientist associated with their work, but considered \"Manley\" too long and \"John\" too closely associated with toilets. But this latter association, combined with the \"rural background\" of one of the scientists, suggested to them the term \"barn\", which also worked because the unit was \"really as big as a barn.\" According to the authors, the first published use of the term was in a (secret) Los Alamos report from late June 1943, on which the two originators were co-authors.\n\n\n== Commonly used prefixed versions ==\nThe unit symbol for the barn (b) is also the IEEE standard symbol for bit. In other words, 1 Mb can mean one megabarn or one megabit.\n\n\n== Conversions ==\nCalculated cross sections are often given in terms of inverse squared gigaelectronvolts (GeV\u22122), via the conversion \u01272c2/GeV2 = 0.3894 mb = 38940 am2.\nIn natural units (where \u0127 = c = 1), this simplifies to GeV\u22122 = 0.3894 mb = 38940 am2.\n\n\n=== SI units with prefix ===\nIn SI, one can use units such as square femtometers (fm2). The most common SI prefixed unit for the barn is the femtobarn, which is equal to a tenth of a square zeptometer. Many scientific papers discussing high-energy physics mention quantities of fractions of femtobarn level.\n\n\n== Inverse femtobarn ==\nThe inverse femtobarn (fb\u22121) is the unit typically used to measure the number of particle collision events per femtobarn of target cross-section, and is the conventional unit for time-integrated luminosity. Thus if a detector has accumulated 100 fb\u22121 of integrated luminosity, one expects to find 100 events per femtobarn of cross-section within these data.\nConsider a particle accelerator where two streams of particles, with cross-sectional areas measured in femtobarns, are directed to collide over a period of time. The total number of collisions will be directly proportional to the luminosity of the collisions measured over this time. Therefore, the collision count can be calculated by multiplying the integrated luminosity by the sum of the cross-section for those collision processes. This count is then expressed as inverse femtobarns for the time period (e.g., 100 fb\u22121 in nine months). Inverse femtobarns are often quoted as an indication of particle collider productivity.\nFermilab produced 10 fb\u22121 in the first decade of the 21st century.  Fermilab's Tevatron took about 4 years to reach 1 fb\u22121 in 2005, while two of CERN's LHC experiments, ATLAS and CMS, reached over 5 fb\u22121 of proton\u2013proton data in 2011 alone. In April 2012 the LHC achieved the collision energy of 8 TeV with a luminosity peak of 6760 inverse microbarns per second; by May 2012 the LHC delivered 1 inverse femtobarn of data per week to each detector collaboration. A record of over 23 fb\u22121 was achieved during 2012. As of November 2016, the LHC had achieved 40 fb\u22121 over that year, significantly exceeding the stated goal of 25 fb\u22121. In total, the second run of the LHC has delivered around 150 fb\u22121 to both ATLAS and CMS in 2015\u20132018.\n\n\n=== Usage example ===\nAs a simplified example, if a beamline runs for 8 hours (28 800 seconds) at an instantaneous luminosity of 300\u00d71030 cm\u22122\u22c5s\u22121 = 300 \u03bcb\u22121\u22c5s\u22121, then it will gather data totaling an integrated luminosity of 8640000 \u03bcb\u22121 = 8.64 pb\u22121 = 0.00864 fb\u22121 during this period. If this is multiplied by the cross-section, then a dimensionless number is obtained equal to the number of expected scattering events.\n\n\n== See also ==\n\"Shake\", a unit of time created by the same people at the same time as the barn\nOrders of magnitude (area)\nList of unusual units of measurement\nList of humorous units of measurement\n\n\n== References ==\n\n\n== External links ==\nIUPAC citation for this usage of \"barn\"",
        "unit": "millibarn",
        "url": "https://en.wikipedia.org/wiki/Barn_(unit)"
    },
    {
        "_id": "Fahrenheit",
        "clean": "Fahrenheit",
        "text": "The Fahrenheit scale () is a temperature scale based on one proposed in 1724 by the European physicist Daniel Gabriel Fahrenheit (1686\u20131736). It uses the degree Fahrenheit (symbol: \u00b0F) as the unit. Several accounts of how he originally defined his scale exist, but the original paper suggests the lower defining point, 0 \u00b0F, was established as the freezing temperature of a solution of brine made from a mixture of water, ice, and ammonium chloride (a salt). The other limit established was his best estimate of the average human body temperature, originally set at 90 \u00b0F, then 96 \u00b0F (about 2.6 \u00b0F less than the modern value due to a later redefinition of the scale).\nFor much of the 20th century, the Fahrenheit scale was defined by two fixed points with a 180 \u00b0F separation: the temperature at which pure water freezes was defined as 32 \u00b0F and the boiling point of water was defined to be 212 \u00b0F, both at sea level and under standard atmospheric pressure. It is now formally defined using the Kelvin scale.\nIt continues to be used in the United States (including its unincorporated territories), its freely associated states in the Western Pacific (Palau, the Federated States of Micronesia and the Marshall Islands), the Cayman Islands, and Liberia.\nFahrenheit is commonly still used alongside the Celsius scale in other countries that use the U.S. metrological service, such as Antigua and Barbuda, Saint Kitts and Nevis, the Bahamas, and Belize. A handful of British Overseas Territories, including the Virgin Islands, Montserrat, Anguilla, and Bermuda, also still use both scales. All other countries now use Celsius (\"centigrade\" until 1948), which was invented 18 years after the Fahrenheit scale.\n\n\n== Definition and conversion ==\n\nHistorically, on the Fahrenheit scale the freezing point of water was 32 \u00b0F, and the boiling point was 212 \u00b0F (at standard atmospheric pressure). This put the boiling and freezing points of water 180 degrees apart. Therefore, a degree on the Fahrenheit scale was 1\u2044180 of the interval between the freezing point and the boiling point. On the Celsius scale, the freezing and boiling points of water were originally defined to be 100 degrees apart. A temperature interval of 1 \u00b0F was equal to an interval of 5\u20449 degrees Celsius. With the Fahrenheit and Celsius scales now both defined by the kelvin, this relationship was preserved, a temperature interval of 1 \u00b0F being equal to an interval of 5\u20449 K and of 5\u20449 \u00b0C. The Fahrenheit and Celsius scales intersect numerically at \u221240 in the respective unit (i.e., \u221240 \u00b0F \u2258 \u221240 \u00b0C).\nAbsolute zero is 0 K, \u2212273.15 \u00b0C, or \u2212459.67 \u00b0F. The Rankine temperature scale uses degree intervals of the same size as those of the Fahrenheit scale, except that absolute zero is 0 \u00b0R \u2013  the same way that the Kelvin temperature scale matches the Celsius scale, except that absolute zero is 0 K.\nThe combination of degree symbol (\u00b0) followed by an uppercase letter F is the conventional symbol for the Fahrenheit temperature scale. A number followed by this symbol (and separated from it with a space) denotes a specific temperature point (e.g., \"Gallium melts at 85.5763 \u00b0F\"). A difference between temperatures or an uncertainty in temperature is also conventionally written the same way as well, e.g., \"The output of the heat exchanger experiences an increase of 72 \u00b0F\" or \"Our standard uncertainty is \u00b15 \u00b0F\". However, some authors instead use the notation \"An increase of 50 F\u00b0\" (reversing the symbol order) to indicate temperature differences. Similar conventions exist for the Celsius scale, see Celsius \u00a7 Temperatures and intervals.\n\n\n=== Conversion (specific temperature point) ===\nFor an exact conversion between degrees Fahrenheit and Celsius, and kelvins of a specific temperature point, the following formulas can be applied. Here, f is the value in degrees Fahrenheit, c the value in degrees Celsius, and k the value in kelvins:\n\nf \u00b0F to c \u00b0C: c = \u2060f \u2212 32/1.8\u2060\nc \u00b0C to f \u00b0F: f = c \u00d7 1.8 + 32\nf \u00b0F to k K: k = \u2060f + 459.67/1.8\u2060\nk K to f \u00b0F: f = k \u00d7 1.8 \u2212 459.67\nThere is also an exact conversion between Celsius and Fahrenheit scales making use of the correspondence \u221240 \u00b0F \u2258 \u221240 \u00b0C. Again, f is the numeric value in degrees Fahrenheit, and c the numeric value in degrees Celsius:\n\nf \u00b0F to c \u00b0C: c = \u2060f + 40/1.8\u2060 \u2212 40\nc \u00b0C to f \u00b0F: f = (c + 40) \u00d7 1.8 \u2212 40\n\n\n=== Conversion (temperature difference or interval) ===\nWhen converting a temperature interval between the Fahrenheit and Celsius scales, only the ratio is used, without any constant (in this case, the interval has the same numeric value in kelvins as in degrees Celsius):\n\nf \u00b0F to c \u00b0C or k K: c = k = \u2060f/1.8\u2060\nc \u00b0C or k K to f \u00b0F: f = c \u00d7 1.8 = k \u00d7 1.8\n\n\n== History ==\nFahrenheit proposed his temperature scale in 1724, basing it on two reference points of temperature. In his initial scale (which is not the final Fahrenheit scale), the zero point was determined by placing the thermometer in \"a mixture of ice, water, and salis Armoniaci [transl. ammonium chloride] or even sea salt\". This combination forms a eutectic system, which stabilizes its temperature automatically: 0 \u00b0F was defined to be that stable temperature. A second point, 96 degrees, was approximately the human body's temperature. A third point, 32 degrees, was marked as being the temperature of ice and water \"without the aforementioned salts\".\nAccording to a German story, Fahrenheit actually chose the lowest air temperature measured in his hometown Danzig (Gda\u0144sk, Poland) in winter 1708\u201309 as 0 \u00b0F, and only later had the need to be able to make this value reproducible using brine.\nAccording to a letter Fahrenheit wrote to his friend Herman Boerhaave, his scale was built on the work of Ole R\u00f8mer, whom he had met earlier. In R\u00f8mer scale, brine freezes at zero, water freezes and melts at 7.5 degrees, body temperature is 22.5, and water boils at 60 degrees. Fahrenheit multiplied each value by 4 in order to eliminate fractions and make the scale more fine-grained. He then re-calibrated his scale using the melting point of ice and normal human body temperature (which were at 30 and 90 degrees); he adjusted the scale so that the melting point of ice would be 32 degrees, and body temperature 96 degrees, so that 64 intervals would separate the two, allowing him to mark degree lines on his instruments by simply bisecting the interval 6 times (since 64 = 26).\nFahrenheit soon after observed that water boils at about 212 degrees using this scale. The use of the freezing and boiling points of water as thermometer fixed reference points became popular following the work of Anders Celsius, and these fixed points were adopted by a committee of the Royal Society led by Henry Cavendish in 1776\u201377. Under this system, the Fahrenheit scale is redefined slightly so that the freezing point of water was exactly 32 \u00b0F, and the boiling point was exactly 212 \u00b0F, or 180 degrees higher. It is for this reason that normal human body temperature is approximately 98.6 \u00b0F (oral temperature) on the revised scale (whereas it was 90\u00b0 on Fahrenheit's multiplication of R\u00f8mer, and 96\u00b0 on his original scale).\nIn the present-day Fahrenheit scale, 0 \u00b0F no longer corresponds to the eutectic temperature of ammonium chloride brine as described above. Instead, that eutectic is at approximately 4 \u00b0F on the final Fahrenheit scale.\nThe Rankine temperature scale was based upon the Fahrenheit temperature scale, with its zero representing absolute zero instead.\n\n\n== Usage ==\n\n\n=== General ===\n\nThe Fahrenheit scale was the primary temperature standard for climatic, industrial and medical purposes in Anglophone countries until the 1960s. In the late 1960s and 1970s, the Celsius scale replaced Fahrenheit in almost all of those countries\u2014with the notable exception of the United States.\nFahrenheit is used in the United States, its territories and associated states (all serviced by the U.S. National Weather Service), as well as the (British) Cayman Islands and Liberia for everyday applications. The Fahrenheit scale is in use in U.S. for all temperature measurements including weather forecasts, cooking, and food freezing temperatures, however for scientific research the scale is Celsius and Kelvin.\n\n\n=== United States ===\nEarly in the 20th century, Halsey and Dale suggested that reasons for resistance to use the centigrade (now Celsius) system in the U.S. included the larger size of each degree Celsius and the lower zero point in the Fahrenheit system; the Fahrenheit scale is supposedly more intuitive than Celsius for describing outdoor temperatures in temperate latitudes, with 100 \u00b0F being a hot summer day and 0 \u00b0F a cold winter day.\n\n\n=== Canada ===\nCanada has passed legislation favoring the International System of Units, while also maintaining legal definitions for traditional Canadian imperial units. Canadian weather reports are conveyed using degrees Celsius with occasional reference to Fahrenheit especially for cross-border broadcasts. Fahrenheit is still used on virtually all Canadian ovens. Thermometers, both digital and analog, sold in Canada usually employ both the Celsius and Fahrenheit scales.\n\n\n=== European Union ===\nIn the European Union, it is mandatory to use Kelvins or degrees Celsius when quoting temperature for \"economic, public health, public safety and administrative\" purposes, though degrees Fahrenheit may be used alongside degrees Celsius as a supplementary unit.\n\n\n=== United Kingdom ===\nMost British people use Celsius. However, the use of Fahrenheit still may appear at times alongside degrees Celsius in the print media with no standard convention for when the measurement is included.\nFor example, The Times has an all-metric daily weather page but includes a Celsius-to-Fahrenheit conversion table. Some UK tabloids have adopted a tendency of using Fahrenheit for mid to high temperatures. It has been suggested that the rationale to keep using Fahrenheit was one of emphasis for high temperatures: \"\u22126 \u00b0C\" sounds colder than \"21 \u00b0F\", and \"94 \u00b0F\" sounds more sensational than \"34 \u00b0C\".\n\n\n== Unicode representation of symbol ==\nUnicode provides the Fahrenheit symbol at code point U+2109 \u2109 DEGREE FAHRENHEIT. However, this is a compatibility character encoded for roundtrip compatibility with legacy encodings. The Unicode standard explicitly discourages the use of this character: \"The sequence U+00B0 \u00b0 DEGREE SIGN + U+0046 F LATIN CAPITAL LETTER F is preferred over U+2109 \u2109 DEGREE FAHRENHEIT, and those two sequences should be treated as identical for searching.\"\n\n\n== See also ==\n\nOutline of metrology and measurement\nComparison of temperature scales\nDegree of frost\n\n\n== Notes ==\n\n\n== References ==\n\n\n== External links ==\n Media related to Fahrenheit temperature at Wikimedia Commons\n The dictionary definition of fahrenheit at Wiktionary\nDaniel Gabriel Fahrenheit (Polish-born Dutch physicist) \u2013 Encyclop\u00e6dia Britannica\n\"At Auction | One of Only Three Original Fahrenheit Thermometers\" Enfilade page for 2012 Christie's sale of a Fahrenheit mercury thermometer\nChristie's press release\n\"SI Units - Temperature\". NIST. National Institute of Standards and Technology (US Department of Commerce). 15 November 2019. Retrieved 25 February 2020.",
        "unit": "degree fahrenheit",
        "url": "https://en.wikipedia.org/wiki/Fahrenheit"
    },
    {
        "_id": "Barn_(unit)",
        "clean": "Barn (unit)",
        "text": "A barn (symbol: b) is a metric unit of area equal to 10\u221228 m2 (100 fm2). Originally used in nuclear physics for expressing the cross sectional area of nuclei and nuclear reactions, today it is also used in all fields of high-energy physics to express the cross sections of any scattering process, and is best understood as a measure of the probability of interaction between small particles. A barn is approximately the cross-sectional area of a uranium nucleus. The barn is also the unit of area used in nuclear quadrupole resonance and nuclear magnetic resonance to quantify the interaction of a nucleus with an electric field gradient. While the barn never was an SI unit, the SI standards body acknowledged it in the 8th SI Brochure (superseded in 2019) due to its use in particle physics.\n\n\n== Etymology ==\nDuring Manhattan Project research on the atomic bomb during World War II, American physicists Marshall Holloway and Charles P. Baker were working at Purdue University on a project using a particle accelerator to measure the cross sections of certain nuclear reactions. According to an account of theirs from a couple years later, they were dining in a cafeteria in December 1942 and discussing their work. They \"lamented\" that there was no name for the unit of cross section and challenged themselves to develop one. They initially tried to find the name of \"some great man closely associated with the field\" that they could name the unit after, but struggled to find one that was appropriate. They considered \"Oppenheimer\" too long (in retrospect, they considered an \"Oppy\" to perhaps have been allowable), and considered \"Bethe\" to be too easily confused with the commonly-used Greek letter beta. They then considered naming it after John Manley, another scientist associated with their work, but considered \"Manley\" too long and \"John\" too closely associated with toilets. But this latter association, combined with the \"rural background\" of one of the scientists, suggested to them the term \"barn\", which also worked because the unit was \"really as big as a barn.\" According to the authors, the first published use of the term was in a (secret) Los Alamos report from late June 1943, on which the two originators were co-authors.\n\n\n== Commonly used prefixed versions ==\nThe unit symbol for the barn (b) is also the IEEE standard symbol for bit. In other words, 1 Mb can mean one megabarn or one megabit.\n\n\n== Conversions ==\nCalculated cross sections are often given in terms of inverse squared gigaelectronvolts (GeV\u22122), via the conversion \u01272c2/GeV2 = 0.3894 mb = 38940 am2.\nIn natural units (where \u0127 = c = 1), this simplifies to GeV\u22122 = 0.3894 mb = 38940 am2.\n\n\n=== SI units with prefix ===\nIn SI, one can use units such as square femtometers (fm2). The most common SI prefixed unit for the barn is the femtobarn, which is equal to a tenth of a square zeptometer. Many scientific papers discussing high-energy physics mention quantities of fractions of femtobarn level.\n\n\n== Inverse femtobarn ==\nThe inverse femtobarn (fb\u22121) is the unit typically used to measure the number of particle collision events per femtobarn of target cross-section, and is the conventional unit for time-integrated luminosity. Thus if a detector has accumulated 100 fb\u22121 of integrated luminosity, one expects to find 100 events per femtobarn of cross-section within these data.\nConsider a particle accelerator where two streams of particles, with cross-sectional areas measured in femtobarns, are directed to collide over a period of time. The total number of collisions will be directly proportional to the luminosity of the collisions measured over this time. Therefore, the collision count can be calculated by multiplying the integrated luminosity by the sum of the cross-section for those collision processes. This count is then expressed as inverse femtobarns for the time period (e.g., 100 fb\u22121 in nine months). Inverse femtobarns are often quoted as an indication of particle collider productivity.\nFermilab produced 10 fb\u22121 in the first decade of the 21st century.  Fermilab's Tevatron took about 4 years to reach 1 fb\u22121 in 2005, while two of CERN's LHC experiments, ATLAS and CMS, reached over 5 fb\u22121 of proton\u2013proton data in 2011 alone. In April 2012 the LHC achieved the collision energy of 8 TeV with a luminosity peak of 6760 inverse microbarns per second; by May 2012 the LHC delivered 1 inverse femtobarn of data per week to each detector collaboration. A record of over 23 fb\u22121 was achieved during 2012. As of November 2016, the LHC had achieved 40 fb\u22121 over that year, significantly exceeding the stated goal of 25 fb\u22121. In total, the second run of the LHC has delivered around 150 fb\u22121 to both ATLAS and CMS in 2015\u20132018.\n\n\n=== Usage example ===\nAs a simplified example, if a beamline runs for 8 hours (28 800 seconds) at an instantaneous luminosity of 300\u00d71030 cm\u22122\u22c5s\u22121 = 300 \u03bcb\u22121\u22c5s\u22121, then it will gather data totaling an integrated luminosity of 8640000 \u03bcb\u22121 = 8.64 pb\u22121 = 0.00864 fb\u22121 during this period. If this is multiplied by the cross-section, then a dimensionless number is obtained equal to the number of expected scattering events.\n\n\n== See also ==\n\"Shake\", a unit of time created by the same people at the same time as the barn\nOrders of magnitude (area)\nList of unusual units of measurement\nList of humorous units of measurement\n\n\n== References ==\n\n\n== External links ==\nIUPAC citation for this usage of \"barn\"",
        "unit": "megabarn",
        "url": "https://en.wikipedia.org/wiki/Barn_(unit)"
    },
    {
        "_id": "Volume",
        "clean": "Volume",
        "text": "Volume is a measure of regions in three-dimensional space. It is often quantified numerically using SI derived units (such as the cubic metre and litre) or by various imperial or US customary units (such as the gallon, quart, cubic inch). The definition of length and height (cubed) is interrelated with volume. The volume of a container is generally understood to be the capacity of the container; i.e., the amount of fluid (gas or liquid) that the container could hold, rather than the amount of space the container itself displaces. \nBy metonymy, the term \"volume\" sometimes is used to refer to the corresponding region (e.g., bounding volume).\nIn ancient times, volume was measured using similar-shaped natural containers. Later on, standardized containers were used. Some simple three-dimensional shapes can have their volume easily calculated using arithmetic formulas. Volumes of more complicated shapes can be calculated with integral calculus if a formula exists for the shape's boundary. Zero-, one- and two-dimensional objects have no volume; in four and higher dimensions, an analogous concept to the normal volume is the hypervolume.\n\n\n== History ==\n\n\n=== Ancient history ===\n\nThe precision of volume measurements in the ancient period usually ranges between 10\u201350 mL (0.3\u20132 US fl oz; 0.4\u20132 imp fl oz).:\u200a8\u200a The earliest evidence of volume calculation came from ancient Egypt and Mesopotamia as mathematical problems, approximating volume of simple shapes such as cuboids, cylinders, frustum and cones. These math problems have been written in the Moscow Mathematical Papyrus (c. 1820 BCE).:\u200a403\u200a In the Reisner Papyrus, ancient Egyptians have written concrete units of volume for grain and liquids, as well as a table of length, width, depth, and volume for blocks of material.:\u200a116\u200a The Egyptians use their units of length (the cubit, palm, digit) to devise their units of volume, such as the volume cubit:\u200a117\u200a or deny:\u200a396\u200a (1 cubit \u00d7 1 cubit \u00d7 1 cubit), volume palm (1 cubit \u00d7 1 cubit \u00d7 1 palm), and volume digit (1 cubit \u00d7 1 cubit \u00d7 1 digit).:\u200a117\u200a\nThe last three books of Euclid's Elements, written in around 300 BCE, detailed the exact formulas for calculating the volume of parallelepipeds, cones, pyramids, cylinders, and spheres. The formula were determined by prior mathematicians by using a primitive form of integration, by breaking the shapes into smaller and simpler pieces.:\u200a403\u200a A century later, Archimedes (c.\u2009287 \u2013 212 BCE) devised approximate volume formula of several shapes using the method of exhaustion approach, meaning to derive solutions from previous known formulas from similar shapes. Primitive integration of shapes was also discovered independently by Liu Hui in the 3rd century CE, Zu Chongzhi in the 5th century CE, the Middle East and India.:\u200a404\u200a\nArchimedes also devised a way to calculate the volume of an irregular object, by submerging it underwater and measure the difference between the initial and final water volume. The water volume difference is the volume of the object.:\u200a404\u200a Though highly popularized, Archimedes probably does not submerge the golden crown to find its volume, and thus its density and purity, due to the extreme precision involved. Instead, he likely have devised a primitive form of a hydrostatic balance. Here, the crown and a chunk of pure gold with a similar weight are put on both ends of a weighing scale submerged underwater, which will tip accordingly due to the Archimedes' principle.\n\n\n=== Calculus and standardization of units ===\n\nIn the Middle Ages, many units for measuring volume were made, such as the sester, amber, coomb, and seam. The sheer quantity of such units motivated British kings to standardize them, culminated in the Assize of Bread and Ale statute in 1258 by Henry III of England. The statute standardized weight, length and volume as well as introduced the peny, ounce, pound, gallon and bushel.:\u200a73\u201374\u200a In 1618, the London Pharmacopoeia (medicine compound catalog) adopted the Roman gallon or congius as a basic unit of volume and gave a conversion table to the apothecaries' units of weight. Around this time, volume measurements are becoming more precise and the uncertainty is narrowed to between 1\u20135 mL (0.03\u20130.2 US fl oz; 0.04\u20130.2 imp fl oz).:\u200a8\u200a\nAround the early 17th century, Bonaventura Cavalieri applied the philosophy of modern integral calculus to calculate the volume of any object. He devised Cavalieri's principle, which said that using thinner and thinner slices of the shape would make the resulting volume more and more accurate. This idea would then be later expanded by Pierre de Fermat, John Wallis, Isaac Barrow, James Gregory, Isaac Newton, Gottfried Wilhelm Leibniz and Maria Gaetana Agnesi in the 17th and 18th centuries to form the modern integral calculus, which remains in use in the 21st century.:\u200a404\u200a\n\n\n=== Metrication and redefinitions ===\n\nOn 7 April 1795, the metric system was formally defined in French law using six units. Three of these are related to volume: the st\u00e8re (1 m3) for volume of firewood; the litre (1 dm3) for volumes of liquid; and the gramme, for mass\u2014defined as the mass of one cubic centimetre of water at the temperature of melting ice. Thirty years later in 1824, the imperial gallon was defined to be the volume occupied by ten pounds of water at 17 \u00b0C (62 \u00b0F).:\u200a394\u200a This definition was further refined until the United Kingdom's Weights and Measures Act 1985, which makes 1 imperial gallon precisely equal to 4.54609 litres with no use of water.\nThe 1960 redefinition of the metre from the International Prototype Metre to the orange-red emission line of krypton-86 atoms unbounded the metre, cubic metre, and litre from physical objects. This also make the metre and metre-derived units of volume resilient to changes to the International Prototype Metre. The definition of the metre was redefined again in 1983 to use the speed of light and second (which is derived from the caesium standard) and reworded for clarity in 2019.\n\n\n== Properties ==\n\nAs a measure of the Euclidean three-dimensional space, volume cannot be physically measured as a negative value, similar to length and area. Like all continuous monotonic (order-preserving) measures, volumes of bodies can be compared against each other and thus can be ordered. Volume can also be added together and be decomposed indefinitely; the latter property is integral to Cavalieri's principle and to the infinitesimal calculus of three-dimensional bodies. A 'unit' of infinitesimally small volume in integral calculus is the volume element; this formulation is useful when working with different coordinate systems, spaces and manifolds.\n\n\n== Measurement ==\nThe oldest way to roughly measure a volume of an object is using the human body, such as using hand size and pinches. However, the human body's variations make it extremely unreliable. A better way to measure volume is to use roughly consistent and durable containers found in nature, such as gourds, sheep or pig stomachs, and bladders. Later on, as metallurgy and glass production improved, small volumes nowadays are usually measured using standardized human-made containers.:\u200a393\u200a This method is common for measuring small volume of fluids or granular materials, by using a multiple or fraction of the container. For granular materials, the container is shaken or leveled off to form a roughly flat surface. This method is not the most accurate way to measure volume but is often used to measure cooking ingredients.:\u200a399\u200a\nAir displacement pipette is used in biology and biochemistry to measure volume of fluids at the microscopic scale. Calibrated measuring cups and spoons are adequate for cooking and daily life applications, however, they are not precise enough for laboratories. There, volume of liquids is measured using graduated cylinders, pipettes and volumetric flasks. The largest of such calibrated containers are petroleum storage tanks, some can hold up to 1,000,000 bbl (160,000,000 L) of fluids.:\u200a399\u200a Even at this scale, by knowing petroleum's density and temperature, very precise volume measurement in these tanks can still be made.:\u200a403\u200a\nFor even larger volumes such as in a reservoir, the container's volume is modeled by shapes and calculated using mathematics.:\u200a403\u200a\n\n\n=== Units ===\n\nTo ease calculations, a unit of volume is equal to the volume occupied by a unit cube (with a side length of one). Because the volume occupies three dimensions, if the metre (m) is chosen as a unit of length, the corresponding unit of volume is the cubic metre (m3). The cubic metre is also a SI derived unit. Therefore, volume has a unit dimension of L3.\nThe metric units of volume uses metric prefixes, strictly in powers of ten. When applying prefixes to units of volume, which are expressed in units of length cubed, the cube operators are applied to the unit of length including the prefix. An example of converting cubic centimetre to cubic metre is: 2.3 cm3 = 2.3 (cm)3 = 2.3 (0.01 m)3 = 0.0000023 m3 (five zeros).:\u200a143\u200a\nCommonly used prefixes for cubed length units are the cubic millimetre (mm3), cubic centimetre (cm3), cubic decimetre (dm3), cubic metre (m3) and the cubic kilometre (km3). The conversion between the prefix units are as follows: 1000 mm3 = 1 cm3, 1000 cm3 = 1 dm3, and 1000 dm3 = 1 m3. The metric system also includes the litre (L) as a unit of volume, where 1 L = 1 dm3 = 1000 cm3 = 0.001 m3.:\u200a145\u200a For the litre unit, the commonly used prefixes are the millilitre (mL), centilitre (cL), and the litre (L), with 1000 mL = 1 L, 10 mL = 1 cL, 10 cL = 1 dL, and 10 dL = 1 L.\nVarious other imperial or U.S. customary units of volume are also in use, including::\u200a396\u2013398\u200a\n\ncubic inch, cubic foot, cubic yard, acre-foot, cubic mile;\nminim, drachm, fluid ounce, pint;\nteaspoon, tablespoon;\ngill, quart, gallon, barrel;\ncord, peck, bushel, hogshead.\n\n\n=== Capacity and volume ===\nCapacity is the maximum amount of material that a container can hold, measured in volume or weight. However, the contained volume does not need to fill towards the container's capacity, or vice versa. Containers can only hold a specific amount of physical volume, not weight (excluding practical concerns). For example, a 50,000 bbl (7,900,000 L) tank that can just hold 7,200 t (15,900,000 lb) of fuel oil will not be able to contain the same 7,200 t (15,900,000 lb) of naphtha, due to naphtha's lower density and thus larger volume.:\u200a390\u2013391\u200a\n\n\n== Computation ==\n\n\n=== Basic shapes ===\n\nFor many shapes such as the cube, cuboid and cylinder, they have an essentially the same volume calculation formula as one for the prism: the base of the shape multiplied by its height.\n\n\n=== Integral calculus ===\n\nThe calculation of volume is a vital part of integral calculus. One of which is calculating the volume of solids of revolution, by rotating a plane curve around a line on the same plane. The washer or disc integration method is used when integrating by an axis parallel to the axis of rotation. The general equation can be written as:\n  \n    \n      \n        V\n        =\n        \u03c0\n        \n          \u222b\n          \n            a\n          \n          \n            b\n          \n        \n        \n          |\n          \n            f\n            (\n            x\n            \n              )\n              \n                2\n              \n            \n            \u2212\n            g\n            (\n            x\n            \n              )\n              \n                2\n              \n            \n          \n          |\n        \n        \n        d\n        x\n      \n    \n    {\\displaystyle V=\\pi \\int _{a}^{b}\\left|f(x)^{2}-g(x)^{2}\\right|\\,dx}\n  \nwhere \n  \n    \n      \n        f\n        (\n        x\n        )\n      \n    \n    {\\textstyle f(x)}\n  \n and \n  \n    \n      \n        g\n        (\n        x\n        )\n      \n    \n    {\\textstyle g(x)}\n  \n are the plane curve boundaries.:\u200a1,\u200a3\u200a The shell integration method is used when integrating by an axis perpendicular to the axis of rotation. The equation can be written as::\u200a6\u200a\n  \n    \n      \n        V\n        =\n        2\n        \u03c0\n        \n          \u222b\n          \n            a\n          \n          \n            b\n          \n        \n        x\n        \n          |\n        \n        f\n        (\n        x\n        )\n        \u2212\n        g\n        (\n        x\n        )\n        \n          |\n        \n        \n        d\n        x\n      \n    \n    {\\displaystyle V=2\\pi \\int _{a}^{b}x|f(x)-g(x)|\\,dx}\n  \n The volume of a region D in three-dimensional space is given by the triple or volume integral of the constant function \n  \n    \n      \n        f\n        (\n        x\n        ,\n        y\n        ,\n        z\n        )\n        =\n        1\n      \n    \n    {\\displaystyle f(x,y,z)=1}\n  \n over the region. It is usually written as::\u200aSection 14.4\u200a\n\n  \n    \n      \n        \n          \u222d\n          \n            D\n          \n        \n        1\n        \n        d\n        x\n        \n        d\n        y\n        \n        d\n        z\n        .\n      \n    \n    {\\displaystyle \\iiint _{D}1\\,dx\\,dy\\,dz.}\n  \n\nIn cylindrical coordinates, the volume integral is\n\n  \n    \n      \n        \n          \u222d\n          \n            D\n          \n        \n        r\n        \n        d\n        r\n        \n        d\n        \u03b8\n        \n        d\n        z\n        ,\n      \n    \n    {\\displaystyle \\iiint _{D}r\\,dr\\,d\\theta \\,dz,}\n  \n\nIn spherical coordinates (using the convention for angles with \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n  \n as the azimuth and \n  \n    \n      \n        \u03c6\n      \n    \n    {\\displaystyle \\varphi }\n  \n measured from the polar axis; see more on conventions), the volume integral is\n\n  \n    \n      \n        \n          \u222d\n          \n            D\n          \n        \n        \n          \u03c1\n          \n            2\n          \n        \n        sin\n        \u2061\n        \u03c6\n        \n        d\n        \u03c1\n        \n        d\n        \u03b8\n        \n        d\n        \u03c6\n        .\n      \n    \n    {\\displaystyle \\iiint _{D}\\rho ^{2}\\sin \\varphi \\,d\\rho \\,d\\theta \\,d\\varphi .}\n  \n\n\n=== Geometric modeling ===\n\nA polygon mesh is a representation of the object's surface, using polygons. The volume mesh explicitly define its volume and surface properties.\n\n\n== Derived quantities ==\n\nDensity is the substance's mass per unit volume, or total mass divided by total volume.\nSpecific volume is total volume divided by mass, or the inverse of density.\nThe volumetric flow rate or discharge is the volume of fluid which passes through a given surface per unit time.\nThe volumetric heat capacity is the heat capacity of the substance divided by its volume.\n\n\n== See also ==\nBanach\u2013Tarski paradox\nDimensional weight\nDimensioning\n\n\n== Notes ==\n\n\n== References ==\n\n\n== External links ==\n\n Perimeters, Areas, Volumes at Wikibooks\n Volume at Wikibooks",
        "unit": "volume",
        "url": "https://en.wikipedia.org/wiki/Volume"
    },
    {
        "_id": "Dime_(United_States_coin)",
        "clean": "Dime (United States coin)",
        "text": "The dime, in United States usage, is a ten-cent coin, one tenth of a United States dollar, labeled formally as \"one dime\". The denomination was first authorized by the Coinage Act of 1792.\nThe dime is the smallest in diameter and is the thinnest of all U.S. coins currently minted for circulation, being 0.705 inches (17.91 millimeters) in diameter and 0.053 in (1.35 mm) in thickness. The obverse of the current dime depicts the profile of President Franklin D. Roosevelt and the reverse has an olive branch, a torch, and an oak branch, from left to right respectively.\nThe word dime comes from the Old French disme (Modern French d\u00eeme), meaning \"tithe\" or \"tenth part\", from the Latin decima [pars]. The dime is currently the only United States coin in general circulation that is not denominated in terms of dollars or cents. As of 2011, the dime cost 5.65 cents to produce.\n\n\n== History ==\nThe Coinage Act of 1792 established the dime (spelled \"disme\" in the legislation), cent, and mill as subdivisions of the dollar equal to 1\u204410, 1\u2044100 and 1\u20441000 dollar respectively.\nThe first known proposal for a decimal-based coinage system in the United States was made in 1783 by Thomas Jefferson, Benjamin Franklin, Alexander Hamilton, and David Rittenhouse. Hamilton, the nation's first Secretary of the Treasury, recommended the issuance of six such coins in 1791, in a report to Congress. Among the six was a silver coin, \"which shall be, in weight and value, one-tenth part of a silver unit or dollar\".\nFrom 1796 to 1837, dimes were composed of 89.24% silver and 10.76% copper, the value of which required the coins to be physically very small to prevent their commodity value from being worth more than face value. Thus dimes are made small and thin. The silver percentage was increased to 90.0% with the introduction of the Seated Liberty dime; the use of a richer alloy was offset by reducing the diameter from 18.8 millimeters (0.740 inches) to its current figure of 17.9 millimeters (0.705 inches).\nWith the passage of the Coinage Act of 1965, the dime's silver content was removed. Dimes from 1965 to the present are struck from a clad metal composed of outer layers of 75% copper and 25% nickel alloy, bonded to a pure copper core.  Pre-1965 dimes followed Gresham's law and vanished from ordinary currency circulation at face value.  Most now trade as informal bullion coins known as junk silver, priced at some multiple of face value, which price follows the spot price of silver on commodity markets.\nStarting in 1992, the U.S. Mint began issuing Silver Proof Sets annually, which contain dimes composed of the pre-1965 standard of 90% silver and 10% copper, then switched to .999 fine silver from 2019 onward. These sets are intended solely for collectors and are not meant for general circulation.\n\n\n== Design history ==\nSince its introduction in 1796, the dime has been issued in six different major types, excluding the 1792 \"disme\". The name for each type (except for the Barber dime) indicates the design on the coin's obverse.\n\nDraped Bust 1796\u20131807\nCapped Bust 1809\u20131837\nSeated Liberty 1837\u20131891\nBarber 1892\u20131916\nWinged Liberty Head (Mercury) 1916\u20131945\nRoosevelt 1946\u2013present\n\n\n=== \"Disme\" (1792) ===\n\nThe Coinage Act of 1792, passed on April 2, 1792, authorized the mintage of a \"disme\", one-tenth the silver weight and value of a dollar. The composition of the disme was set at 89.24% silver and 10.76% copper. In 1792, a limited number of dismes were minted but never circulated. Some of these were struck in copper, indicating that the 1792 dismes were in fact pattern coins. The first dimes minted for circulation did not appear until 1796, due to a lack of demand for the coin and production problems at the United States Mint.\n\n\n=== Draped Bust (1796\u20131807) ===\n\nThe first dime to be circulated was the Draped Bust dime, in 1796. It featured the same obverse and reverse as all other circulating coins of the time, the so-called Draped Bust/Small Eagle design. This design was the work of then-Chief Engraver Robert Scot. The portrait of Liberty on the obverse was based on a Gilbert Stuart drawing of prominent Philadelphia socialite Ann Willing Bingham, wife of noted American statesman William Bingham. The reverse design is of a small bald eagle surrounded by palm and olive branches, and perched on a cloud. Since the Coinage Act of 1792 required only that the cent and half cent display their denomination, Draped Bust dimes were minted with no indication of their value.\nAll 1796 dimes have 15 stars on the obverse, representing the number of U.S. states then in the Union. The first 1797 dimes were minted with 16 stars, reflecting Tennessee's admission as the 16th state. Realizing that the practice of adding one star per state could quickly clutter the coin's design, U.S. Mint Director Elias Boudinot ordered a design alteration, to feature just 13 stars (for the original Thirteen Colonies). Therefore, 1797 dimes can be found with either 13 or 16 stars.\nAlso designed by Robert Scot, the Heraldic Eagle reverse design made its debut in 1798. The obverse continued from the previous series, but the eagle on the reverse was changed from the widely criticized \"scrawny\" hatchling to a scaled-down version of the Great Seal of the United States. The Draped Bust/Heraldic Eagles series continued through 1807 (although no dimes dated 1799 or 1806 were minted). Both Draped Bust designs were composed of 89.24% silver and 10.76% copper.\n\n\n=== Capped Bust (1809\u20131837) ===\n\nThe Draped Bust design was succeeded by the Capped Bust, designed by Mint Assistant Engraver John Reich. Both the obverse and reverse were changed extensively. The new reverse featured a bald eagle grasping three arrows (symbolizing strength) and an olive branch (symbolizing peace). Covering the eagle's breast is a U.S. shield with six horizontal lines and 13 vertical stripes. Also on the reverse is the lettering \"10C,\" making it the only dime minted with the value given in cents (subsequent issues are inscribed with the words \"ONE DIME\"). The lack of numeric value markings on subsequent dime coins causes some confusion amongst foreign visitors, who may be unaware of the value of the coin. Also, the Capped Bust dime was the first dime to have its value written on the coin. Previous designs of the dime had no indication of its value, the way people determined its value was by its size \nCapped Bust dimes minted through 1828 are known as the Large type. This is partially because they were struck without a restraining collar, which gave them a broader appearance. In 1828, Chief Engraver William Kneass introduced the close collar method of coining (which automated the process of placing reeds on a coin's edge). In addition to standardizing the diameter of coins, the new method allowed the Mint to produce thicker coins. To maintain a standard weight and alloy, the diameter of most coins was reduced. In particular, the dime was reduced in diameter from 18.8 to 18.5 millimeters. This new Capped Bust dime, which began production in 1828, is known as the Small type. There are 123 varieties known of Capped Bust Dimes.\n\n\n=== Seated Liberty (1837\u20131891) ===\n\nChristian Gobrecht completed the design of the Seated Liberty dime, whose obverse was used with every circulating silver U.S. coin of the period. Mint Director Robert Maskell Patterson requested a new coin design, to be reminiscent of the Britannia image found on coinage of the United Kingdom. Chief Engraver William Kneass drew the original sketches, but suffered a stroke and was too ill to finish them or to oversee preparation of the dies. The task then fell to Gobrecht, who was promoted to Second Engraver.\nThe obverse features an image of Liberty sitting on a rock, wearing a dress and holding a staff with a liberty cap on top. Her right hand is balancing a shield with the inscription \"LIBERTY.\" The reverse featured the inscription \"ONE DIME,\" surrounded by a wreath. All Seated Liberty dimes contain 90% silver and 10% copper, and are 17.9 millimeters (0.705 inch) in diameter. This size and metal composition would continue until 1965, when silver was permanently removed from circulating dimes.\nThere were several minor varieties during the Seated Liberty's run. The initial design (1837) had no stars on the obverse and, further, the dates were minted in a Large Date and Small Date variety. These two types can be distinguished by noting the \"3\" and the \"7\" in the date. In the Large Date variety, the \"3\" has a pointed serif at top, and the horizontal element of the \"7\" is straight. In the Small Date variety, the \"3\" has a rounded serif, and there is small a knob, or bulge, in the \"7\" horizontal element. Only the Philadelphia Mint made both varieties. The Small Date is slightly rarer. The New Orleans Mint also made the Seated Liberty Dime in this year, but only in the Small Date variety.\nThirteen stars (symbolizing the 13 original colonies) were added to the perimeter of the obverse in 1838. These were replaced with the legend \"United States of America,\" which was moved from the reverse in mid-1860. At the same time, the laurel wreath on the reverse was changed to a wreath of corn, wheat, maple, and oak leaves and expanded nearly to the rim of the coin. This reverse design continued through the end of the series in 1891 and was changed only slightly in 1892, when the Barber dime debuted. Another variety is the 1838\u201340 dime minted with no drapery underneath the left elbow of Liberty.\n\nArrows at the date in 1853 and 1873 indicated changes made in the coin's mass (from 2.67 grams to 2.49 grams in 1853, then to 2.50 grams in 1873). The first change was made in response to rising silver prices, while the latter alteration was brought about by the Mint Act of 1873 which, in an attempt to make U.S. coinage the currency of the world, added a small amount of mass to the dime, quarter, and half-dollar to bring their weights in line with fractions of the French 5-franc piece. The change also ensured the quarter dollar (which is valued 2.5 times the dime) weighed 2.5 times the dime (6.25g), and the half dollar (twice the value of the quarter dollar) weighed twice what the quarter dollar weighed (12.5g). In this way, a specific weight of these coins, no matter the mixture of denominations, would always be worth the same. This relation in weight and value continued in the cupronickel coins from 1965 on.\nThis produced the greatest rarities in the Seated Dime Series, the 1873 and 1874 Carson City Dimes, with arrows and the unique 1873 Carson City Dime without arrows.\n\n\n=== Barber (1892\u20131916) ===\n\nThe Barber dime is named for its designer, Charles E. Barber, who was Chief Engraver of the U.S. Mint from 1879 to 1917. The design was shared with the quarter and half-dollar of the same period. Extensive internal politics surrounded the awarding of the design job, which had initially been opened to the public. A four-member committee (which included Barber), appointed by then-Mint Director James Kimball, accorded only two of more than 300 submissions an honorable mention. Kimball's successor, Edward O. Leech, decided to dispense with the committees and public design competitions and simply instructed Barber to develop a new design. It has been speculated that this is what Barber had wanted all along.\nThe Barber dime, as with all previous dimes, featured an image of Liberty on the obverse. She is wearing a Phrygian cap, a laurel wreath with a ribbon, and a headband with the inscription \"LIBERTY\". This inscription is one of the key elements used in determining the condition of Barber dimes. Liberty's portrait was inspired by two sources\u2014French coins and medals of the period, as well as ancient Greek and Roman sculpture. The obverse also contains the long-used 13 stars (for the 13 colonies) design element. The reverse contained a wreath and inscription almost identical to the one used on the final design of the Seated Liberty dime. Dimes were produced at all four of the mints that operated during the period. While circulated coins of the entire series are readily available to collectors there is one outstanding rarity, the 1894-S Barber Dime. Twenty-four were minted, with 9 currently known.\n\n\n=== Winged Liberty Head (\"Mercury\") (1916\u20131945) ===\n\nAlthough most commonly referred to as the \"Mercury\" dime, the Winged Liberty Head does not depict the Roman messenger god. The obverse figure is a depiction of the mythological goddess Liberty wearing a Phrygian cap, a classic Western symbol of liberty and freedom, with its wings intended to symbolize freedom of thought. Designed by noted sculptor Adolph A. Weinman, the Winged Liberty Head dime is considered by many to be one of the most beautiful U.S. coin designs ever produced. The composition (90% silver, 10% copper) and diameter (17.9 millimeters) of the \"Mercury\" dime was unchanged from the Barber dime.\nWeinman (who had studied under Augustus Saint-Gaudens) won a 1915 competition against two other artists for the design job, and is thought to have modeled his version of Liberty on Elsie Kachel Stevens, wife of noted poet Wallace Stevens. The reverse design, a fasces juxtaposed with an olive branch, was intended to symbolize America's readiness for war, combined with its desire for peace. Although the fasces was later officially adopted by Benito Mussolini and his National Fascist Party, the symbol was also common in American iconography and has generally avoided any stigma associated with its usage in wartime Italy.\n\n\n=== Franklin D. Roosevelt (1946\u2013present) ===\n\nSoon after the death of President Franklin D. Roosevelt in April 1945, legislation was introduced by Virginia Congressman Ralph H. Daughton that called for the replacement of the Mercury dime with one bearing Roosevelt's image. The dime was chosen to honor Roosevelt partly due to his efforts in the founding of the National Foundation for Infantile Paralysis (later renamed the March of Dimes), which originally raised money for polio research and to aid victims of the disease and their families.\nDue to the limited amount of time available to design the new coin, the Roosevelt dime was the first regular-issue U.S. coin designed by a Mint employee in more than 40 years. Chief Engraver John R. Sinnock was chosen, as he had already designed a Mint presidential medal of Roosevelt. Sinnock's first design, submitted on October 12, 1945, was rejected, but a subsequent one was accepted on January 6, 1946. The dime was released to the public on January 30, 1946, which would have been Roosevelt's 64th birthday. Sinnock's design placed his initials (\"JS\") at the base of Roosevelt's neck, on the coin's obverse. His reverse design elements of a torch, olive branch, and oak branch symbolized, respectively, liberty, peace, and strength.\nControversy immediately ensued, as strong anti-Communist sentiment in the United States led to the circulation of rumors that the \"JS\" engraved on the coin was the initials of Joseph Stalin, placed there by a Soviet agent in the mint. The Mint quickly issued a statement denying this, confirming that the initials were indeed Sinnock's. The same rumor arose after the release of the Sinnock designed Franklin half dollar in 1948.\n\nAnother controversy surrounding Sinnock's design involves his image of Roosevelt. Soon after the coin's release, it was claimed that Sinnock borrowed his design of Roosevelt from a bas relief created by African American sculptor Selma Burke, unveiled at the Recorder of Deeds Building in Washington, D.C. in September 1945. Sinnock denied this and stated that he simply utilized his earlier design on the Roosevelt medal.\nWith the passage of the Coinage Act of 1965, the composition of the dime changed from 90% silver and 10% copper to a clad \"sandwich\" of pure copper inner layer between two outer layers of cupronickel (75% copper, 25% nickel) alloy giving a total composition of 91.67% Cu and 8.33% Ni. This composition was selected because it gave similar mass (now 2.268 grams instead of 2.5 grams) and electrical properties (important in vending machines)\u2014and most importantly, because it contained no precious metal.\nThe Roosevelt dime has been minted every year, beginning in 1946. Through 1955, all three mints, Philadelphia, Denver, and San Francisco produced circulating coinage; production at San Francisco ended in 1955, resuming in 1968 with proof coinage only. Through 1964 \"D\" and \"S\" mintmarks can be found to the left of the torch. From 1968, the mintmarks have appeared above the date. None were used in 1965\u201367, and Philadelphia did not show a mintmark until 1980 (in 1982, an error left the \"P\" off a small number of dimes, which are now valuable). To commemorate the 50th anniversary of the design, the 1996 mint sets included a \"W\" mintmarked dime made at the West Point Mint. A total of 1,457,000 dimes were issued in the sets, making it the lowest mintage Roosevelt dime up to that time. Since then, the \"P\" mint mark 2015 reverse proof dime and \"W\" mint mark 2015 proof dime, minted at Philadelphia and West Point for inclusion in the March of Dimes collector set, have the lowest mintages with 75,000 pieces struck for each.\n\n\n== See also ==\n\n1792 half disme\nBrother, Can You Spare a Dime?, a popular song of the Great Depression\nDime store, also known as a \"five and dime\"\nDime novel, later known as dime store novel\nMarch of Dimes\n\"Stop on a dime\"\nUnited States Mint coin production\n\n\n== References ==\n\n\n== External links ==\n\nOfficial specifications for all U.S. legal tender coins Archived 2009-11-11 at the Wayback Machine",
        "unit": "dime",
        "url": "https://en.wikipedia.org/wiki/Dime_(United_States_coin)"
    },
    {
        "_id": "Lumber",
        "clean": "Lumber",
        "text": "Lumber is wood that has been processed into uniform and useful sizes (dimensional lumber), including beams and planks or boards. Lumber is mainly used for construction framing, as well as finishing (floors, wall panels, window frames). Lumber has many uses beyond home building. Lumber is referred to as timber in the United Kingdom, Europe, Australia, and New Zealand, while in other parts of the world (mainly the United States and Canada) the term timber refers specifically to unprocessed wood fiber, such as cut logs or standing trees that have yet to be cut.\nLumber may be supplied either rough-sawn, or surfaced on one or more of its faces. Rough lumber is the raw material for furniture-making, and manufacture of other items requiring cutting and shaping. It is available in many species, including hardwoods and softwoods, such as white pine and red pine, because of their low cost.\nFinished lumber is supplied in standard sizes, mostly for the construction industry \u2013 primarily softwood, from coniferous species, including pine, fir and spruce (collectively spruce-pine-fir), cedar, and hemlock, but also some hardwood, for high-grade flooring. It is more commonly made from softwood than hardwoods, and 80% of lumber comes from softwood.\n\n\n== Terminology ==\nIn the United States and Canada, milled boards are called lumber, while timber describes standing or felled trees.\nIn contrast, in Britain, and some other Commonwealth nations and Ireland, the term timber is used in both senses. (In the UK, the word lumber is rarely used in relation to wood and has several other meanings.)\n\n\n=== Re-manufactured lumber ===\n\nRe-manufactured lumber is the result of secondary or tertiary processing of previously milled lumber. Specifically, it refers to lumber cut for industrial or wood-packaging use. Lumber is cut by ripsaw or resaw to create dimensions that are not usually processed by a primary sawmill.\nRe-sawing is the splitting of 1-to-12-inch (25\u2013305 mm) hardwood or softwood lumber into two or more thinner pieces of full-length boards. For example, splitting a 10-foot-long (3.0 m) 2\u00d74 (1+1\u20442 by 3+1\u20442 in or 38 by 89 mm) into two 1\u00d74s (3\u20444 by 3+1\u20442 in or 19 by 89 mm) of the same length is considered re-sawing.\n\n\n=== Plastic lumber ===\n\nStructural lumber may also be produced from recycled plastic and new plastic stock. Its introduction has been strongly opposed by the forestry industry. Blending fiberglass in plastic lumber enhances its strength, durability, and fire resistance. Plastic fiberglass structural lumber can have a \"class 1 flame spread rating of 25 or less, when tested in accordance with ASTM standard E 84,\" which means it burns more slowly than almost all treated wood lumber.\n\n\n=== Timber mark ===\nA timber mark is a code beaten on to cut wood by a specially made hammer to show the logging licence.\n\n\n== History ==\nThe basic understanding of lumber, or \"sawn planks\", came about in North America in the seventeenth century.\nLumber is the most common and widely used method of sawing logs. Plain sawn lumber is produced by making the first cut on a tangent to the circumference of the log. Each additional cut is then made parallel to the one before. This method produces the widest possible boards with the least amount of log waste.\nLumber manufacturing globally is determined by the preferred style of building; areas with a \"wood building culture\" (homes were built from wood rather than other materials like brick) are the countries with significant sawmilling industries. Historical wood-frame home building regions are: Europe, North America, Japan. Different areas of the world are recognized as significant timber suppliers; however, these areas (Indonesia, Sarawak, New Guinea, etc.) are exporters of raw logs and do not have a significant domestic lumber producing industry.\nThe largest lumber manufacturing regions in the world are: China (18%); United States (17%); Canada (10%); Russia (9%); Germany (5%); Sweden (4%).\nIn early periods of society, to make wood for building, the trunks of trees were split with wedges into as many and as thin pieces as possible. If it was necessary to have them still thinner, they were hewn, by some sharp instrument, on both sides, to the proper size. This simple but wasteful manner of making boards is still continued in some places.\nOtherwise, logs were sawn using a two-person whipsaw, or pit-saw, using saddleblocks to hold the log, and a pit for the pitman who worked below.\nIn 1420 the island of Madeira \u2013  an archipelago comprising four islands off the northwest coast of Africa and an autonomous region of Portugal \u2013  was discovered. King Henry VI sent settlers to Madeira and the settlers started clearing the huge expanses of forest in order to grow crops. Felled trees were made into planks by water-powered mills and the timber (cedar and yew) was shipped to Portugal and Spain. About 1427, the first sawmill in Germany was built.\nCornelis Corneliszoon (or Krelis Lootjes) was a Dutch windmill owner from Uitgeest who invented the first mechanical sawmill, which was wind-powered, on 15 December 1593. This made the conversion of log timber into planks 30 times faster than previously.\nThe circular saw, as used in modern sawmills, was invented by an Englishman named Miller in 1777. It was not until the nineteenth century, however, that it was generally applied, and its great work belongs to that period. The first insertable teeth for this saw were invented by W. Kendal, an American, in 1826.\nLogging in the American colonies began in 1607 when the Jamestown settlers cut timber to build the first settlement in the new world. America's first sawmill was built at the Falls of Piscatauqua, on the line between the Province of Maine and the Province of New Hampshire, in 1634. Unauthenticated records, however, claim that as early as 1633 several mills were operating in New Netherland.\nThe American colonies were essential to England in the role of supplier of lumber for the British fleet. By the 1790s, New England was exporting 36 million feet of pine boards and at least 300 ship masts per year to the British Empire. The timber supply began to dwindle at the start of the twentieth century due to significant harvest volumes, so the logging industry was forced to seek timber elsewhere; hence, the expansion into the American West.\n\n\n== Conversion of wood logs ==\n\nLogs are converted into lumber by being sawn, hewn, or split. Sawing with a rip saw is the most common method, because sawing allows logs of lower quality, with irregular grain and large knots, to be used and is more economical. There are various types of sawing:\n\nPlain sawn (flat sawn, through and through, bastard sawn) \u2013 A log sawn through without adjusting the position of the log and the grain runs across the width of the boards.\nQuarter sawn and rift sawn \u2013 These terms have been confused in history but generally mean lumber sawn so the annual rings are reasonably perpendicular to the sides (not edges) of the lumber.\nBoxed heart \u2013 The pith remains within the timber, post or beam, with some allowance for exposure.\nHeart center \u2013 the center core of a log.\nFree of heart center (FOHC) \u2013 A side-cut timber, post or beam without any pith.\nFree of knots (FOK) \u2013 No knots are present.\n\n\n== Dimensional lumber ==\n\nDimensional lumber is lumber that is cut to standardized width and depth, often specified in millimetres or inches (but see below for information on nominal dimensions vs. actual dimensions). Carpenters extensively use dimensional lumber in framing wooden buildings. Common sizes include 2\u00d74 (pictured) (also two-by-four and other variants, such as four-by-two in Australia, New Zealand, and the UK), 2\u00d76, and 4\u00d74. The length of a board is usually specified separately from the width and depth. It is thus possible to find 2\u00d74s that are four, eight, and twelve feet in length. In Canada and the United States, the standard lengths of lumber are 6, 8, 10, 12, 14, 16, 18, 20, 22 and 24 feet (1.8, 2.4, 3.0, 3.7, 4.3, 4.9, 5.5, 6.1, 6.7 and 7.3 m). For wall framing, precut \"stud\" lengths are available, and are commonly used. For ceilings heights of 8, 9 or 10 feet (2.4, 2.7 or 3.0 m), studs are available in 92+5\u20448 inches (2.35 m), 104+5\u20448 inches (2.66 m), and 116+5\u20448 inches (2.96 m).\n\n\n=== North American softwoods ===\nThe length of a unit of dimensional lumber is limited by the height and girth of the tree it is milled from. In general the maximum length is 24 ft (7.32 m). Engineered wood products, manufactured by binding the strands, particles, fibers, or veneers of wood, together with adhesives, to form composite materials, offer more flexibility and greater structural strength than typical wood building materials.\nPre-cut studs save a framer much time, because they are pre-cut by the manufacturer for use in 8-, 9-, and 10-foot ceiling applications, which means the manufacturer has removed a few inches or centimetres of the piece to allow for the sill plate and the double top plate with no additional sizing necessary.\nIn the Americas, two-bys (2\u00d74s, 2\u00d76s, 2\u00d78s, 2\u00d710s, and 2\u00d712s), named for traditional board thickness in inches, along with the 4\u00d74 (89 mm \u00d7 89 mm), are common lumber sizes used in modern construction. They are the basic building blocks for such common structures as balloon-frame or platform-frame housing. Dimensional lumber made from softwood is typically used for construction, while hardwood boards are more commonly used for making cabinets or furniture.\nLumber's nominal dimensions are larger than the actual standard dimensions of finished lumber. Historically, the nominal dimensions were the size of the green (not dried), rough (unfinished) boards that eventually became smaller finished lumber through drying and planing (to smooth the wood). Today, the standards specify the final finished dimensions and the mill cuts the logs to whatever size it needs to achieve those final dimensions. Typically, that rough cut is smaller than the nominal dimensions because modern technology makes it possible to use the logs more efficiently. For example, a \"2\u00d74\" board historically started out as a green, rough board actually 2 by 4 inches (51 mm \u00d7 102 mm). After drying and planing, it would be smaller by a nonstandard amount. Today, a \"2\u00d74\" board starts out as something smaller than 2 inches by 4 inches and not specified by standards, and after drying and planing is minimally 1+1\u20442 by 3+1\u20442 inches (38 mm \u00d7 89 mm).\n\nAs previously noted, less wood is needed to produce a given finished size than when standards called for the green lumber to be the full nominal dimension. However, even the dimensions for finished lumber of a given nominal size have changed over time. In 1910, a typical finished 1-inch (25 mm) board was 13\u204416 in (21 mm). In 1928, that was reduced by 4%, and yet again by 4% in 1956. In 1961, at a meeting in Scottsdale, Arizona, the Committee on Grade Simplification and Standardization agreed to what is now the current U.S. standard: in part, the dressed size of a 1-inch (nominal) board was fixed at 3\u20444 inch; while the dressed size of 2 inch (nominal) lumber was reduced from 1+5\u20448 inch to the current 1+1\u20442 inch.\nDimensional lumber is available in green, unfinished state, and for that kind of lumber, the nominal dimensions are the actual dimensions.\n\n\n=== Grades and standards ===\n\nIndividual pieces of lumber exhibit a wide range in quality and appearance with respect to knots, slope of grain, shakes and other natural characteristics. Therefore, they vary considerably in strength, utility, and value.\nThe move to set national standards for lumber in the United States began with the publication of the American Lumber Standard in 1924, which set specifications for lumber dimensions, grade, and moisture content; it also developed inspection and accreditation programs. These standards have changed over the years to meet the changing needs of manufacturers and distributors, with the goal of keeping lumber competitive with other construction products. Current standards are set by the American Lumber Standard Committee, appointed by the U.S. Secretary of Commerce.\nDesign values for most species and grades of visually graded structural products are determined in accordance with ASTM standards, which consider the effect of strength reducing characteristics, load duration, safety, and other influencing factors. The applicable standards are based on results of tests conducted in cooperation with the USDA Forest Products Laboratory. Design Values for Wood Construction, which is a supplement to the ANSI/AF&PA National Design Specification\u00ae for Wood Construction, provides these lumber design values, which are recognized by the model building codes.\nCanada has grading rules that maintain a standard among mills manufacturing similar woods to assure customers of uniform quality. Grades standardize the quality of lumber at different levels and are based on moisture content, size, and manufacture at the time of grading, shipping, and unloading by the buyer. The National Lumber Grades Authority (NLGA) is responsible for writing, interpreting and maintaining Canadian lumber grading rules and standards. The Canadian Lumber Standards Accreditation Board (CLSAB) monitors the quality of Canada's lumber grading and identification system. Their common grade abbrievation, CLS, Canadian Lumber Standard is well utilised in the construction industry.\nAttempts to maintain lumber quality over time have been challenged by historical changes in the timber resources of the United States \u2013 from the slow-growing virgin forests common over a century ago to the fast-growing plantations now common in today's commercial forests. Resulting declines in lumber quality have been of concern to both the lumber industry and consumers and have caused increased use of alternative construction products.\nMachine stress-rated and machine-evaluated lumber are readily available for end-uses where high strength is critical, such as trusses, rafters, laminating stock, I-beams and web joints. Machine grading measures a characteristic such as stiffness or density that correlates with the structural properties of interest, such as bending strength. The result is a more precise understanding of the strength of each piece of lumber than is possible with visually graded lumber, which allows designers to use full-design strength and avoid overbuilding.\nIn Europe, strength grading of rectangular sawn lumber/timber (both softwood and hardwood) is done according to EN-14081 and commonly sorted into classes defined by EN-338. For softwoods, the common classes are (in increasing strength) C16, C18, C24, and C30. There are also classes specifically for hardwoods and those in most common use (in increasing strength) are D24, D30, D40, D50, D60, and D70. For these classes, the number refers to the required 5th percentile bending strength in newtons per square millimetre. There are other strength classes, including T-classes based on tension intended for use in glulam.\n\nC14, used for scaffolding and formwork\nC16 and C24, general construction\nC30, prefab roof trusses and where design requires somewhat stronger joists than C24 can offer. TR26 is also a common trussed rafter strength class in long standing use in the UK.\nC40, usually seen in glulam\nGrading rules for African and South American sawn lumber have been developed by ATIBT according to the rules of the Sciages Aviv\u00e9s Tropicaux Africains (SATA) and is based on clear cuttings \u2013 established by the percentage of the clear surface.\n\n\n=== North American hardwoods ===\nIn North America, market practices for dimensional lumber made from hardwoods varies significantly from the regularized standardized 'dimension lumber' sizes used for sales and specification of softwoods \u2013 hardwood boards are often sold totally rough cut, or machine planed only on the two (broader) face sides. When hardwood boards are also supplied with planed faces, it is usually both by random widths of a specified thickness (normally matching milling of softwood dimensional lumber) and somewhat random lengths. But besides those older (traditional and normal) situations, in recent years some product lines have been widened to also market boards in standard stock sizes; these usually retail in big-box stores and using only a relatively small set of specified lengths; in all cases hardwoods are sold to the consumer by the board-foot (144 cubic inches or 2,360 cubic centimetres), whereas that measure is not used for softwoods at the retailer (to the cognizance of the buyer).\n\nAlso in North America, hardwood lumber is commonly sold in a \"quarter\" system, when referring to thickness; 4/4 (four quarter) refers to a 1-inch-thick (25 mm) board, 8/4 (eight quarter) is a 2-inch-thick (51 mm) board, etc. This \"quarter\" system is rarely used for softwood lumber; although softwood decking is sometimes sold as 5/4, even though it is actually one inch thick (from milling 1\u20448 in or 3.2 mm off each side in a motorized planing step of production). The \"quarter\" system of reference is a traditional North American lumber industry nomenclature used specifically to indicate the thickness of rough sawn hardwood lumber.\nIn rough-sawn lumber it immediately clarifies that the lumber is not yet milled, avoiding confusion with milled dimension lumber which is measured as actual thickness after machining. Examples \u2013 3\u20444-inch, 19 mm, or 1x. In recent years architects, designers, and builders have begun to use the \"quarter\" system in specifications as a vogue of insider knowledge, though the materials being specified are finished lumber, thus conflating the separate systems and causing confusion.\nHardwoods cut for furniture are cut in the fall and winter, after the sap has stopped running in the trees. If hardwoods are cut in the spring or summer the sap ruins the natural color of the lumber and decreases the value of the wood for furniture.\n\n\n=== Engineered lumber ===\n\nEngineered lumber is lumber created by a manufacturer and designed for a certain structural purpose. The main categories of engineered lumber are:\n\nLaminated veneer lumber (LVL) \u2013 LVL comes in 1+3\u20444-inch (44 mm) thicknesses with depths such as 9+1\u20442, 11+7\u20448, 14, 16, 18 and 24 inches (240, 300, 360, 410, 460 and 610 mm), and are often doubled or tripled up. They function as beams to provide support over large spans, such as removed support walls and garage door openings, places where dimensional lumber is insufficient, and also in areas where a heavy load is bearing from a floor, wall or roof above on a somewhat short span where dimensional lumber is impractical. This type of lumber is compromised if it is altered by holes or notches anywhere within the span or at the ends, but nails can be driven into it wherever necessary to anchor the beam or to add hangers for I-joists or dimensional lumber joists that terminate at an LVL beam.\nWooden I-joists \u2013 sometimes called \"TJI\", \"Trus Joists\" or \"BCI\", all of which are brands of wooden I-joists, they are used for floor joists on upper floors and also in first floor conventional foundation construction on piers as opposed to slab floor construction. They are engineered for long spans and are doubled up in places where a wall will be aligned over them, and sometimes tripled where heavy roof-loaded support walls are placed above them. They consist of a top and bottom chord or flange made from dimensional lumber with a webbing in-between made from oriented strand board (OSB) (or, latterly, steel mesh forms which allow passage of services without cutting). The webbing can be removed up to certain sizes or shapes according to the manufacturer's or engineer's specifications, but for small holes, wooden I-joists come with \"knockouts\", which are perforated, pre-cut areas where holes can be made easily, typically without engineering approval. When large holes are needed, they can typically be made in the webbing only and only in the center third of the span; the top and bottom chords lose their integrity if cut. Sizes and shapes of the hole, and typically the placing of a hole itself, must be approved by an engineer prior to the cutting of the hole and in many areas, a sheet showing the calculations made by the engineer must be provided to the building inspection authorities before the hole will be approved. Some I-joists are made with W-style webbing like a truss to eliminate cutting and to allow ductwork to pass through.\nFinger-jointed lumber \u2013 solid dimensional lumber lengths typically are limited to lengths of 22 to 24 feet (6.7\u20137.3 m), but can be made longer by the technique of \"finger-jointing\" by using small solid pieces, usually 18 to 24 inches (460\u2013610 mm) long, and joining them together using finger joints and glue to produce lengths that can be up to 36 feet (11 m) long in 2\u00d76 size. Finger-jointing also is predominant in precut wall studs. It is also an affordable alternative for non-structural hardwood that will be painted (staining would leave the finger-joints visible). Care is taken during construction to avoid nailing directly into a glued joint as stud breakage can occur.\nGlulam beams \u2013 created from 2\u00d74 or 2\u00d76 stock by gluing the faces together to create beams such as 4\u00d712 or 6\u00d716. As such, a beam acts as one larger piece of lumber \u2013 thus eliminating the need to harvest larger, older trees for the same size beam.\nManufactured trusses \u2013 trusses are used in home construction as a pre-fabricated replacement for roof rafters and ceiling joists (stick-framing). It is seen as an easier installation and a better solution for supporting roofs than the use of dimensional lumber's struts and purlins as bracing. In the southern U.S. and elsewhere, stick-framing with dimensional lumber roof support is still predominant. The main drawbacks of trusses are reduced attic space, time required for engineering and ordering, and a cost higher than the dimensional lumber needed if the same project were conventionally framed. The advantages are significantly reduced labor costs (installation is faster than conventional framing), consistency, and overall schedule savings.\n\n\n=== Various pieces and cuts ===\n\nSquare and rectangular forms: plank, slat, batten, board, lath, strapping (typically 3\u20444 in \u00d7 1+1\u20442 in [19 mm \u00d7 38 mm]), cant (A partially sawn log such as sawn on two sides or squared to a large size and later resawn into lumber. A flitch is a type of cant with wane on one or both sides). Various pieces are also known by their uses such as post, beam, (girt), stud, rafter, joist, sill plate, wall plate.\nRod forms: pole, (dowel), stick (staff, baton)\n\n\n=== Timber piles ===\nIn the United States, pilings are mainly cut from southern yellow pines and Douglas-fir. Treated pilings are available in chromated copper arsenate retentions of 0.60, 0.80 and 2.50 pounds per cubic foot (9.6, 12.8 and 40.0 kg/m3) if treatment is required.\n\n\n=== Historical Chinese construction ===\nUnder the prescription of the Method of Construction (\u71df\u9020\u6cd5\u5f0f) issued by the Song dynasty government in the early twelfth century, timbers were standardized to eight cross-sectional dimensions. Regardless of the actual dimensions of the timber, the ratio between width and height was maintained at 1:1.5. Units are in Song dynasty inches (31.2 mm).\n\nTimber smaller than the 8th class were called \"unclassed\" (\u7b49\u5916). The width of a timber is referred to as one \"timber\" (\u6750), and the dimensions of other structural components were quoted in multiples of \"timber\"; thus, as the width of the actual timber varied, the dimensions of other components were easily calculated, without resorting to specific figures for each scale. The dimensions of timbers in similar applications show a gradual diminution from the Sui dynasty (580\u2013618) to the modern era; a 1st class timber during the Sui was reconstructed as 15\u00d710 (Sui dynasty inches, or 29.4 mm).\n\n\n== Defects in lumber ==\n\nDefects occurring in lumber are grouped into the following four divisions:\n\n\n=== Conversion ===\nDuring the process of converting timber to commercial forms of lumber the following defects may occur:\n\nChip mark: this defect is indicated by the marks or signs placed by chips on the finished surface of timber\nDiagonal grain: improper sawing of timber\nTorn grain: when a small dent is made on the finished surface due to falling of some tool\nWane: presence of original rounded surface in the finished product\n\n\n=== Defects due to fungi and animals ===\nFungi attack wood (both timber and lumber) when these conditions are all present:\n\nThe wood moisture content is above 25% on a dry-weight basis\nThe environment is sufficiently warm\nOxygen (O2) is present\nWood with less than 25% moisture (dry weight basis) can remain free of decay for centuries. Similarly, wood submerged in water may not be attacked by fungi if the amount of oxygen is inadequate.\nFungi lumber/timber defects:\n\nBlue stain\nBrown rot\nDry rot\nHeart rot\nSap stain\nWet rot\nWhite rot\nFollowing are the insects and molluscs which are usually responsible for the decay of timber/lumber:\n\nWoodboring beetles\nMarine borers (Barnea similis)\nTeredos (Teredo navalis)\nTermites\nCarpenter ants\nCarpenter bees\n\n\n=== Natural forces ===\n\nThere are two main natural forces responsible for causing defects in timber and lumber: abnormal growth and rupture of tissues. Rupture of tissue includes cracks or splits in the wood called \"shakes\". \"Ring shake\", \"wind shake\", or \"ring failure\" is when the wood grain separates around the growth rings either while standing or during felling. Shakes may reduce the strength of a timber and the appearance thus reduce lumber grade and may capture moisture, promoting decay. Eastern hemlock is known for having ring shake. A \"check\" is a crack on the surface of the wood caused by the outside of a timber shrinking as it seasons. Checks may extend to the pith and follow the grain. Like shakes, checks can hold water promoting rot. A \"split\" goes all the way through a timber. Checks and splits occur more frequently at the ends of lumber because of the more rapid drying in these locations.\n\n\n=== Seasoning ===\nThe seasoning of lumber is typically either kiln- or air-dried. Defects due to seasoning are the main cause of splits, bowing and honeycombing. Seasoning is the process of drying timber to remove the bound moisture contained in the walls of the wood cells to produce seasoned timber.\n\n\n== Durability and service life ==\nUnder proper conditions, wood provides excellent, lasting performance. However, it also faces several potential threats to service life, including fungal activity and insect damage \u2013 which can be avoided in numerous ways. Section 2304.11 of the International Building Code addresses protection against decay and termites. This section provides requirements for non-residential construction applications, such as wood used above ground (e.g., for framing, decks, stairs, etc.), as well as other applications.\nThere are four recommended methods to protect wood-frame structures against durability hazards and thus provide maximum service life for the building. All require proper design and construction:\n\nControlling moisture using design techniques to avoid decay\nProviding effective control of termites and other insects\nUsing durable materials such as pressure-treated or naturally durable species of wood where appropriate\nProviding quality assurance during design and construction and throughout the building's service life using appropriate maintenance practices\n\n\n=== Moisture control ===\nWood is a hygroscopic material, which means it naturally absorbs and releases water to balance its internal moisture content with the surrounding environment. The moisture content of wood is measured by the weight of water as a percentage of the oven-dry weight of the wood fiber. The key to controlling decay is controlling moisture. Once decay fungi are established, the minimum moisture content for decay to propagate is 22 to 24 percent, so building experts recommend 19 percent as the maximum safe moisture content for untreated wood in service. Water by itself does not harm the wood, but rather, wood with consistently high moisture content enables fungal organisms to grow.\nThe primary objective when addressing moisture loads is to keep water from entering the building envelope in the first place and to balance the moisture content within the building itself. Moisture control by means of accepted design and construction details is a simple and practical method of protecting a wood-frame building against decay. For applications with a high risk of staying wet, designers specify durable materials such as naturally decay-resistant species or wood that has been treated with preservatives. Cladding, shingles, sill plates and exposed timbers or glulam beams are examples of potential applications for treated wood.\n\n\n=== Controlling termites and other insects ===\nFor buildings in termite zones, basic protection practices addressed in current building codes include (but are not limited to) the following:\n\nGrading the building site away from the foundation to provide proper drainage\nCovering exposed ground in any crawl spaces with 6-mil polyethylene film and maintaining at least 12 to 18 inches (300 to 460 mm) of clearance between the ground and the bottom of framing members above (12 inches to beams or girders, 18 inches to joists or plank flooring members)\nSupporting post columns by concrete piers so that there is at least 6 inches (150 mm) of clear space between the wood and exposed earth\nInstalling wood framing and sheathing in exterior walls at least eight inches above exposed earth; locating siding at least six inches from the finished grade\nWhere appropriate, ventilating crawl spaces according to local building codes\nRemoving building material scraps from the job site before backfilling.\nIf allowed by local regulation, treating the soil around the foundation with an approved termiticide to provide protection against subterranean termites\n\n\n=== Preservatives ===\n\nTo avoid decay and termite infestation, untreated wood is separated from the ground and other sources of moisture. These separations are required by many building codes and are considered necessary to maintain wood elements in permanent structures at a safe moisture content for decay protection. When it is not possible to separate wood from the sources of moisture, designers often rely on preservative-treated wood.\nWood can be treated with a preservative that improves service life under severe conditions without altering its basic characteristics. It can also be pressure-impregnated with fire-retardant chemicals that improve its performance in a fire. One of the early treatments to \"fireproof lumber\", which retard fires, was developed in 1936 by the Protexol Corporation, in which lumber is heavily treated with salt. Wood does not deteriorate simply because it gets wet. When wood breaks down, it is because an organism is eating it. Preservatives work by making the food source inedible to these organisms. Properly preservative-treated wood can have 5 to 10 times the service life of untreated wood. Preserved wood is used most often for railroad ties, utility poles, marine piles, decks, fences and other outdoor applications. Various treatment methods and types of chemicals are available, depending on the attributes required in the particular application and the level of protection needed.\nThere are two basic methods of treating: with and without pressure. Non-pressure methods are the application of preservatives by brushing, spraying, or dipping the piece to be treated. Deeper, more thorough penetration is achieved by driving the preservative into the wood cells with pressure. Various combinations of pressure and vacuum are used to force adequate levels of chemical into the wood. Pressure-treating preservatives consist of chemicals carried in a solvent.\nChromated copper arsenate, once the most commonly used wood preservative in North America began being phased out of most residential applications in 2004. Replacing it are amine copper quat and copper azole.\nAll wood preservatives used in the United States and Canada are registered and regularly re-examined for safety by the U.S. Environmental Protection Agency and Health Canada's Pest Management and Regulatory Agency, respectively.\n\n\n== Timber framing ==\n\nTimber framing is a style of construction that uses heavier framing elements (larger posts and beams) than modern stick framing, which uses smaller standard dimensional lumber. The timbers are cut from log boles and squared with a saw, broadaxe or adze, and then joined together with joinery without nails. Modern timber framing has been growing in popularity in the United States since the 1970s.\n\n\n== Environmental effects of lumber ==\nGreen building minimizes the impact or \"environmental footprint\" of a building. Wood is a major building material that is renewable and replenishable in a continuous cycle. Studies show manufacturing wood uses less energy and results in less air and water pollution than steel and concrete. However, demand for lumber is blamed for deforestation.\n\n\n=== Residual wood ===\nThe conversion from coal to biomass power is a growing trend in the United States.\nThe United Kingdom, Uzbekistan, Kazakhstan, Australia, Fiji, Madagascar, Mongolia, Russia, Denmark, Switzerland, and Eswatini governments all support an increased role for energy derived from biomass, which are organic materials available on a renewable basis and include residues and/or byproducts of the logging, saw milling and paper-making processes. In particular, they view it as a way to lower greenhouse gas emissions by reducing the consumption of oil and gas while supporting the growth of forestry, agriculture and rural economies. Studies by the U.S. government have found the country's combined forest and agriculture land resources have the power to sustainably supply more than one-third of its current petroleum consumption.\nBiomass is already an important source of energy for the North American forest products industry. It is common for companies to have cogeneration facilities, also known as combined heat and power, which convert some of the biomass that results from wood and paper manufacturing to electrical and thermal energy in the form of steam. The electricity is used to, among other things, dry lumber and supply heat to the dryers used in paper-making.\n\n\n== Environmental impacts ==\nLumber is a sustainable and environmentally friendly construction material that could replace modern building materials (e.g. concrete and steel) given its structural performance, capacity to fixate CO2 and low energy demand during the manufacturing process.\nSubstituting lumber for concrete or steel avoids the carbon emissions of those materials. Cement and concrete manufacture is responsible for around 8% of global GHG emissions while the iron and steel industry is responsible for another 5% (half a ton of CO2 is emitted to manufacture a ton of concrete; two tons of CO2  are emitted in the manufacture of a ton of steel).\nAdvantages of lumber:\n\nFire performance: In the case of fire, the outer layer of mass timber will tend to char in a predictable way that effectively self-extinguishes and shields the interior, allowing it to retain structural integrity for several hours, even in an intense fire.\nReduction of carbon emissions: Building materials and construction make up 11% of global greenhouse gas emissions. Though the exact amount will depend on tree species, forestry practices, transportation costs, and several other factors, that one cubic meter of lumber sequesters roughly one tonne of CO2.\nNatural insulation: lumber is a natural insulator which makes it particularly good for windows and doors.\nLess construction time, labor costs, and waste: it is easy to manufacture prefabricated lumber, from which pieces can be assembled simultaneously (with relatively little labor). This reduces material waste, avoids massive on-site inventory, and minimizes on-site disruption. According to the softwood lumber industry, \"Mass timber buildings are roughly 25% faster to construct than concrete buildings and require 90% less construction traffic\".\n\n\n== End-of-life ==\nAn EPA study showed the typical end-of-life scenario for wood waste from municipal solid waste (MSW), wood packaging, and other miscellaneous wood products in the US. Based on the 2018 data, about 67% of wood waste was landfilled, 16% incinerated with energy recovery, and 17% recycled.\nA 2020 study conducted by Edinburgh Napier University demonstrated the proportional waste stream of recovered lumber in the UK. The study showed that timber from municipal solid waste and packaging waste made up 13 and 26% of waste collected. Construction and demolition waste made up the biggest bulk of waste collectively at 52%, with the remaining 10% coming from industry.\n\n\n== In the circular economy ==\nThe Ellen MacArthur Foundation defines the circular economy as \"based on the principles of designing out waste and pollution, keeping products and materials in use, and regenerating natural systems.\"\nThe circular economy can be considered as a model that aims to eliminate waste by targeting materials, and products at their maximum value of utility and time. In short, it is a whole new model of production and consumption that ensures sustainable development over time. It is related to the reuse of materials, components, and products over a longer life cycle.\nWood is among the most demanding materials, which makes it important to come up with a model of the circular economy. The lumber industry creates a lot of waste, especially in its manufacturing process. From log debarking to finished products, there are several stages of processing that generate a considerable volume of waste, which includes solid wood waste, harmful gases, and residual water. Therefore, it is important to identify and apply measures to reduce environmental contamination, giving a financial return to the industries (e.g., selling the waste to wood chippings manufacturers) and maintaining a healthy relationship between the environment and industries.\nWood waste can be recycled at its end of life to make new products. Recycled chips can be used to make wood panels, which is beneficial for both the environment and industry. Such practice reduces the use of virgin raw materials, eliminating emissions that would have otherwise been emitted in its manufacturing.\nOne of the studies conducted in Hong Kong was done using life-cycle assessment (LCA). The study aimed to assess and compare the environmental impacts of wood waste management from building construction activities using different alternative management scenarios in Hong Kong. Despite various advantages of lumber and its waste, the contribution to the study of the circular economy of lumber is still very small. Some areas where improvements can be made to improve the circularity of lumber is as follows:\n\nFirst, regulations to support recycled lumber use. For example, establishing grading standards and enforcing penalties for improper disposal, especially in sectors that produce big quantities of wood waste, such as the construction and demolition sector.\nSecond, creating a stronger supply force. This can be achieved by improving demolition protocol and technology and enhancing the secondary raw materials market through circular business models.\nThird, increase demand by introducing incentives to the construction sector and new homeowners to use recycled lumber. This can be in the form of reduced taxes for the construction of the new build.\n\n\n== Secondary raw material ==\nThe term secondary raw material denotes waste material that has been recycled and injected back into use as productive material. Lumber has a high potential to be used as a secondary raw material at various stages, as listed below:\n\nRecovery of branches and leaves for use as fertilisers\nTimber undergo multiple processing stages before lumber of desired shapes, size, and standards are achieved for commercial use. The process generates a lot of waste which in most cases is disregarded. But being an organic waste, the positive aspect of such waste is that it can be used as a fertiliser or to protect the soil in severe weather conditions.\nRecovery of woodchips for thermal energy generation\nWaste generated during the manufacturing of lumber products can be used to produce thermal energy. Lumber products after their end-of-life can be downcycled into chips and be used as biomass to produce thermal energy. It is beneficial for industries that need thermal energy.\nCircular economy practices offer effective solutions concerning waste. It targets its unnecessary generation through waste reduction, reuse, and recycling. There is no clear explicit evidence of circular economy in the wood panel industry. However, based on the circular economy concept and its characteristics, there are opportunities present in the wood panel industry from the raw material extraction phase to its end-of-life. Therefore, there lies a gap yet to be explored.\n\n\n== See also ==\n\n\n== Explanatory notes ==\n\n\n== References ==\n\n\n== Further reading ==\nDavis, Richard C. Encyclopedia of American forest and conservation history (1983) vol 1 online see also 2 online, 871pp. See online review of this book\nSathre, R; O'Conner, J (2010). A Synthesis of Research on Wood Products and Greenhouse Gas Impacts (PDF) (2nd ed.). FPInnovations. ISBN 978-0-86488-546-3. Archived from the original (PDF) on 21 March 2012.\n\n\n== External links ==\n\nNational Hardwood Lumber Association (Rules for Grading Hardwood Lumber \u2013 Inspector Training School)\nTimber Development Association of NSW \u2013 Australia\nTDA: Timber Decking Association \u2013 UK\nTRADA: Timber Research And Development Association Archived 6 March 2010 at the Wayback Machine\nThe Forest Products Laboratory. U.S. main wood products research lab. Madison, WI (E)\nWCTE, World Conference on Timber Engineering Archived 27 January 2010 at the Wayback Machine\u3000June 20\u201324, 2010, Riva del Garda, Trentino, Italy\nForest Products data in Canada since 1990",
        "unit": "volume (lumber)",
        "url": "https://en.wikipedia.org/wiki/Lumber"
    },
    {
        "_id": "Penny",
        "clean": "Penny",
        "text": "A penny is a coin (pl.: pennies) or a unit of currency (pl.: pence) in various countries. Borrowed from the Carolingian denarius (hence its former abbreviation d.), it is usually the smallest denomination within a currency system. At present, it is the formal name of the British penny (abbr. p) and the de facto name of the American one-cent coin (abbr. \u00a2) as well as the informal Irish designation of the 1 euro cent coin (abbr. c). Due to inflation, pennies have lost virtually all their purchasing power and are often viewed as an expensive burden to merchants, banks, government mints and the public in general.\nPenny is also the informal name of the cent unit of account in Canada, although one-cent coins were removed from circulation in 2012. \nThe name penny is also used in reference to various historical currencies, also derived from the Carolingian system, such as the French denier and the German pfennig. It may also be informally used to refer to any similar smallest-denomination coin, such as the euro cent or Chinese fen.\nThe Carolingian penny was originally a 0.940-fine silver coin, weighing 1\u2044240 pound. It was adopted by Offa of Mercia and other English kings and remained the principal currency in Europe over the next few centuries, until repeated debasements necessitated the development of more valuable coins. The British penny remained a silver coin until the expense of the Napoleonic Wars prompted the use of base metals in 1797. Despite the decimalization of currencies in the United States and, later, throughout the British Commonwealth, the name remains in informal use.\nNo penny is currently formally subdivided, although farthings (1\u20444d), halfpennies, and half cents have previously been minted and the mill (1\u204410\u00a2) remains in use as a unit of account in some contexts.\n\n\n== Etymology ==\n\nPenny is first attested in a 1394 Scots text, a variant of Old English peni, a development of numerous variations including pennig, penning, and pending. The etymology of the term \"penny\" is uncertain, although cognates are common across almost all Germanic languages and suggest a base *pan-, *pann-, or *pand- with the individualizing suffix -ing. Common suggestions include that it was originally *panding as a Low Franconian form of Old High German pfant \"pawn\" (in the sense of a pledge or debt, as in a pawnbroker putting up collateral as a pledge for repayment of loans); *panning as a form of the West Germanic word for \"frying pan\", presumably owing to its shape; and *ponding as a very early borrowing of Latin pondus (\"pound\"). Recently, it has been proposed that it may represent an early borrowing of Punic pn (Pane or Pene, \"Face\"), as the face of Carthaginian goddess Tanit was represented on nearly all Carthaginian currency. Following decimalization, the British and Irish coins were marked \"new penny\" until 1982 and 1985, respectively.\nFrom the 16th century, the regular plural pennies fell out of use in England, when referring to a sum of money (e.g. \"That costs tenpence.\"), but continued to be used to refer to more than one penny coin (\"Here you are, a sixpence and four pennies.\"). It remains common in Scottish English, and is standard for all senses in American English, where, however, the informal \"penny\" is typically only used of the coins in any case, values being expressed in \"cents\". The informal name for the American cent seems to have spread from New York State.\nIn Britain, prior to decimalization, values from two to eleven pence were often written, and spoken as a single word, as twopence or tuppence, threepence or thruppence, etc. (Other values were usually expressed in terms of shillings and pence or written as two words, which might or might not be hyphenated.) Where a single coin represented a number of pence, it was treated as a single noun, as a sixpence. Thus, \"a threepence\" (but more usually \"a threepenny bit\") would be a single coin of that value whereas \"three pence\" would be its value, and \"three pennies\" would be three penny coins. In British English, divisions of a penny were added to such combinations without a conjunction, as sixpence-farthing, and such constructions were also treated as single nouns. Adjectival use of such coins used the ending -penny, as sixpenny.\nThe British abbreviation d. derived from the Latin denarius. It followed the amount, e.g. \"11d\". It has been replaced since decimalization by p, usually written without a space or period. From this abbreviation, it is common to speak of pennies and values in pence as \"p\". In North America, it is common to abbreviate cents with the currency symbol \u00a2. Elsewhere, it is usually written with a simple c.\n\n\n== History ==\n\n\n=== Antiquity ===\n\nThe medieval silver penny was modeled on similar coins in antiquity, such as the Greek drachma, the Carthaginian shekel, and the Roman denarius. Forms of these seem to have reached as far as Norway and Sweden. The use of Roman currency in Britain, seems to have fallen off after the Roman withdrawal and subsequent Saxon invasions.\n\n\n=== Frankish Empire ===\n\nCharlemagne's father Pepin the Short instituted a major currency reform around AD 755, aiming to reorganize Francia's previous silver standard with a standardized .940-fine denier (Latin: denarius) weighing 1\u2044240 pound. (As the Carolingian pound seems to have been about 489.5 grams, each penny weighed about 2 grams.) Around 790, Charlemagne introduced a new .950 or .960-fine penny with a smaller diameter. Surviving specimens have an average weight of 1.70 grams, although some estimate the original ideal mass at 1.76 grams. But despite the purity and quality of these pennies, they were often rejected by traders throughout the Carolingian period, in favor of the gold coins used elsewhere; this led to repeated legislation against such refusal, to accept the king's currency.\n\n\n=== England ===\n\nSome of the Anglo-Saxon kingdoms initially copied the solidus, the late Roman gold coin; at the time, however, gold was so rare and valuable that even the smallest coins had such a great value that they could only be used in very large transactions and were sometimes not available at all. Around 641\u2013670, there seems to have been a movement to use coins with lower gold content. This decreased their value and may have increased the number that could be minted, but these paler coins do not seem to have solved the problem of the value and scarcity of the currency. The miscellaneous silver sceattas minted in Frisia and Anglo-Saxon England after around 680 were probably known as \"pennies\" at the time. (The misnomer is based on a probable misreading of the Anglo-Saxon legal codes.) Their purity varied and their weight fluctuated from about 0.8 to about 1.3 grams. They continued to be minted in East Anglia under Beonna and in Northumbria as late as the mid-9th century.\nThe first Carolingian-style pennies were introduced by King Offa of Mercia (r. 757\u2013796), modeled on Pepin's system. His first series was 1\u2044240 of the Saxon pound of 5400 grains (350 grams), giving a pennyweight of about 1.46 grams. His queen Cynethryth also minted these coins under her own name. Near the end of his reign, Offa minted his coins in imitation of Charlemagne's reformed pennies. Offa's coins were imitated by East Anglia, Kent, Wessex and Northumbria, as well as by two Archbishops of Canterbury. As in the Frankish Empire, all these pennies were notionally fractions of shillings (solidi; sol) and pounds (librae; livres) but during this period neither larger unit was minted. Instead, they functioned only as notional units of account. (For instance, a \"shilling\" or \"solidus\" of grain was a measure equivalent to the amount of grain that 12 pennies could purchase.) English currency was notionally .925-fine sterling silver at the time of Henry II, but the weight and value of the silver penny steadily declined from 1300 onwards.\nIn 1257, Henry III minted a gold penny which had the nominal value of 1 shilling 8 pence (i.e. 20 d.). At first, the coin proved unpopular because it was overvalued for its weight; by 1265 it was so undervalued\u2014the bullion value of its gold being worth 2 shillings (i.e. 24 d.) by then\u2014that the coins still in circulation were almost entirely melted down for the value of their gold. Only eight gold pennies are known to survive. It was not until the reign of Edward III that the florin and noble established a common gold currency in England.\n\nThe earliest halfpenny and farthing (\u00bcd.) found date from the reign of Henry III. The need for small change was also sometimes met by simply cutting a full penny into halves or quarters. In 1527, Henry VIII abolished the Tower pound of 5400 grains, replacing it with the Troy pound of 5760 grains (making a penny 5760/240 = 24 grains) and establishing a new pennyweight of 1.56 grams, although, confusingly, the penny coin by then weighed about 8 grains, and had never weighed as much as this 24 grains. The last silver pence for general circulation were minted during the reign of Charles II around 1660. Since then, they have only been coined for issue as Maundy money, royal alms given to the elderly on Maundy Thursday.\n\n\n=== United Kingdom ===\n\nThroughout the 18th century, the British government did not mint pennies for general circulation and the bullion value of the existing silver pennies caused them to be withdrawn from circulation. Merchants and mining companies, such as Anglesey's Parys Mining Co., began to issue their own copper tokens to fill the need for small change. Finally, amid the Napoleonic Wars, the government authorized Matthew Boulton to mint copper pennies and twopences at Soho Mint in Birmingham in 1797. Typically, 1 lb. of copper produced 24 pennies. In 1860, the copper penny was replaced with a bronze one (95% copper, 4% tin, 1% zinc). Each pound of bronze was coined into 48 pennies.\n\n\n=== United States ===\n\nThe United States' cent, popularly known as the \"penny\" since the early 19th century, began with the unpopular copper chain cent in 1793. Abraham Lincoln was the first historical figure to appear on a U.S. coin when he was portrayed on the one-cent coin to commemorate his 100th birthday.\n\n\n=== South Africa ===\nThe penny that was brought to the Cape Colony (in what is now South Africa) was a large coin\u201436 mm in diameter, 3.3 mm thick, and 1 oz (28 g)\u2014and the twopence was correspondingly larger at 41 mm in diameter, 5 mm thick and 2 oz (57 g). On them was Britannia with a trident in her hand. The English called this coin the Cartwheel penny due to its large size and raised rim, but the Capetonians referred to it as the Devil's Penny as they assumed that only the Devil used a trident. The coins were very unpopular due to their large weight and size. On 6 June 1825, Lord Charles Somerset, the governor, issued a proclamation that only British Sterling would be legal tender in the Cape Colony (colonial South Africa). The new British coins (which were introduced in England in 1816), among them being the shilling, six-pence of silver, the penny, half-penny, and quarter-penny in copper, were introduced to the Cape. Later two-shilling, four-penny, and three-penny coins were added to the coinage. The size and denomination of the 1816 British coins, with the exception of the four-penny coins, were used in South Africa until 1960.\n\n\n== Criticism of continued use ==\n\nHandling and counting penny coins entail transaction costs that may be higher than a penny. It has been claimed that, for micropayments, the mental arithmetic costs more than the penny. Changes in the market prices of metals, combined with currency inflation, have caused the metal value of penny coins to exceed their face value.\nCanada adopted 5\u00a2 as its lowest denomination in 2012. Several nations have stopped minting equivalent value coins, and efforts have been made to end the routine use of pennies in several countries. In the UK, since 1992, one- and two-penny coins have been made from copper-plated steel (making them magnetic) instead of bronze.\n\n\n== In popular culture ==\nIn British and American culture, finding a penny is traditionally considered lucky. A proverbial expression of this is \"Find a penny, pick it up, and all the day you'll have good luck.\"\n\"A penny for your thoughts\" is an idiomatic way of asking someone what they are thinking about. It is first attested in John Heywood's 1547 Dialogue Conteinying the Nomber in Effect of All the Proverbes in the Englishe Tongue, at a time when the penny was still a sterling silver coin.\n\"In for a penny, in for a pound,\" is a common expression used to express someone's intention to see an undertaking through, however much time, effort, or money this entails.\nTo \"give (one's) tuppence/tuppenny/two'penneth (worth)\", is a commonwealth saying that uses the words for two pence to share one's opinion, idea, or point of view, regardless of whether or not others want to hear it. A similar expression using the US term of cents is my two cents.\nIn British English, to \"spend a penny\" means to urinate. Its etymology is literal: coin-operated public toilets commonly charged a pre-decimal penny, beginning with the Great Exhibition of 1851.\n\"Tuppence\" - Old British slang word for \u2018vagina\u2019.\nIn 1936 U.S. shoemaker G.H. Bass & Co. introduced its \"Weejuns\" penny loafers. Other companies followed with similar products.\nA common myth is that a penny dropped from the Empire State Building would  kill a person or crack the sidewalk. However, a penny is too light and has too much air resistance to acquire enough speed to do much damage since it reaches terminal velocity after falling about 50 feet.\n\n\n== List of pennies ==\n\n\n== See also ==\n\n\n== Notes ==\n\n\n== References ==\n\n\n=== Citations ===\n\n\n=== Bibliography ===\nAllen, Larry (2009), \"Carolingian Reform\", The Encyclopedia of Money, Sta. Barbara: ABC Clio, pp. 59\u201360, ISBN 978-1-59884-251-7.\nBlackburn, M.A.S.; et al. (1986), Medieval European Coinage, Vol. 1: The Early Middle Ages (5th\u201310th centuries), Cambridge{{citation}}:  CS1 maint: location missing publisher (link).\nBosworth; et al., An Old English Dictionary.\nChisholm, Hugh, ed. (1911). \"Penny\" . Encyclop\u00e6dia Britannica. Vol. 21 (11th ed.). Cambridge University Press. pp. 115\u2013116.\nChown, John F (1994), A History of Money from AD 800, London: Routledge, ISBN 0-415-10279-0.\nCipolla, Carlo M. (1993), Before the Industrial Revolution: European Society and Economy, 1000\u20131700, Taylor & Francis, ISBN 9780203695128.\nFerguson, Wallace K. (1974), \"Money and Coinage of the Age of Erasmus: An Historical and Analytical Glossary with Particular Reference to France, the Low Countries, England, the Rhineland, and Italy\", The Correspondence of Erasmus: Letters 1 to 141: 1484 to 1500, Toronto: University of Toronto Press, pp. 311\u2013349, ISBN 0-8020-1981-1.\nFrassetto, Michael (2003), Encyclopedia of Barbarian Europe: Society in Transformation, Bloomsbury Academic, ISBN 9781576072639.\nKeary, Charles Francis (2005), A Catalogue of English Coins in the British Museum: Anglo-Saxon Series, Vol. I.\nMunro, John H. (2012), \"The Technology and Economics of Coinage Debasements in Medieval and Early Modern Europe: With Special Reference to the Low Countries and England\", Money in the Pre-Industrial World: Bullion, Debasements, and Coin Substitutes, Pickering & Chatto, republished 2016 by Routledge, pp. 30 ff, ISBN 978-1-84893-230-2.\nScott, Martin (1964), Medieval Europe, New York: Dorset Press, ISBN 0-88029-115-X.\nIslam and the Carolingian Penny, National Bank of Belgium Museum, November 2006.\nSelgin, George A. (2008), Good Money: Birmingham Button Makers, the Royal Mint, and the Beginnings of Modern Coinage, 1775\u20131821, University of Michigan Press, ISBN 978-0-472-11631-7.\nSuchodolski, Stanislaw (1983), \"On the Rejection of Good Coin in Carolingian Europe\", Studies in Numismatic Method: Presented to Philip Grierson, Cambridge: Cambridge University Press, pp. 147\u2013152, ISBN 0-521-22503-5.\n\n\n== External links ==\n\nCopper Penny Importance \u2013 Blog post & video covering the importance of retaining copper pennies.\nThe MegaPenny Project \u2013 A visualisation of what exponential numbers of pennies would look like.\nSilver Pennies \u2013 Pictures of English silver pennies from Anglo-Saxon times to the present.\nCopper Pennies \u2013 Pictures of English copper pennies from 1797 to 1860.\nUS Lincoln Penny on the Planet Mars \u2013 Curiosity Rover (September 10, 2012).\n\"Penny\" . Collier's New Encyclopedia. 1921.\n\"Penny\" . New International Encyclopedia. 1905.",
        "unit": "penny",
        "url": "https://en.wikipedia.org/wiki/Penny"
    },
    {
        "_id": "Gigabit",
        "clean": "Gigabit",
        "text": "The bit is the most basic unit of information in computing and digital communication. The name is a portmanteau of binary digit. The bit represents a logical state with one of two possible values. These values are most commonly represented as either \"1\" or \"0\", but other representations such as true/false, yes/no, on/off, or +/\u2212 are also widely used.\nThe relation between these values and the physical states of the underlying storage or device is a matter of convention, and different assignments may be used even within the same device or program. It may be physically implemented with a two-state device.\nA contiguous group of binary digits is commonly called a bit string, a bit vector, or a single-dimensional (or multi-dimensional) bit array.\nA group of eight bits is called one byte, but historically the size of the byte is not strictly defined. Frequently, half, full, double and quadruple words consist of a number of bytes which is a low power of two. A string of four bits is usually a nibble.\nIn information theory, one bit is the information entropy of a random binary variable that is 0 or 1 with equal probability, or the information that is gained when the value of such a variable becomes known. As a unit of information, the bit is also known as a shannon, named after Claude E. Shannon.\nThe symbol for the binary digit is either \"bit\", per the IEC 80000-13:2008 standard, or the lowercase character \"b\", per the IEEE 1541-2002 standard. Use of the latter may create confusion with the capital \"B\" which is the international standard symbol for the byte.\n\n\n== History ==\nThe encoding of data by discrete bits was used in the punched cards invented by Basile Bouchon and Jean-Baptiste Falcon (1732), developed by Joseph Marie Jacquard (1804), and later adopted by Semyon Korsakov, Charles Babbage, Herman Hollerith, and early computer manufacturers like IBM. A variant of that idea was the perforated paper tape. In all those systems, the medium (card or tape) conceptually carried an array of hole positions; each position could be either punched through or not, thus carrying one bit of information. The encoding of text by bits was also used in Morse code (1844) and early digital communications machines such as teletypes and stock ticker machines (1870).\nRalph Hartley suggested the use of a logarithmic measure of information in 1928. Claude E. Shannon first used the word \"bit\" in his seminal 1948 paper \"A Mathematical Theory of Communication\". He attributed its origin to John W. Tukey, who had written a Bell Labs memo on 9 January 1947 in which he contracted \"binary information digit\" to simply \"bit\".\n\n\n== Physical representation ==\nA bit can be stored by a digital device or other physical system that exists in either of two possible distinct states. These may be the two stable states of a flip-flop, two positions of an electrical switch, two distinct voltage or current levels allowed by a circuit, two distinct levels of light intensity, two directions of magnetization or polarization, the orientation of reversible double stranded DNA, etc.\nBits can be implemented in several forms. In most modern computing devices, a bit is usually represented by an electrical voltage or current pulse, or by the electrical state of a flip-flop circuit.\nFor devices using positive logic, a digit value of 1 (or a logical value of true) is represented by a more positive voltage relative to the representation of 0. Different logic families require different voltages, and variations are allowed to account for component aging and noise immunity. For example, in transistor\u2013transistor logic (TTL) and compatible circuits, digit values 0 and 1 at the output of a device are represented by no higher than 0.4 V and no lower than 2.6 V, respectively; while TTL inputs are specified to recognize 0.8 V or below as 0 and 2.2 V or above as 1.\n\n\n=== Transmission and processing ===\nBits are transmitted one at a time in serial transmission, and by a multiple number of bits in parallel transmission. A bitwise operation optionally processes bits one at a time. Data transfer rates are usually measured in decimal SI multiples of the unit bit per second (bit/s), such as kbit/s.\n\n\n=== Storage ===\nIn the earliest non-electronic information processing devices, such as Jacquard's loom or Babbage's Analytical Engine, a bit was often stored as the position of a mechanical lever or gear, or the presence or absence of a hole at a specific point of a paper card or tape. The first electrical devices for discrete logic (such as elevator and traffic light control circuits, telephone switches, and Konrad Zuse's computer) represented bits as the states of electrical relays which could be either \"open\" or \"closed\". When relays were replaced by vacuum tubes, starting in the 1940s, computer builders experimented with a variety of storage methods, such as pressure pulses traveling down a mercury delay line, charges stored on the inside surface of a cathode-ray tube, or opaque spots printed on glass discs by photolithographic techniques.\nIn the 1950s and 1960s, these methods were largely supplanted by magnetic storage devices such as magnetic-core memory, magnetic tapes, drums, and disks, where a bit was represented by the polarity of magnetization of a certain area of a ferromagnetic film, or by a change in polarity from one direction to the other. The same principle was later used in the magnetic bubble memory developed in the 1980s, and is still found in various magnetic strip items such as metro tickets and some credit cards.\nIn modern semiconductor memory, such as dynamic random-access memory, the two values of a bit may be represented by two levels of electric charge stored in a capacitor. In certain types of programmable logic arrays and read-only memory, a bit may be represented by the presence or absence of a conducting path at a certain point of a circuit. In optical discs, a bit is encoded as the presence or absence of a microscopic pit on a reflective surface. In one-dimensional bar codes, bits are encoded as the thickness of alternating black and white lines.\n\n\n== Unit and symbol ==\nThe bit is not defined in the International System of Units (SI). However, the International Electrotechnical Commission issued standard IEC 60027, which specifies that the symbol for binary digit should be 'bit', and this should be used in all multiples, such as 'kbit', for kilobit. However, the lower-case letter 'b' is widely used as well and was recommended by the IEEE 1541 Standard (2002). In contrast, the upper case letter 'B' is the standard and customary symbol for byte.\n\n\n=== Multiple bits ===\n\nMultiple bits may be expressed and represented in several ways. For convenience of representing commonly reoccurring groups of bits in information technology, several units of information have traditionally been used. The most common is the unit byte, coined by Werner Buchholz in June 1956, which historically was used to represent the group of bits used to encode a single character of text (until UTF-8 multibyte encoding took over) in a computer and for this reason it was used as the basic addressable element in many computer architectures. The trend in hardware design converged on the most common implementation of using eight bits per byte, as it is widely used today. However, because of the ambiguity of relying on the underlying hardware design, the unit octet was defined to explicitly denote a sequence of eight bits.\nComputers usually manipulate bits in groups of a fixed size, conventionally named \"words\". Like the byte, the number of bits in a word also varies with the hardware design, and is typically between 8 and 80 bits, or even more in some specialized computers. In the early 21st century, retail personal or server computers have a word size of 32 or 64 bits.\nThe International System of Units defines a series of decimal prefixes for multiples of standardized units which are commonly also used with the bit and the byte. The prefixes kilo (103) through yotta (1024) increment by multiples of one thousand, and the corresponding units are the kilobit (kbit) through the yottabit (Ybit).\n\n\n== Information capacity and information compression ==\n\nWhen the information capacity of a storage system or a communication channel is presented in bits or bits per second, this often refers to binary digits, which is a computer hardware capacity to store binary data (0 or 1, up or down, current or not, etc.). Information capacity of a storage system is only an upper bound to the quantity of information stored therein. If the two possible values of one bit of storage are not equally likely, that bit of storage contains less than one bit of information. If the value is completely predictable, then the reading of that value provides no information at all (zero entropic bits, because no resolution of uncertainty occurs and therefore no information is available). If a computer file that uses n bits of storage contains only m < n bits of information, then that information can in principle be encoded in about m bits, at least on the average. This principle is the basis of data compression technology. Using an analogy, the hardware binary digits refer to the amount of storage space available (like the number of buckets available to store things), and the information content the filling, which comes in different levels of granularity (fine or coarse, that is, compressed or uncompressed information). When the granularity is finer\u2014when information is more compressed\u2014the same bucket can hold more.\nFor example, it is estimated that the combined technological capacity of the world to store information provides 1,300 exabytes of hardware digits. However, when this storage space is filled and the corresponding content is optimally compressed, this only represents 295 exabytes of information. When optimally compressed, the resulting carrying capacity approaches Shannon information or information entropy.\n\n\n== Bit-based computing ==\nCertain bitwise computer processor instructions (such as bit set) operate at the level of manipulating bits rather than manipulating data interpreted as an aggregate of bits.\nIn the 1980s, when bitmapped computer displays became popular, some computers provided specialized bit block transfer instructions to set or copy the bits that corresponded to a given rectangular area on the screen.\nIn most computers and programming languages, when a bit within a group of bits, such as a byte or word, is referred to, it is usually specified by a number from 0 upwards corresponding to its position within the byte or word. However, 0 can refer to either the most or least significant bit depending on the context.\n\n\n== Other information units ==\n\nSimilar to torque and energy in physics; information-theoretic information and data storage size have the same dimensionality of units of measurement, but there is in general no meaning to adding, subtracting or otherwise combining the units mathematically, although one may act as a bound on the other.\nUnits of information used in information theory include the shannon (Sh), the natural unit of information (nat) and the hartley (Hart). One shannon is the maximum amount of information needed to specify the state of one bit of storage. These are related by 1 Sh \u2248 0.693 nat \u2248 0.301 Hart.\nSome authors also define a binit as an arbitrary information unit equivalent to some fixed but unspecified number of bits.\n\n\n== See also ==\nBinary numeral system\nBit rate and baud rate\nBitstream\nByte\nEntropy (information theory)\nFuzzy bit\nInteger (computer science)\nNibble\nPrimitive data type\nP-bit (probabilistic bit)\nQubit (quantum bit)\nShannon (unit)\nTernary numeral system\nTrit (Trinary digit)\n\n\n== References ==\n\n\n== External links ==\n\nBit Calculator \u2013 a tool providing conversions between bit, byte, kilobit, kilobyte, megabit, megabyte, gigabit, gigabyte\nBitXByteConverter Archived 2016-04-06 at the Wayback Machine \u2013 a tool for computing file sizes, storage capacity, and digital information in various units",
        "unit": "gigabit",
        "url": "https://en.wikipedia.org/wiki/Gigabit"
    },
    {
        "_id": "Base_pair",
        "clean": "Base pair",
        "text": "A base pair (bp) is a fundamental unit of double-stranded nucleic acids consisting of two nucleobases bound to each other by hydrogen bonds.  They form the building blocks of the DNA double helix and contribute to the folded structure of both DNA and RNA. Dictated by specific hydrogen bonding patterns, \"Watson\u2013Crick\" (or \"Watson\u2013Crick\u2013Franklin\") base pairs (guanine\u2013cytosine and adenine\u2013thymine) allow the DNA helix to maintain a regular helical structure that is subtly dependent on its nucleotide sequence. The complementary nature of this based-paired structure provides a redundant copy of the genetic information encoded within each strand of DNA. The regular structure and data redundancy provided by the DNA double helix make DNA well suited to the storage of genetic information, while base-pairing between DNA and incoming nucleotides provides the mechanism through which DNA polymerase replicates DNA and RNA polymerase transcribes DNA into RNA. Many DNA-binding proteins can recognize specific base-pairing patterns that identify particular regulatory regions of genes.\nIntramolecular base pairs can occur within single-stranded nucleic acids. This is particularly important in RNA molecules (e.g., transfer RNA), where Watson\u2013Crick base pairs (guanine\u2013cytosine and adenine\u2013uracil) permit the formation of short double-stranded helices, and a wide variety of non\u2013Watson\u2013Crick interactions (e.g., G\u2013U or A\u2013A) allow RNAs to fold into a vast range of specific three-dimensional structures. In addition, base-pairing between transfer RNA (tRNA) and messenger RNA (mRNA) forms the basis for the molecular recognition events that result in the nucleotide sequence of mRNA becoming translated into the amino acid sequence of proteins via the genetic code.\nThe size of an individual gene or an organism's entire genome is often measured in base pairs because DNA is usually double-stranded. Hence, the number of total base pairs is equal to the number of nucleotides in one of the strands (with the exception of non-coding single-stranded regions of telomeres). The haploid human genome (23 chromosomes) is estimated to be about 3.2 billion base pairs long and to contain 20,000\u201325,000 distinct protein-coding genes. A kilobase (kb) is a unit of measurement in molecular biology equal to 1000 base pairs of DNA or RNA. The total number of DNA base pairs on Earth is estimated at 5.0\u00d71037 with a weight of 50 billion tonnes. In comparison, the total mass of the biosphere has been estimated to be as much as 4 TtC (trillion tons of carbon).\n\n\n== Hydrogen bonding and stability ==\n\nHydrogen bonding is the chemical interaction that underlies the base-pairing rules described above. Appropriate geometrical correspondence of hydrogen bond donors and acceptors allows only the \"right\" pairs to form stably. DNA with high GC-content is more stable than DNA with low GC-content. Crucially, however, stacking interactions are primarily responsible for stabilising the double-helical structure; Watson-Crick base pairing's contribution to global structural stability is minimal, but its role in the specificity underlying complementarity is, by contrast, of maximal importance as this underlies the template-dependent processes of the central dogma (e.g. DNA replication).\nThe bigger nucleobases, adenine and guanine, are members of a class of double-ringed chemical structures called purines; the smaller nucleobases, cytosine and thymine (and uracil), are members of a class of single-ringed chemical structures called pyrimidines. Purines are complementary only with pyrimidines: pyrimidine\u2013pyrimidine pairings are energetically unfavorable because the molecules are too far apart for hydrogen bonding to be established; purine\u2013purine pairings are energetically unfavorable because the molecules are too close, leading to overlap repulsion. Purine\u2013pyrimidine base-pairing of AT or GC or UA (in RNA) results in proper duplex structure. The only other purine\u2013pyrimidine pairings would be AC and GT and UG (in RNA); these pairings are mismatches because the patterns of hydrogen donors and acceptors do not correspond. The GU pairing, with two hydrogen bonds, does occur fairly often in RNA (see wobble base pair).\nPaired DNA and RNA molecules are comparatively stable at room temperature, but the two nucleotide strands will separate above a melting point that is determined by the length of the molecules, the extent of mispairing (if any), and the GC content. Higher GC content results in higher melting temperatures; it is, therefore, unsurprising that the genomes of extremophile organisms such as Thermus thermophilus are particularly GC-rich. On the converse, regions of a genome that need to separate frequently \u2014 for example, the promoter regions for often-transcribed genes \u2014 are comparatively GC-poor (for example, see TATA box). GC content and melting temperature must also be taken into account when designing  primers for PCR reactions.\n\n\n=== Examples ===\nThe following DNA sequences illustrate pair double-stranded patterns. By convention, the top strand is written from the 5\u2032-end to the 3\u2032-end; thus, the bottom strand is written 3\u2032 to 5\u2032.\n\nA base-paired DNA sequence:\nATCGATTGAGCTCTAGCG\nTAGCTAACTCGAGATCGC\nThe corresponding RNA sequence, in which uracil is substituted for thymine in the RNA strand:\nAUCGAUUGAGCUCUAGCG\nUAGCUAACUCGAGAUCGC\n\n\n== Base analogs and intercalators ==\n\nChemical analogs of nucleotides can take the place of proper nucleotides and establish non-canonical base-pairing, leading to errors (mostly point mutations) in DNA replication and DNA transcription. This is due to their isosteric chemistry. One common mutagenic base analog is 5-bromouracil, which resembles thymine but can base-pair to guanine in its enol form.\nOther chemicals, known as DNA intercalators, fit into the gap between adjacent bases on a single strand and induce frameshift mutations by \"masquerading\" as a base, causing the DNA replication machinery to skip or insert additional nucleotides at the intercalated site. Most intercalators are large polyaromatic compounds and are known or suspected carcinogens. Examples include ethidium bromide and acridine.\n\n\n== Mismatch repair ==\nMismatched base pairs can be generated by errors of DNA replication and as intermediates during homologous recombination. The process of mismatch repair ordinarily must recognize and correctly repair a small number of base mispairs within a long sequence of normal DNA base pairs.  To repair mismatches formed during DNA replication, several distinctive repair processes have evolved to distinguish between the template strand and the newly formed strand so that only the newly inserted incorrect nucleotide is removed (in order to avoid generating a mutation).  The proteins employed in mismatch repair during DNA replication, and the clinical significance of defects in this process are described in the article DNA mismatch repair.  The process of mispair correction during recombination is described in the article gene conversion.\n\n\n== Length measurements ==\n\nThe following abbreviations are commonly used to describe the length of a D/RNA molecule:\n\nbp  = base pair\u2014one bp corresponds to approximately 3.4 \u00c5 (340 pm)  of length along the strand, and to roughly 618 or 643 daltons for DNA and RNA respectively.\nkb (= kbp) = kilo\u2013base-pair = 1,000 bp\nMb (= Mbp) = mega\u2013base-pair = 1,000,000 bp\nGb (= Gbp) = giga\u2013base-pair = 1,000,000,000 bp\nFor single-stranded DNA/RNA, units of nucleotides are used\u2014abbreviated nt (or knt, Mnt, Gnt)\u2014as they are not paired.\nTo distinguish between units of computer storage and bases, kbp, Mbp, Gbp, etc. may be used for base pairs.\nThe centimorgan is also often used to imply distance along a chromosome, but the number of base pairs it corresponds to varies widely. In the human genome, the centimorgan is about 1 million base pairs.\n\n\n== Unnatural base pair (UBP) ==\n\nAn unnatural base pair (UBP) is a designed subunit (or nucleobase) of DNA which is created in a laboratory and does not occur in nature.  DNA sequences have been described which use newly created nucleobases to form a third base pair, in addition to the two base pairs found in nature, A-T (adenine \u2013 thymine) and G-C (guanine \u2013 cytosine).  A few research groups have been searching for a third base pair for DNA, including teams led by Steven A. Benner, Philippe Marliere, Floyd E. Romesberg and Ichiro Hirao. Some new base pairs based on alternative hydrogen bonding, hydrophobic interactions and metal coordination have been reported.\nIn 1989 Steven Benner (then working at the Swiss Federal Institute of Technology in Zurich) and his team led with modified forms of cytosine and guanine into DNA molecules in vitro. The nucleotides, which encoded RNA and proteins, were successfully replicated in vitro. Since then, Benner's team has been trying to engineer cells that can make foreign bases from scratch, obviating the need for a feedstock.\nIn 2002, Ichiro Hirao's group in Japan developed an unnatural base pair between 2-amino-8-(2-thienyl)purine (s) and pyridine-2-one (y) that functions in transcription and translation, for the site-specific incorporation of non-standard amino acids into proteins. In 2006, they created 7-(2-thienyl)imidazo[4,5-b]pyridine (Ds) and pyrrole-2-carbaldehyde (Pa) as a third base pair for replication and transcription. Afterward, Ds and 4-[3-(6-aminohexanamido)-1-propynyl]-2-nitropyrrole (Px) was discovered as a high fidelity pair in PCR amplification. In 2013, they applied the Ds-Px pair to DNA aptamer generation by in vitro selection (SELEX) and demonstrated the genetic alphabet expansion significantly augment DNA aptamer affinities to target proteins.\nIn 2012, a group of American scientists led by Floyd Romesberg, a chemical biologist at the Scripps Research Institute in San Diego, California, published that his team designed an unnatural base pair (UBP).  The two new artificial nucleotides or Unnatural Base Pair (UBP) were named d5SICS and dNaM. More technically, these artificial nucleotides bearing hydrophobic nucleobases, feature two fused aromatic rings that form a (d5SICS\u2013dNaM) complex or base pair in DNA. His team designed a variety of in vitro or \"test tube\" templates containing the unnatural base pair and they confirmed that it was efficiently replicated with high fidelity in virtually all sequence contexts using the modern standard in vitro techniques, namely PCR amplification of DNA and PCR-based applications. Their results show that for PCR and PCR-based applications, the d5SICS\u2013dNaM unnatural base pair is functionally equivalent to a natural base pair, and when combined with the other two natural base pairs used by all organisms, A\u2013T and G\u2013C, they provide a fully functional and expanded six-letter \"genetic alphabet\".\nIn 2014 the same team from the Scripps Research Institute reported that they synthesized a stretch of circular DNA known as a plasmid containing natural T-A and C-G base pairs along with the best-performing UBP Romesberg's laboratory had designed and inserted it into cells of the common bacterium E. coli that successfully replicated the unnatural base pairs through multiple generations. The transfection did not hamper the growth of the E. coli cells and showed no sign of losing its unnatural base pairs to its natural DNA repair mechanisms. This is the first known example of a living organism passing along an expanded genetic code to subsequent generations. Romesberg said he and his colleagues created 300 variants to refine the design of nucleotides that would be stable enough and would be replicated as easily as the natural ones when the cells divide.  This was in part achieved by the addition of a supportive algal gene that expresses a nucleotide triphosphate transporter which efficiently imports the triphosphates of both d5SICSTP and dNaMTP into E. coli bacteria. Then, the natural bacterial replication pathways use them to accurately replicate a plasmid containing d5SICS\u2013dNaM. Other researchers were surprised that the bacteria replicated these human-made DNA subunits.\nThe successful incorporation of a third base pair is a significant breakthrough toward the goal of greatly expanding the number of amino acids which can be encoded by DNA, from the existing 20 amino acids to a theoretically possible 172, thereby expanding the potential for living organisms to produce novel proteins. The artificial strings of DNA do not encode for anything yet, but scientists speculate they could be designed to manufacture new proteins which could have industrial or pharmaceutical uses. Experts said the synthetic DNA incorporating the unnatural base pair raises the possibility of life forms based on a different DNA code.\n\n\n== Non-canonical base pairing ==\n\nIn addition to the canonical pairing, some conditions can also favour base-pairing with alternative base orientation, and number and geometry of hydrogen bonds. These pairings are accompanied by alterations to the local backbone shape.\nThe most common of these is the wobble base pairing that occurs between tRNAs and mRNAs at the third base position of many codons during transcription and during the charging of tRNAs by some tRNA synthetases. They have also been observed in the secondary structures of some RNA sequences.\nAdditionally, Hoogsteen base pairing (typically written as A\u2022U/T and G\u2022C) can exist in some DNA sequences (e.g. CA and TA dinucleotides) in dynamic equilibrium with standard Watson\u2013Crick pairing. They have also been observed in some protein\u2013DNA complexes.\nIn addition to these alternative base pairings, a wide range of base-base hydrogen bonding is observed in RNA secondary and tertiary structure. These bonds are often necessary for the precise, complex shape of an RNA, as well as its binding to interaction partners.\n\n\n== See also ==\nList of Y-DNA single-nucleotide polymorphisms\nNon-canonical base pairing\nChargaff's rules\n\n\n== References ==\n\n\n== Further reading ==\n\n\n== External links ==\n\nDAN\u2014webserver version of the EMBOSS tool for calculating melting temperatures",
        "unit": "kilo base pair",
        "url": "https://en.wikipedia.org/wiki/Base_pair"
    },
    {
        "_id": "Gigayear",
        "clean": "Gigayear",
        "text": "A year is the time taken for astronomical objects to complete one orbit. For example, a year on Earth is the time taken for Earth to revolve around the Sun. Generally, a year is taken to mean a calendar year, but the word is also used for periods loosely associated with the calendar or astronomical year, such as the seasonal year, the fiscal year, the academic year, etc. The term can also be used in reference to any long period or cycle, such as the Great Year.\nDue to the Earth's axial tilt, the course of a year sees the passing of the seasons, marked by changes in weather, the hours of daylight, and, consequently, vegetation and soil fertility. In temperate and subpolar regions around the planet, four seasons are generally recognized: spring, summer, autumn, and winter. In tropical and subtropical regions, several geographical sectors do not present defined seasons; but in the seasonal tropics, the annual wet and dry seasons are recognized and tracked.\n\n\n== Calendar year ==\nA calendar year is an approximation of the number of days of the Earth's orbital period, as counted in a given calendar. The Gregorian calendar, or modern calendar, presents its calendar year to be either a common year of 365 days or a leap year of 366 days, as do the Julian calendars. For the Gregorian calendar, the average length of the calendar year (the mean year) across the complete leap cycle of 400 years is 365.2425 days (97 out of 400 years are leap years).\n\n\n== Abbreviation ==\nIn English, the unit of time for year is commonly abbreviated as \"y\" or \"yr\". The symbol \"a\" (for Latin: annus, year) is sometimes used in scientific literature, though its exact duration may be inconsistent.\n\n\n== Etymology ==\nEnglish year (via West Saxon \u0121\u0113ar (/j\u025bar/), Anglian \u0121\u0113r) continues Proto-Germanic *j\u01e3ran (*j\u0113\u2081ran). Cognates are German Jahr, Old High German j\u0101r, Old Norse \u00e1r and Gothic jer, from the Proto-Indo-European noun *yeh\u2081r-om \"year, season\". Cognates also descended from the same Proto-Indo-European noun (with variation in suffix ablaut) are Avestan y\u0101r\u01dd \"year\", Greek \u1f65\u03c1\u03b1 (h\u1e53ra) \"year, season, period of time\" (whence \"hour\"), Old Church Slavonic jar\u016d, and Latin hornus \"of this year\".\nLatin annus (a 2nd declension masculine noun; annum is the accusative singular; ann\u012b is genitive singular and nominative plural; ann\u014d the dative and ablative singular) is from a PIE noun *h\u2082et-no-, which also yielded Gothic a\u00fen \"year\" (only the dative plural a\u00fenam is attested).\nAlthough most languages treat the word as thematic *yeh\u2081r-o-, there is evidence for an original derivation with an *-r/n suffix, *yeh\u2081-ro-. Both Indo-European words for year, *yeh\u2081-ro- and *h\u2082et-no-, would then be derived from verbal roots meaning \"to go, move\", *h\u2081ey- and *h\u2082et-, respectively (compare Vedic Sanskrit \u00e9ti \"goes\", atasi \"thou goest, wanderest\"). A number of English words are derived from Latin annus, such as annual, annuity, anniversary, etc.; per annum means \"each year\", ann\u014d Domin\u012b means \"in the year of the Lord\".\nThe Greek word for \"year\", \u1f14\u03c4\u03bf\u03c2, is cognate with Latin vetus \"old\", from the PIE word *wetos- \"year\", also preserved in this meaning in Sanskrit vat-sa-ras \"year\" and vat-sa- \"yearling (calf)\", the latter also reflected in Latin vitulus \"bull calf\", English wether \"ram\" (Old English we\u00f0er, Gothic wi\u00ferus \"lamb\").\nIn some languages, it is common to count years by referencing to one season, as in \"summers\", or \"winters\", or \"harvests\". Examples include Chinese \u5e74 \"year\", originally \u79c2, an ideographic compound of a person carrying a bundle of wheat denoting \"harvest\". Slavic besides god\u016d \"time period; year\" uses l\u011bto \"summer; year\".\n\n\n== Intercalation ==\nAstronomical years do not have an integer number of days or lunar months. Any calendar that follows an astronomical year must have a system of intercalation such as leap years.\n\n\n=== Julian calendar ===\nIn the Julian calendar, the average (mean) length of a year is 365.25 days. In a non-leap year, there are 365 days, in a leap year there are 366 days. A leap year occurs every fourth year during which a leap day is intercalated into the month of February. The name \"Leap Day\" is applied to the added day.\nIn astronomy, the Julian year is a unit of time defined as 365.25 days, each of exactly 86,400 seconds (SI base unit), totaling exactly 31,557,600 seconds in the Julian astronomical year.\n\n\n==== Revised Julian calendar ====\nThe Revised Julian calendar, proposed in 1923 and used in some Eastern Orthodox Churches, has 218 leap years every 900 years, for the average (mean) year length of 365.2422222 days, close to the length of the mean tropical year, 365.24219 days (relative error of 9\u00b710). In the year 2800 CE, the Gregorian and Revised Julian calendars will begin to differ by one calendar day.\n\n\n=== Gregorian calendar ===\nThe Gregorian calendar aims to ensure that the northward equinox falls on or shortly before March 21 and hence it follows the northward equinox year, or tropical year. Because 97 out of 400 years are leap years, the mean length of the Gregorian calendar year is 365.2425 days; with a relative error below one ppm (8\u00b710) relative to the current length of the mean tropical year (365.242189 days) and even closer to the current March equinox year of 365.242374 days that it aims to match. \n\n\n=== Other calendars ===\n\nHistorically, lunisolar calendars intercalated entire leap months on an observational basis. Lunisolar calendars have mostly fallen out of use except for liturgical reasons (Hebrew calendar, various Hindu calendars).\nA modern adaptation of the historical Jalali calendar, known as the Solar Hijri calendar (1925), is a purely solar calendar with an irregular pattern of leap days based on observation (or astronomical computation), aiming to place new year (Nowruz) on the day of vernal equinox (for the time zone of Tehran), as opposed to using an algorithmic system of leap years.\n\n\n== Year numbering ==\nA calendar era assigns a cardinal number to each sequential year, using a reference event in the past (called the epoch) as the beginning of the era.\nThe Gregorian calendar era is the world's most widely used civil calendar. Its epoch is a 6th century estimate of the date of birth of Jesus of Nazareth. Two notations are used to indicate year numbering in the Gregorian calendar: the Christian \"Anno Domini\" (meaning \"in the year of the Lord\"), abbreviated AD; and \"Common Era\", abbreviated CE, preferred by many of other faiths and none.  Year numbers are based on inclusive counting, so that there is no \"year zero\".  Years before the epoch are abbreviated BC for Before Christ or BCE for Before the Common Era. In Astronomical year numbering, positive numbers indicate years AD/CE, the number 0 designates 1 BC/BCE, \u22121 designates 2 BC/BCE, and so on.\nOther eras include that of Ancient Rome, Ab Urbe Condita (\"from the foundation of the city), abbreviated AUC; Anno Mundi (\"year of the world\"), used for the Hebrew calendar and abbreviated AM; and the Japanese imperial eras. The Islamic Hijri year, (year of the Hijrah, Anno Hegirae abbreviated AH), is a lunar calendar of twelve lunar months and thus is shorter than a solar year.\n\n\n== Pragmatic divisions ==\nFinancial and scientific calculations often use a 365-day calendar to simplify daily rates.\n\n\n=== Fiscal year ===\n\nA fiscal year or financial year is a 12-month period used for calculating annual financial statements in businesses and other organizations. In many jurisdictions, regulations regarding accounting require such reports once per twelve months, but do not require that the twelve months constitute a calendar year.\nFor example, in Canada and India the fiscal year runs from April 1; in the United Kingdom it runs from April 1 for purposes of corporation tax and government financial statements, but from April 6 for purposes of personal taxation and payment of state benefits; in Australia it runs from July 1; while in the United States the fiscal year of the federal government runs from October 1.\n\n\n=== Academic year ===\n\nAn academic year is the annual period during which a student attends an educational institution. The academic year may be divided into academic terms, such as semesters or quarters. The school year in many countries starts in August or September and ends in May, June or July. In Israel the academic year begins around October or November, aligned with the second month of the Hebrew calendar.\nSome schools in the UK, Canada and the United States divide the academic year into three roughly equal-length terms (called trimesters or quarters in the United States), roughly coinciding with autumn, winter, and spring. At some, a shortened summer session, sometimes considered part of the regular academic year, is attended by students on a voluntary or elective basis. Other schools break the year into two main semesters, a first (typically August through December) and a second semester (January through May). Each of these main semesters may be split in half by mid-term exams, and each of the halves is referred to as a quarter (or term in some countries). There may also be a voluntary summer session or a short January session.\nSome other schools, including some in the United States, have four marking periods. Some schools in the United States, notably Boston Latin School, may divide the year into five or more marking periods. Some state in defense of this that there is perhaps a positive correlation between report frequency and academic achievement.\nThere are typically 180 days of teaching each year in schools in the US, excluding weekends and breaks, while there are 190 days for pupils in state schools in Canada, New Zealand and the United Kingdom, and 200 for pupils in Australia.\nIn India the academic year normally starts from June 1 and ends on May 31. Though schools start closing from mid-March, the actual academic closure is on May 31 and in Nepal it starts from July 15.\nSchools and universities in Australia typically have academic years that roughly align with the calendar year (i.e., starting in February or March and ending in October to December), as the southern hemisphere experiences summer from December to February.\n\n\n== Astronomical years ==\n\n\n=== Julian year ===\n\nThe Julian year, as used in astronomy and other sciences, is a time unit defined as exactly 365.25 days of 86,400 SI seconds each (\"ephemeris days\"). This is the normal meaning of the unit \"year\" used in various scientific contexts. The Julian century of 36525 ephemeris days and the Julian millennium of 365250 ephemeris days are used in astronomical calculations. Fundamentally, expressing a time interval in Julian years is a way to precisely specify an amount of time (not how many \"real\" years), for long time intervals where stating the number of ephemeris days would be unwieldy and unintuitive. By convention, the Julian year is used in the computation of the distance covered by a light-year.\nIn the Unified Code for Units of Measure (but not according to the International Union of Pure and Applied Physics or the International Union of Geological Sciences, see below), the symbol a (without subscript) always refers to the Julian year, aj, of exactly 31557600 seconds.\n\n365.25 d \u00d7 86400 s = 1 a = 1 aj = 31.5576 Ms\nThe SI multiplier prefixes may be applied to it to form \"ka\", \"Ma\", etc.\n\n\n=== Sidereal, tropical, and anomalistic years ===\n\nEach of these three years can be loosely called an astronomical year.\nThe sidereal year is the time taken for the Earth to complete one revolution of its orbit, as measured against a fixed frame of reference (such as the fixed stars, Latin sidera, singular sidus). Its average duration is 365.256363004 days (365 d 6 h 9 min 9.76 s) (at the epoch J2000.0 = January 1, 2000, 12:00:00 TT).\nToday the mean tropical year is defined as the period of time for the mean ecliptic longitude of the Sun to increase by 360 degrees. Since the Sun's ecliptic longitude is measured with respect to the equinox, the tropical year comprises a complete cycle of the seasons and is the basis of solar calendars such as the internationally used Gregorian calendar. The modern definition of mean tropical year differs from the actual time between passages of, e.g., the northward equinox, by a minute or two, for several reasons explained below. Because of the Earth's axial precession, this year is about 20 minutes shorter than the sidereal year. The mean tropical year is approximately 365 days, 5 hours, 48 minutes, 45 seconds, using the modern definition ( = 365.24219 d \u00d7 86\u2009400 s). The length of the tropical year varies a bit over thousands of years because the rate of axial precession is not constant.\nThe anomalistic year is the time taken for the Earth to complete one revolution with respect to its apsides. The orbit of the Earth is elliptical; the extreme points, called apsides, are the perihelion, where the Earth is closest to the Sun, and the aphelion, where the Earth is farthest from the Sun. The anomalistic year is usually defined as the time between perihelion passages. Its average duration is 365.259636 days (365 d 6 h 13 min 52.6 s) (at the epoch J2011.0).\n\n\n=== Draconic year ===\n\nThe draconic year, draconitic year, eclipse year, or ecliptic year is the time taken for the Sun (as seen from the Earth) to complete one revolution with respect to the same lunar node (a point where the Moon's orbit intersects the ecliptic). The year is associated with eclipses: these occur only when both the Sun and the Moon are near these nodes; so eclipses occur within about a month of every half eclipse year. Hence there are two eclipse seasons every eclipse year. The average duration of the eclipse year is\n\n346.620075883 days (346 d 14 h 52 min 54 s) (at the epoch J2000.0).\nThis term is sometimes erroneously used for the draconic or nodal period of lunar precession, that is the period of a complete revolution of the Moon's ascending node around the ecliptic: 18.612815932 Julian years (6798.331019 days; at the epoch J2000.0).\n\n\n=== Full moon cycle ===\nThe full moon cycle is the time for the Sun (as seen from the Earth) to complete one revolution with respect to the perigee of the Moon's orbit. This period is associated with the apparent size of the full moon, and also with the varying duration of the synodic month. The duration of one full moon cycle is:\n\n411.78443029 days (411 days 18 hours 49 minutes 35 seconds) (at the epoch J2000.0).\n\n\n=== Lunar year ===\nThe lunar year comprises twelve full cycles of the phases of the Moon, as seen from Earth. It has a duration of approximately 354.37 days. Muslims use this for celebrating their Eids and for marking the start of the fasting month of Ramadan. A Muslim calendar year is based on the lunar cycle. The Jewish calendar is also essentially lunar, except that an intercalary lunar month is added once every two or three years, in order to keep the calendar synchronized with the solar cycle as well. Thus, a lunar year on the Jewish (Hebrew) calendar consists of either twelve or thirteen lunar months.\n\n\n=== Vague year ===\nThe vague year, from annus vagus or wandering year, is an integral approximation to the year equaling 365 days, which wanders in relation to more exact years. Typically the vague year is divided into 12 schematic months of 30 days each plus 5 epagomenal days. The vague year was used in the calendars of Ethiopia, Ancient Egypt, Iran, Armenia and in Mesoamerica among the Aztecs and Maya. It is still used by many Zoroastrian communities.\n\n\n=== Heliacal year ===\nA heliacal year is the interval between the heliacal risings of a star. It differs from the sidereal year for stars away from the ecliptic due mainly to the precession of the equinoxes.\n\n\n==== Sothic year ====\nThe Sothic year is the heliacal year, the interval between heliacal risings, of the star Sirius. It is currently less than the sidereal year and its duration is very close to the Julian year of 365.25 days.\n\n\n=== Gaussian year ===\nThe Gaussian year is the sidereal year for a planet of negligible mass (relative to the Sun) and unperturbed by other planets that is governed by the Gaussian gravitational constant. Such a planet would be slightly closer to the Sun than Earth's mean distance. Its length is:\n\n365.2568983 days (365 d 6 h 9 min 56 s).\n\n\n=== Besselian year ===\nThe Besselian year is a tropical year that starts when the (fictitious) mean Sun reaches an ecliptic longitude of 280\u00b0. This is currently on or close to January 1. It is named after the 19th-century German astronomer and mathematician Friedrich Bessel. The following equation can be used to compute the current Besselian epoch (in years):\n\nB = 1900.0 + (Julian dateTT \u2212 2415020.31352) / 365.242198781\nThe TT subscript indicates that for this formula, the Julian date should use the Terrestrial Time scale, or its predecessor, ephemeris time.\n\n\n=== Variation in the length of the year and the day ===\n\nThe exact length of an astronomical year changes over time.\n\nThe positions of the equinox and solstice points with respect to the apsides of Earth's orbit change: the equinoxes and solstices move westward relative to the stars because of precession, and the apsides move in the other direction because of the long-term effects of gravitational pull by the other planets. Since the speed of the Earth varies according to its position in its orbit as measured from its perihelion, Earth's speed when in a solstice or equinox point changes over time: if such a point moves toward perihelion, the interval between two passages decreases a little from year to year; if the point moves towards aphelion, that period increases a little from year to year. So a \"tropical year\" measured from one passage of the northward (\"vernal\") equinox to the next, differs from the one measured between passages of the southward (\"autumnal\") equinox. The average over the full orbit does not change because of this, so the length of the average tropical year does not change because of this second-order effect.\nEach planet's movement is perturbed by the gravity of every other planet. This leads to short-term fluctuations in its speed, and therefore its period from year to year. Moreover, it causes long-term changes in its orbit, and therefore also long-term changes in these periods.\nTidal drag between the Earth and the Moon and Sun increases the length of the day and of the month (by transferring angular momentum from the rotation of the Earth to the revolution of the Moon); since the apparent mean solar day is the unit with which we measure the length of the year in civil life, the length of the year appears to decrease. The rotation rate of the Earth is also changed by factors such as post-glacial rebound and sea level rise.\nNumerical value of year variation\nMean year lengths in this section are calculated for 2000, and differences in year lengths, compared to 2000, are given for past and future years. In the tables a day is 86,400 SI seconds long.\n\n\n=== Summary ===\nSome of the year lengths in this table are in average solar days, which are slowly getting longer (at a rate that cannot be exactly predicted in advance) and are now around 86,400.002 SI seconds.\n\nAn average Gregorian year may be said to be 365.2425 days (52.1775 weeks, and if an hour is defined as one twenty-fourth of a day, 8765.82 hours, 525949.2 minutes or 31556952 seconds). Note however that in absolute time the average Gregorian year is not adequately defined unless the period of the averaging (start and end dates) is stated, because each period of 400 years is longer (by more than 1000 seconds) than the preceding one as the rotation of the Earth slows. In this calendar, a common year is 365 days (8760 hours, 525600 minutes or 31536000 seconds), and a leap year is 366 days (8784 hours, 527040 minutes or 31622400 seconds). The 400-year civil cycle of the Gregorian calendar has 146097 days and hence exactly 20871 weeks.\n\n\n== Greater astronomical years ==\n\n\n=== Equinoctial cycle ===\nThe Great Year, or equinoctial cycle, corresponds to a complete revolution of the equinoxes around the ecliptic. Its length is about 25,700 years.\n\n\n=== Galactic year ===\nThe Galactic year is the time it takes Earth's Solar System to revolve once around the Galactic Center. It comprises roughly 230 million Earth years.\n\n\n== Seasonal year ==\n\nA seasonal year is the time between successive recurrences of a seasonal event such as the flooding of a river, the migration of a species of bird, the flowering of a species of plant, the first frost, or the first scheduled game of a certain sport. All of these events can have wide variations of more than a month from year to year.\n\n\n== Symbols and abbreviations ==\nA common symbol for the year as a unit of time is \"a\", taken from the Latin word annus.\nFor example, the U.S. National Institute of Standards and Technology (NIST) Guide for the Use of the International System of Units (SI) supports the symbol \"a\" as the unit of time for a year.\nIn English, the abbreviations \"y\" or \"yr\" are more commonly used in non-scientific literature. In some Earth sciences branches (geology and paleontology), \"kyr, myr, byr\" (thousands, millions, and billions of years, respectively) and similar abbreviations are used to denote intervals of time remote from the present. In astronomy the abbreviations kyr, Myr and Gyr are in common use for kiloyears, megayears and gigayears.\nThe Unified Code for Units of Measure (UCUM) disambiguates the varying symbologies of ISO 1000, ISO 2955 and ANSI X3.50 by using:\n\nat = 365.24219 days for the mean tropical year;\naj = 365.25 days for the mean Julian year;\nag = 365.2425 days for the mean Gregorian year;\nIn the UCUM, the symbol \"a\", without any qualifier, equals 1 aj.\nThe UCUM also minimizes confusion with are, a unit of area, by using the abbreviation \"ar\".\nSince 1989, the International Astronomical Union (IAU) recognizes the symbol \"a\" rather than \"yr\" for a year, notes the different kinds of year, and recommends adopting the Julian year of 365.25 days, unless otherwise specified (IAU Style Manual).\nSince 1987, the International Union of Pure and Applied Physics (IUPAP) notes \"a\" as the general symbol for the time unit year (IUPAP Red Book).\nSince 1993, the International Union of Pure and Applied Chemistry (IUPAC) Green Book also uses the same symbol \"a\", notes the difference between Gregorian year and Julian year, and adopts the former (a=365.2425 days), also noted in the IUPAC Gold Book.\nIn 2011, the IUPAC and the International Union of Geological Sciences jointly recommended defining the \"annus\", with symbol \"a\", as the length of the tropical year in the year 2000:\n\na = 31556925.445 seconds (approximately 365.24219265 ephemeris days)\nThis differs from the above definition of 365.25 days by about 20 parts per million. The joint document says that definitions such as the Julian year \"bear an inherent, pre-programmed obsolescence because of the variability of Earth's orbital movement\", but then proposes using the length of the tropical year as of 2000 AD (specified down to the millisecond), which suffers from the same problem. (The tropical year oscillates with time by more than a minute.)\nThe notation has proved controversial as it conflicts with an earlier convention among geoscientists to use \"a\" specifically for \"years ago\" (e.g. 1 Ma for 1 million years ago), and \"y\" or \"yr\" for a one-year time period.\nHowever, this historical practice does not comply with the NIST Guide, considering the unacceptability of mixing information concerning the physical quantity being measured (in this case, time intervals or points in time) with the units and also the unacceptability of using abbreviations for units.\nFurthermore, according to the UK Metric Association (UKMA), language-independent symbols are more universally understood (UKMA Style guide).\n\n\n=== SI prefix multipliers ===\n\nFor the following, there are alternative forms that elide the consecutive vowels, such as kilannus, megannus, etc. The exponents and exponential notations are typically used for calculating and in displaying calculations, and for conserving space, as in tables of data.\n\n\n=== Abbreviations for \"years ago\" ===\n\nIn geology and paleontology, a distinction sometimes is made between abbreviation \"yr\" for years and \"ya\" for years ago, combined with prefixes for thousand, million, or billion. In archaeology, dealing with more recent periods, normally expressed dates, e.g. \"10,000 BC\", may be used as a more traditional form than Before Present (\"BP\").\nThese abbreviations include:\n\nUse of \"mya\" and \"bya\" is deprecated in modern geophysics, the recommended usage being \"Ma\" and \"Ga\" for dates Before Present, but \"m.y.\" for the durations of epochs. This ad hoc distinction between \"absolute\" time and time intervals is somewhat controversial amongst members of the Geological Society of America.\n\n\n== See also ==\n\n\n== References ==\n\n\n=== Notes ===\n\n\n== Further reading ==\nFraser, Julius Thomas (1987). Time, the Familiar Stranger (illustrated ed.). Amherst: University of Massachusetts Press. Bibcode:1988tfs..book.....F. ISBN 978-0-87023-576-4. OCLC 15790499.\nWhitrow, Gerald James (2003). What is Time?. Oxford: Oxford University Press. ISBN 978-0-19-860781-6. OCLC 265440481.",
        "unit": "gigayear",
        "url": "https://en.wikipedia.org/wiki/Gigayear"
    },
    {
        "_id": "Magnetic_field",
        "clean": "Magnetic field",
        "text": "A magnetic field (sometimes called B-field) is a physical field that describes the magnetic influence on moving electric charges, electric currents,:\u200ach1\u200a and magnetic materials. A moving charge in a magnetic field experiences a force perpendicular to its own velocity and to the magnetic field.:\u200ach13\u200a:\u200a278\u200a A permanent magnet's magnetic field pulls on ferromagnetic materials such as iron, and attracts or repels other magnets. In addition, a nonuniform magnetic field exerts minuscule forces on \"nonmagnetic\" materials by three other magnetic effects: paramagnetism, diamagnetism, and antiferromagnetism, although these forces are usually so small they can only be detected by laboratory equipment. Magnetic fields surround magnetized materials, electric currents, and electric fields varying in time. Since both strength and direction of a magnetic field may vary with location, it is described mathematically by a function assigning a vector to each point of space, called a vector field (more precisely, a pseudovector field).\nIn electromagnetics, the term magnetic field is used for two distinct but closely related vector fields denoted by the symbols B and H. In the International System of Units, the unit of B, magnetic flux density, is the tesla (in SI base units: kilogram per second squared per ampere),:\u200a21\u200a which is equivalent to newton per meter per ampere. The unit of H, magnetic field strength, is ampere per meter (A/m).:\u200a22\u200a B and H differ in how they take the medium and/or magnetization into account. In vacuum, the two fields are related through the vacuum permeability, \n  \n    \n      \n        \n          B\n        \n        \n          /\n        \n        \n          \u03bc\n          \n            0\n          \n        \n        =\n        \n          H\n        \n      \n    \n    {\\displaystyle \\mathbf {B} /\\mu _{0}=\\mathbf {H} }\n  \n; in a magnetized material, the quantities on each side of this equation differ by the magnetization field of the material.\nMagnetic fields are produced by moving electric charges and the intrinsic magnetic moments of elementary particles associated with a fundamental quantum property, their spin.:\u200ach1\u200a Magnetic fields and electric fields are interrelated and are both components of the electromagnetic force, one of the four fundamental forces of nature.\nMagnetic fields are used throughout modern technology, particularly in electrical engineering and electromechanics. Rotating magnetic fields are used in both electric motors and generators. The interaction of magnetic fields in electric devices such as transformers is conceptualized and investigated as magnetic circuits. Magnetic forces give information about the charge carriers in a material through the Hall effect. The Earth produces its own magnetic field, which shields the Earth's ozone layer from the solar wind and is important in navigation using a compass.\n\n\n== Description ==\nThe force on an electric charge depends on its location, speed, and direction; two vector fields are used to describe this force.:\u200ach1\u200a The first is the electric field, which describes the force acting on a stationary charge and gives the component of the force that is independent of motion. The magnetic field, in contrast, describes the component of the force that is proportional to both the speed and direction of charged particles.:\u200ach13\u200a The field is defined by the Lorentz force law and is, at each instant, perpendicular to both the motion of the charge and the force it experiences.\nThere are two different, but closely related vector fields which are both sometimes called the \"magnetic field\" written B and H. While both the best names for these fields and exact interpretation of what these fields represent has been the subject of long running debate, there is wide agreement about how the underlying physics work. Historically, the term \"magnetic field\" was reserved for H while using other terms for B, but many recent textbooks use the term \"magnetic field\" to describe B as well as or in place of H.\nThere are many alternative names for both (see sidebars).\n\n\n=== The B-field ===\n\nThe magnetic field vector B at any point can be defined as the vector that, when used in the Lorentz force law, correctly predicts the force on a charged particle at that point::\u200a204\u200a\n\nHere F is the force on the particle, q is the particle's electric charge, v, is the particle's velocity, and \u00d7 denotes the cross product. The direction of force on the charge can be determined by a mnemonic known as the right-hand rule (see the figure). Using the right hand, pointing the thumb in the direction of the current, and the fingers in the direction of the magnetic field, the resulting force on the charge points outwards from the palm. The force on a negatively charged particle is in the opposite direction. If both the speed and the charge are reversed then the direction of the force remains the same. For that reason a magnetic field measurement (by itself) cannot distinguish whether there is a positive charge moving to the right or a negative charge moving to the left. (Both of these cases produce the same current.) On the other hand, a magnetic field combined with an electric field can distinguish between these, see Hall effect below.\nThe first term in the Lorentz equation is from the theory of electrostatics, and says that a particle of charge q in an electric field E experiences an electric force:\n\n  \n    \n      \n        \n          \n            F\n          \n          \n            electric\n          \n        \n        =\n        q\n        \n          E\n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {F} _{\\text{electric}}=q\\mathbf {E} .}\n  \n\nThe second term is the magnetic force:\n\n  \n    \n      \n        \n          \n            F\n          \n          \n            magnetic\n          \n        \n        =\n        q\n        (\n        \n          v\n        \n        \u00d7\n        \n          B\n        \n        )\n        .\n      \n    \n    {\\displaystyle \\mathbf {F} _{\\text{magnetic}}=q(\\mathbf {v} \\times \\mathbf {B} ).}\n  \n\nUsing the definition of the cross product, the magnetic force can also be written as a scalar equation::\u200a357\u200a\n\n  \n    \n      \n        \n          F\n          \n            magnetic\n          \n        \n        =\n        q\n        v\n        B\n        sin\n        \u2061\n        (\n        \u03b8\n        )\n      \n    \n    {\\displaystyle F_{\\text{magnetic}}=qvB\\sin(\\theta )}\n  \n\nwhere Fmagnetic, v, and B are the scalar magnitude of their respective vectors, and \u03b8 is the angle between the velocity of the particle and the magnetic field. The vector B is defined as the vector field necessary to make the Lorentz force law correctly describe the motion of a charged particle. In other words,:\u200a173\u20134\u200a\n\n[T]he command, \"Measure the direction and magnitude of the vector B at such and such a place,\" calls for the following operations: Take a particle of known charge q. Measure the force on q at rest, to determine E. Then measure the force on the particle when its velocity is v; repeat with v in some other direction. Now find a B that makes the Lorentz force law fit all these results\u2014that is the magnetic field at the place in question.\nThe B field can also be defined by the torque on a magnetic dipole, m.:\u200a174\u200a\n\nThe SI unit of B is tesla (symbol: T). The Gaussian-cgs unit of B is the gauss (symbol: G). (The conversion is 1 T \u2258 10000 G.) One nanotesla corresponds to 1 gamma (symbol: \u03b3).\n\n\n=== The H-field ===\n\nThe magnetic H field is defined::\u200a269\u200a:\u200a192\u200a:\u200ach36\u200a\n\nwhere \n  \n    \n      \n        \n          \u03bc\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\mu _{0}}\n  \n is the vacuum permeability, and M is the magnetization vector. In a vacuum, B and H are proportional to each other. Inside a material they are different (see H and B inside and outside magnetic materials). The SI unit of the H-field is the ampere per metre (A/m), and the CGS unit is the oersted (Oe).:\u200a286\u200a\n\n\n=== Measurement ===\n\nAn instrument used to measure the local magnetic field is known as a magnetometer. Important classes of magnetometers include using induction magnetometers (or search-coil magnetometers) which measure only varying magnetic fields, rotating coil magnetometers, Hall effect magnetometers, NMR magnetometers, SQUID magnetometers, and fluxgate magnetometers. The magnetic fields of distant astronomical objects are measured through their effects on local charged particles. For instance, electrons spiraling around a field line produce synchrotron radiation that is detectable in radio waves. The finest precision for a magnetic field measurement was attained by Gravity Probe B at 5 aT (5\u00d710\u221218 T).\n\n\n=== Visualization ===\n\nThe field can be visualized by a set of magnetic field lines, that follow the direction of the field at each point. The lines can be constructed by measuring the strength and direction of the magnetic field at a large number of points (or at every point in space). Then, mark each location with an arrow (called a vector) pointing in the direction of the local magnetic field with its magnitude proportional to the strength of the magnetic field. Connecting these arrows then forms a set of magnetic field lines. The direction of the magnetic field at any point is parallel to the direction of nearby field lines, and the local density of field lines can be made proportional to its strength. Magnetic field lines are like streamlines in fluid flow, in that they represent a continuous distribution, and a different resolution would show more or fewer lines.\nAn advantage of using magnetic field lines as a representation is that many laws of magnetism (and electromagnetism) can be stated completely and concisely using simple concepts such as the \"number\" of field lines through a surface. These concepts can be quickly \"translated\" to their mathematical form. For example, the number of field lines through a given surface is the surface integral of the magnetic field.:\u200a237\u200a\nVarious phenomena \"display\" magnetic field lines as though the field lines were physical phenomena. For example, iron filings placed in a magnetic field form lines that correspond to \"field lines\". Magnetic field \"lines\" are also visually displayed in polar auroras, in which plasma particle dipole interactions create visible streaks of light that line up with the local direction of Earth's magnetic field.\nField lines can be used as a qualitative tool to visualize magnetic forces. In ferromagnetic substances like iron and in plasmas, magnetic forces can be understood by imagining that the field lines exert a tension, (like a rubber band) along their length, and a pressure perpendicular to their length on neighboring field lines. \"Unlike\" poles of magnets attract because they are linked by many field lines; \"like\" poles repel because their field lines do not meet, but run parallel, pushing on each other.\n\n\n== Magnetic field of permanent magnets ==\n\nPermanent magnets are objects that produce their own persistent magnetic fields. They are made of ferromagnetic materials, such as iron and nickel, that have been magnetized, and they have both a north and a south pole.\nThe magnetic field of permanent magnets can be quite complicated, especially near the magnet. The magnetic field of a small straight magnet is proportional to the magnet's strength (called its magnetic dipole moment m). The equations are non-trivial and depend on the distance from the magnet and the orientation of the magnet. For simple magnets, m points in the direction of a line drawn from the south to the north pole of the magnet. Flipping a bar magnet is equivalent to rotating its m by 180 degrees.\nThe magnetic field of larger magnets can be obtained by modeling them as a collection of a large number of small magnets called dipoles each having their own m. The magnetic field produced by the magnet then is the net magnetic field of these dipoles; any net force on the magnet is a result of adding up the forces on the individual dipoles.\nThere are two simplified models for the nature of these dipoles: the magnetic pole model and the Amperian loop model. These two models produce two different magnetic fields, H and B. Outside a material, though, the two are identical (to a multiplicative constant) so that in many cases the distinction can be ignored. This is particularly true for magnetic fields, such as those due to electric currents, that are not generated by magnetic materials.\nA realistic model of magnetism is more complicated than either of these models; neither model fully explains why materials are magnetic. The monopole model has no experimental support. The Amperian loop model explains some, but not all of a material's magnetic moment. The model predicts that the motion of electrons within an atom are connected to those electrons' orbital magnetic dipole moment, and these orbital moments do contribute to the magnetism seen at the macroscopic level. However, the motion of electrons is not classical, and the spin magnetic moment of electrons (which is not explained by either model) is also a significant contribution to the total moment of magnets.\n\n\n=== Magnetic pole model ===\n\nHistorically, early physics textbooks would model the force and torques between two magnets as due to magnetic poles repelling or attracting each other in the same manner as the Coulomb force between electric charges. At the microscopic level, this model contradicts the experimental evidence, and the pole model of magnetism is no longer the typical way to introduce the concept.:\u200a258\u200a However, it is still sometimes used as a macroscopic model for ferromagnetism due to its mathematical simplicity.\nIn this model, a magnetic H-field is produced by fictitious magnetic charges that are spread over the surface of each pole. These magnetic charges are in fact related to the magnetization field M. The H-field, therefore, is analogous to the electric field E, which starts at a positive electric charge and ends at a negative electric charge. Near the north pole, therefore, all H-field lines point away from the north pole (whether inside the magnet or out) while near the south pole all H-field lines point toward the south pole (whether inside the magnet or out). Too, a north pole feels a force in the direction of the H-field while the force on the south pole is opposite to the H-field.\nIn the magnetic pole model, the elementary magnetic dipole m is formed by two opposite magnetic poles of pole strength qm separated by a small distance vector d, such that m = qm\u2009d. The magnetic pole model predicts correctly the field H both inside and outside magnetic materials, in particular the fact that H is opposite to the magnetization field M inside a permanent magnet.\nSince it is based on the fictitious idea of a magnetic charge density, the pole model has limitations. Magnetic poles cannot exist apart from each other as electric charges can, but always come in north\u2013south pairs. If a magnetized object is divided in half, a new pole appears on the surface of each piece, so each has a pair of complementary poles. The magnetic pole model does not account for magnetism that is produced by electric currents, nor the inherent connection between angular momentum and magnetism.\nThe pole model usually treats magnetic charge as a mathematical abstraction, rather than a physical property of particles. However, a magnetic monopole is a hypothetical particle (or class of particles) that physically has only one magnetic pole (either a north pole or a south pole). In other words, it would possess a \"magnetic charge\" analogous to an electric charge. Magnetic field lines would start or end on magnetic monopoles, so if they exist, they would give exceptions to the rule that magnetic field lines neither start nor end. Some theories (such as Grand Unified Theories) have predicted the existence of magnetic monopoles, but so far, none have been observed.\n\n\n=== Amperian loop model ===\n\nIn the model developed by Ampere, the elementary magnetic dipole that makes up all magnets is a sufficiently small Amperian loop with current I and loop area A. The dipole moment of this loop is m = IA.\nThese magnetic dipoles produce a magnetic B-field.\nThe magnetic field of a magnetic dipole is depicted in the figure. From outside, the ideal magnetic dipole is identical to that of an ideal electric dipole of the same strength. Unlike the electric dipole, a magnetic dipole is properly modeled as a current loop having a current I and an area a. Such a current loop has a magnetic moment of\n\n  \n    \n      \n        m\n        =\n        I\n        a\n        ,\n      \n    \n    {\\displaystyle m=Ia,}\n  \n\nwhere the direction of m is perpendicular to the area of the loop and depends on the direction of the current using the right-hand rule. An ideal magnetic dipole is modeled as a real magnetic dipole whose area a has been reduced to zero and its current I increased to infinity such that the product m = Ia is finite. This model clarifies the connection between angular momentum and magnetic moment, which is the basis of the Einstein\u2013de Haas effect rotation by magnetization and its inverse, the Barnett effect or magnetization by rotation. Rotating the loop faster (in the same direction) increases the current and therefore the magnetic moment, for example.\n\n\n== Interactions with magnets ==\n\n\n=== Force between magnets ===\n\nSpecifying the force between two small magnets is quite complicated because it depends on the strength and orientation of both magnets and their distance and direction relative to each other. The force is particularly sensitive to rotations of the magnets due to magnetic torque. The force on each magnet depends on its magnetic moment and the magnetic field of the other.\nTo understand the force between magnets, it is useful to examine the magnetic pole model given above. In this model, the H-field of one magnet pushes and pulls on both poles of a second magnet. If this H-field is the same at both poles of the second magnet then there is no net force on that magnet since the force is opposite for opposite poles. If, however, the magnetic field of the first magnet is nonuniform (such as the H near one of its poles), each pole of the second magnet sees a different field and is subject to a different force. This difference in the two forces moves the magnet in the direction of increasing magnetic field and may also cause a net torque.\nThis is a specific example of a general rule that magnets are attracted (or repulsed depending on the orientation of the magnet) into regions of higher magnetic field. Any non-uniform magnetic field, whether caused by permanent magnets or electric currents, exerts a force on a small magnet in this way.\nThe details of the Amperian loop model are different and more complicated but yield the same result: that magnetic dipoles are attracted/repelled into regions of higher magnetic field. Mathematically, the force on a small magnet having a magnetic moment m due to a magnetic field B is::\u200aEq. 11.42\u200a\n\n  \n    \n      \n        \n          F\n        \n        =\n        \n          \u2207\n        \n        \n          (\n          \n            \n              m\n            \n            \u22c5\n            \n              B\n            \n          \n          )\n        \n        ,\n      \n    \n    {\\displaystyle \\mathbf {F} ={\\boldsymbol {\\nabla }}\\left(\\mathbf {m} \\cdot \\mathbf {B} \\right),}\n  \n\nwhere the gradient \u2207 is the change of the quantity m \u00b7 B per unit distance and the direction is that of maximum increase of m \u00b7 B. The dot product m \u00b7 B = mBcos(\u03b8), where m and B represent the magnitude of the m and B vectors and \u03b8 is the angle between them. If m is in the same direction as B then the dot product is positive and the gradient points \"uphill\" pulling the magnet into regions of higher B-field (more strictly larger m \u00b7 B). This equation is strictly only valid for magnets of zero size, but is often a good approximation for not too large magnets. The magnetic force on larger magnets is determined by dividing them into smaller regions each having their own m then summing up the forces on each of these very small regions.\n\n\n=== Magnetic torque on permanent magnets ===\n\nIf two like poles of two separate magnets are brought near each other, and one of the magnets is allowed to turn, it promptly rotates to align itself with the first. In this example, the magnetic field of the stationary magnet creates a magnetic torque on the magnet that is free to rotate. This magnetic torque \u03c4 tends to align a magnet's poles with the magnetic field lines. A compass, therefore, turns to align itself with Earth's magnetic field.\n\n In terms of the pole model, two equal and opposite magnetic charges experiencing the same H also experience equal and opposite forces. Since these equal and opposite forces are in different locations, this produces a torque proportional to the distance (perpendicular to the force) between them. With the definition of m as the pole strength times the distance between the poles, this leads to \u03c4 = \u03bc0 m H sin\u2009\u03b8, where \u03bc0 is a constant called the vacuum permeability, measuring 4\u03c0\u00d710\u22127 V\u00b7s/(A\u00b7m) and \u03b8 is the angle between H and m.\nMathematically, the torque \u03c4 on a small magnet is proportional both to the applied magnetic field and to the magnetic moment m of the magnet:\n\n  \n    \n      \n        \n          \u03c4\n        \n        =\n        \n          m\n        \n        \u00d7\n        \n          B\n        \n        =\n        \n          \u03bc\n          \n            0\n          \n        \n        \n          m\n        \n        \u00d7\n        \n          H\n        \n        ,\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\tau }}=\\mathbf {m} \\times \\mathbf {B} =\\mu _{0}\\mathbf {m} \\times \\mathbf {H} ,\\,}\n  \n\nwhere \u00d7 represents the vector cross product. This equation includes all of the qualitative information included above. There is no torque on a magnet if m is in the same direction as the magnetic field, since the cross product is zero for two vectors that are in the same direction. Further, all other orientations feel a torque that twists them toward the direction of magnetic field.\n\n\n== Interactions with electric currents ==\nCurrents of electric charges both generate a magnetic field and feel a force due to magnetic B-fields.\n\n\n=== Magnetic field due to moving charges and electric currents ===\n\nAll moving charged particles produce magnetic fields. Moving point charges, such as electrons, produce complicated but well known magnetic fields that depend on the charge, velocity, and acceleration of the particles.\nMagnetic field lines form in concentric circles around a cylindrical current-carrying conductor, such as a length of wire. The direction of such a magnetic field can be determined by using the \"right-hand grip rule\" (see figure at right). The strength of the magnetic field decreases with distance from the wire. (For an infinite length wire the strength is inversely proportional to the distance.)\n\nBending a current-carrying wire into a loop concentrates the magnetic field inside the loop while weakening it outside. Bending a wire into multiple closely spaced loops to form a coil or \"solenoid\" enhances this effect. A device so formed around an iron core may act as an electromagnet, generating a strong, well-controlled magnetic field. An infinitely long cylindrical electromagnet has a uniform magnetic field inside, and no magnetic field outside. A finite length electromagnet produces a magnetic field that looks similar to that produced by a uniform permanent magnet, with its strength and polarity determined by the current flowing through the coil.\nThe magnetic field generated by a steady current I (a constant flow of electric charges, in which charge neither accumulates nor is depleted at any point) is described by the Biot\u2013Savart law::\u200a224\u200a\n\n  \n    \n      \n        \n          B\n        \n        =\n        \n          \n            \n              \n                \u03bc\n                \n                  0\n                \n              \n              I\n            \n            \n              4\n              \u03c0\n            \n          \n        \n        \n          \u222b\n          \n            \n              w\n              i\n              r\n              e\n            \n          \n        \n        \n          \n            \n              \n                d\n              \n              \n                \u2113\n              \n              \u00d7\n              \n                \n                  \n                    r\n                    ^\n                  \n                \n              \n            \n            \n              r\n              \n                2\n              \n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle \\mathbf {B} ={\\frac {\\mu _{0}I}{4\\pi }}\\int _{\\mathrm {wire} }{\\frac {\\mathrm {d} {\\boldsymbol {\\ell }}\\times \\mathbf {\\hat {r}} }{r^{2}}},}\n  \n\nwhere the integral sums over the wire length where vector d\u2113 is the vector line element with direction in the same sense as the current I, \u03bc0 is the magnetic constant, r is the distance between the location of d\u2113 and the location where the magnetic field is calculated, and r\u0302 is a unit vector in the direction of r. For example, in the case of a sufficiently long, straight wire, this becomes:\n\n  \n    \n      \n        \n          |\n        \n        \n          B\n        \n        \n          |\n        \n        =\n        \n          \n            \n              \u03bc\n              \n                0\n              \n            \n            \n              2\n              \u03c0\n              r\n            \n          \n        \n        I\n      \n    \n    {\\displaystyle |\\mathbf {B} |={\\frac {\\mu _{0}}{2\\pi r}}I}\n  \n\nwhere r = |r|. The direction is tangent to a circle perpendicular to the wire according to the right hand rule.:\u200a225\u200a\nA slightly more general way of relating the current \n  \n    \n      \n        I\n      \n    \n    {\\displaystyle I}\n  \n to the B-field is through Amp\u00e8re's law:\n\n  \n    \n      \n        \u222e\n        \n          B\n        \n        \u22c5\n        \n          d\n        \n        \n          \u2113\n        \n        =\n        \n          \u03bc\n          \n            0\n          \n        \n        \n          I\n          \n            \n              e\n              n\n              c\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle \\oint \\mathbf {B} \\cdot \\mathrm {d} {\\boldsymbol {\\ell }}=\\mu _{0}I_{\\mathrm {enc} },}\n  \n\nwhere the line integral is over any arbitrary loop and \n  \n    \n      \n        \n          I\n          \n            enc\n          \n        \n      \n    \n    {\\displaystyle I_{\\text{enc}}}\n  \n is the current enclosed by that loop. Amp\u00e8re's law is always valid for steady currents and can be used to calculate the B-field for certain highly symmetric situations such as an infinite wire or an infinite solenoid.\nIn a modified form that accounts for time varying electric fields, Amp\u00e8re's law is one of four Maxwell's equations that describe electricity and magnetism.\n\n\n=== Force on moving charges and current ===\n\n\n==== Force on a charged particle ====\n\nA charged particle moving in a B-field experiences a sideways force that is proportional to the strength of the magnetic field, the component of the velocity that is perpendicular to the magnetic field and the charge of the particle. This force is known as the Lorentz force, and is given by\n\n  \n    \n      \n        \n          F\n        \n        =\n        q\n        \n          E\n        \n        +\n        q\n        \n          v\n        \n        \u00d7\n        \n          B\n        \n        ,\n      \n    \n    {\\displaystyle \\mathbf {F} =q\\mathbf {E} +q\\mathbf {v} \\times \\mathbf {B} ,}\n  \n\nwhere F is the force, q is the electric charge of the particle, v is the instantaneous velocity of the particle, and B is the magnetic field (in teslas).\nThe Lorentz force is always perpendicular to both the velocity of the particle and the magnetic field that created it. When a charged particle moves in a static magnetic field, it traces a helical path in which the helix axis is parallel to the magnetic field, and in which the speed of the particle remains constant. Because the magnetic force is always perpendicular to the motion, the magnetic field can do no work on an isolated charge. It can only do work indirectly, via the electric field generated by a changing magnetic field. It is often claimed that the magnetic force can do work to a non-elementary magnetic dipole, or to charged particles whose motion is constrained by other forces, but this is incorrect because the work in those cases is performed by the electric forces of the charges deflected by the magnetic field.\n\n\n==== Force on current-carrying wire ====\n\nThe force on a current carrying wire is similar to that of a moving charge as expected since a current carrying wire is a collection of moving charges. A current-carrying wire feels a force in the presence of a magnetic field. The Lorentz force on a macroscopic current is often referred to as the Laplace force.\nConsider a conductor of length \u2113, cross section A, and charge q due to electric current i. If this conductor is placed in a magnetic field of magnitude B that makes an angle \u03b8 with the velocity of charges in the conductor, the force exerted on a single charge q is\n\n  \n    \n      \n        F\n        =\n        q\n        v\n        B\n        sin\n        \u2061\n        \u03b8\n        ,\n      \n    \n    {\\displaystyle F=qvB\\sin \\theta ,}\n  \n\nso, for N charges where\n\n  \n    \n      \n        N\n        =\n        n\n        \u2113\n        A\n        ,\n      \n    \n    {\\displaystyle N=n\\ell A,}\n  \n\nthe force exerted on the conductor is\n\n  \n    \n      \n        f\n        =\n        F\n        N\n        =\n        q\n        v\n        B\n        n\n        \u2113\n        A\n        sin\n        \u2061\n        \u03b8\n        =\n        B\n        i\n        \u2113\n        sin\n        \u2061\n        \u03b8\n        ,\n      \n    \n    {\\displaystyle f=FN=qvBn\\ell A\\sin \\theta =Bi\\ell \\sin \\theta ,}\n  \n\nwhere i = nqvA.\n\n\n== Relation between H and B ==\nThe formulas derived for the magnetic field above are correct when dealing with the entire current. A magnetic material placed inside a magnetic field, though, generates its own bound current, which can be a challenge to calculate. (This bound current is due to the sum of atomic sized current loops and the spin of the subatomic particles such as electrons that make up the material.) The H-field as defined above helps factor out this bound current; but to see how, it helps to introduce the concept of magnetization first.\n\n\n=== Magnetization ===\n\nThe magnetization vector field M represents how strongly a region of material is magnetized. It is defined as the net magnetic dipole moment per unit volume of that region. The magnetization of a uniform magnet is therefore a material constant, equal to the magnetic moment m of the magnet divided by its volume. Since the SI unit of magnetic moment is A\u22c5m2, the SI unit of magnetization M is ampere per meter, identical to that of the H-field.\nThe magnetization M field of a region points in the direction of the average magnetic dipole moment in that region. Magnetization field lines, therefore, begin near the magnetic south pole and ends near the magnetic north pole. (Magnetization does not exist outside the magnet.)\nIn the Amperian loop model, the magnetization is due to combining many tiny Amperian loops to form a resultant current called bound current. This bound current, then, is the source of the magnetic B field due to the magnet. Given the definition of the magnetic dipole, the magnetization field follows a similar law to that of Ampere's law:\n\n  \n    \n      \n        \u222e\n        \n          M\n        \n        \u22c5\n        \n          d\n        \n        \n          \u2113\n        \n        =\n        \n          I\n          \n            \n              b\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle \\oint \\mathbf {M} \\cdot \\mathrm {d} {\\boldsymbol {\\ell }}=I_{\\mathrm {b} },}\n  \n\nwhere the integral is a line integral over any closed loop and Ib is the bound current enclosed by that closed loop.\nIn the magnetic pole model, magnetization begins at and ends at magnetic poles. If a given region, therefore, has a net positive \"magnetic pole strength\" (corresponding to a north pole) then it has more magnetization field lines entering it than leaving it. Mathematically this is equivalent to:\n\n  \n    \n      \n        \n          \u222e\n          \n            S\n          \n        \n        \n          \u03bc\n          \n            0\n          \n        \n        \n          M\n        \n        \u22c5\n        \n          d\n        \n        \n          A\n        \n        =\n        \u2212\n        \n          q\n          \n            \n              M\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle \\oint _{S}\\mu _{0}\\mathbf {M} \\cdot \\mathrm {d} \\mathbf {A} =-q_{\\mathrm {M} },}\n  \n\nwhere the integral is a closed surface integral over the closed surface S and qM is the \"magnetic charge\" (in units of magnetic flux) enclosed by S. (A closed surface completely surrounds a region with no holes to let any field lines escape.) The negative sign occurs because the magnetization field moves from south to north.\n\n\n=== H-field and magnetic materials ===\n\nIn SI units, the H-field is related to the B-field by\n\n  \n    \n      \n        \n          H\n        \n         \n        \u2261\n         \n        \n          \n            \n              B\n            \n            \n              \u03bc\n              \n                0\n              \n            \n          \n        \n        \u2212\n        \n          M\n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {H} \\ \\equiv \\ {\\frac {\\mathbf {B} }{\\mu _{0}}}-\\mathbf {M} .}\n  \n\nIn terms of the H-field, Ampere's law is\n\n  \n    \n      \n        \u222e\n        \n          H\n        \n        \u22c5\n        \n          d\n        \n        \n          \u2113\n        \n        =\n        \u222e\n        \n          (\n          \n            \n              \n                \n                  B\n                \n                \n                  \u03bc\n                  \n                    0\n                  \n                \n              \n            \n            \u2212\n            \n              M\n            \n          \n          )\n        \n        \u22c5\n        \n          d\n        \n        \n          \u2113\n        \n        =\n        \n          I\n          \n            \n              t\n              o\n              t\n            \n          \n        \n        \u2212\n        \n          I\n          \n            \n              b\n            \n          \n        \n        =\n        \n          I\n          \n            \n              f\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle \\oint \\mathbf {H} \\cdot \\mathrm {d} {\\boldsymbol {\\ell }}=\\oint \\left({\\frac {\\mathbf {B} }{\\mu _{0}}}-\\mathbf {M} \\right)\\cdot \\mathrm {d} {\\boldsymbol {\\ell }}=I_{\\mathrm {tot} }-I_{\\mathrm {b} }=I_{\\mathrm {f} },}\n  \n\nwhere If represents the 'free current' enclosed by the loop so that the line integral of H does not depend at all on the bound currents.\nFor the differential equivalent of this equation see Maxwell's equations. Ampere's law leads to the boundary condition\n\n  \n    \n      \n        \n          (\n          \n            \n              \n                H\n                \n                  1\n                \n                \n                  \u2225\n                \n              \n            \n            \u2212\n            \n              \n                H\n                \n                  2\n                \n                \n                  \u2225\n                \n              \n            \n          \n          )\n        \n        =\n        \n          \n            K\n          \n          \n            \n              f\n            \n          \n        \n        \u00d7\n        \n          \n            \n              \n                n\n              \n              ^\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle \\left(\\mathbf {H_{1}^{\\parallel }} -\\mathbf {H_{2}^{\\parallel }} \\right)=\\mathbf {K} _{\\mathrm {f} }\\times {\\hat {\\mathbf {n} }},}\n  \n\nwhere Kf is the surface free current density and the unit normal \n  \n    \n      \n        \n          \n            \n              \n                n\n              \n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {\\mathbf {n} }}}\n  \n points in the direction from medium 2 to medium 1.\nSimilarly, a surface integral of H over any closed surface is independent of the free currents and picks out the \"magnetic charges\" within that closed surface:\n\n  \n    \n      \n        \n          \u222e\n          \n            S\n          \n        \n        \n          \u03bc\n          \n            0\n          \n        \n        \n          H\n        \n        \u22c5\n        \n          d\n        \n        \n          A\n        \n        =\n        \n          \u222e\n          \n            S\n          \n        \n        (\n        \n          B\n        \n        \u2212\n        \n          \u03bc\n          \n            0\n          \n        \n        \n          M\n        \n        )\n        \u22c5\n        \n          d\n        \n        \n          A\n        \n        =\n        0\n        \u2212\n        (\n        \u2212\n        \n          q\n          \n            \n              M\n            \n          \n        \n        )\n        =\n        \n          q\n          \n            \n              M\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle \\oint _{S}\\mu _{0}\\mathbf {H} \\cdot \\mathrm {d} \\mathbf {A} =\\oint _{S}(\\mathbf {B} -\\mu _{0}\\mathbf {M} )\\cdot \\mathrm {d} \\mathbf {A} =0-(-q_{\\mathrm {M} })=q_{\\mathrm {M} },}\n  \n\nwhich does not depend on the free currents.\nThe H-field, therefore, can be separated into two independent parts:\n\n  \n    \n      \n        \n          H\n        \n        =\n        \n          \n            H\n          \n          \n            0\n          \n        \n        +\n        \n          \n            H\n          \n          \n            \n              d\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle \\mathbf {H} =\\mathbf {H} _{0}+\\mathbf {H} _{\\mathrm {d} },}\n  \n\nwhere H0 is the applied magnetic field due only to the free currents and Hd is the demagnetizing field due only to the bound currents.\nThe magnetic H-field, therefore, re-factors the bound current in terms of \"magnetic charges\". The H field lines loop only around \"free current\" and, unlike the magnetic B field, begins and ends near magnetic poles as well.\n\n\n=== Magnetism ===\n\nMost materials respond to an applied B-field by producing their own magnetization M and therefore their own B-fields. Typically, the response is weak and exists only when the magnetic field is applied. The term magnetism describes how materials respond on the microscopic level to an applied magnetic field and is used to categorize the magnetic phase of a material. Materials are divided into groups based upon their magnetic behavior:\n\nDiamagnetic materials produce a magnetization that opposes the magnetic field.\nParamagnetic materials produce a magnetization in the same direction as the applied magnetic field.\nFerromagnetic materials and the closely related ferrimagnetic materials and antiferromagnetic materials can have a magnetization independent of an applied B-field with a complex relationship between the two fields.\nSuperconductors (and ferromagnetic superconductors) are materials that are characterized by perfect conductivity below a critical temperature and magnetic field. They also are highly magnetic and can be perfect diamagnets below a lower critical magnetic field. Superconductors often have a broad range of temperatures and magnetic fields (the so-named mixed state) under which they exhibit a complex hysteretic dependence of M on B.\nIn the case of paramagnetism and diamagnetism, the magnetization M is often proportional to the applied magnetic field such that:\n\n  \n    \n      \n        \n          B\n        \n        =\n        \u03bc\n        \n          H\n        \n        ,\n      \n    \n    {\\displaystyle \\mathbf {B} =\\mu \\mathbf {H} ,}\n  \n\nwhere \u03bc is a material dependent parameter called the permeability. In some cases the permeability may be a second rank tensor so that H may not point in the same direction as B. These relations between B and H are examples of constitutive equations. However, superconductors and ferromagnets have a more complex B-to-H relation; see magnetic hysteresis.\n\n\n== Stored energy ==\n\nEnergy is needed to generate a magnetic field both to work against the electric field that a changing magnetic field creates and to change the magnetization of any material within the magnetic field. For non-dispersive materials, this same energy is released when the magnetic field is destroyed so that the energy can be modeled as being stored in the magnetic field.\nFor linear, non-dispersive, materials (such that B = \u03bcH where \u03bc is frequency-independent), the energy density is:\n\n  \n    \n      \n        u\n        =\n        \n          \n            \n              \n                B\n              \n              \u22c5\n              \n                H\n              \n            \n            2\n          \n        \n        =\n        \n          \n            \n              \n                B\n              \n              \u22c5\n              \n                B\n              \n            \n            \n              2\n              \u03bc\n            \n          \n        \n        =\n        \n          \n            \n              \u03bc\n              \n                H\n              \n              \u22c5\n              \n                H\n              \n            \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle u={\\frac {\\mathbf {B} \\cdot \\mathbf {H} }{2}}={\\frac {\\mathbf {B} \\cdot \\mathbf {B} }{2\\mu }}={\\frac {\\mu \\mathbf {H} \\cdot \\mathbf {H} }{2}}.}\n  \n\nIf there are no magnetic materials around then \u03bc can be replaced by \u03bc0. The above equation cannot be used for nonlinear materials, though; a more general expression given below must be used.\nIn general, the incremental amount of work per unit volume \u03b4W needed to cause a small change of magnetic field \u03b4B is:\n\n  \n    \n      \n        \u03b4\n        W\n        =\n        \n          H\n        \n        \u22c5\n        \u03b4\n        \n          B\n        \n        .\n      \n    \n    {\\displaystyle \\delta W=\\mathbf {H} \\cdot \\delta \\mathbf {B} .}\n  \n\nOnce the relationship between H and B is known this equation is used to determine the work needed to reach a given magnetic state. For hysteretic materials such as ferromagnets and superconductors, the work needed also depends on how the magnetic field is created. For linear non-dispersive materials, though, the general equation leads directly to the simpler energy density equation given above.\n\n\n== Appearance in Maxwell's equations ==\n\nLike all vector fields, a magnetic field has two important mathematical properties that relates it to its sources. (For B the sources are currents and changing electric fields.) These two properties, along with the two corresponding properties of the electric field, make up Maxwell's Equations. Maxwell's Equations together with the Lorentz force law form a complete description of classical electrodynamics including both electricity and magnetism.\nThe first property is the divergence of a vector field A, \u2207 \u00b7 A, which represents how A \"flows\" outward from a given point. As discussed above, a B-field line never starts or ends at a point but instead forms a complete loop. This is mathematically equivalent to saying that the divergence of B is zero. (Such vector fields are called solenoidal vector fields.) This property is called Gauss's law for magnetism and is equivalent to the statement that there are no isolated magnetic poles or magnetic monopoles.\nThe second mathematical property is called the curl, such that \u2207 \u00d7 A represents how A curls or \"circulates\" around a given point. The result of the curl is called a \"circulation source\". The equations for the curl of B and of E are called the Amp\u00e8re\u2013Maxwell equation and Faraday's law respectively.\n\n\n=== Gauss' law for magnetism ===\n\nOne important property of the B-field produced this way is that magnetic B-field lines neither start nor end (mathematically, B is a solenoidal vector field); a field line may only extend to infinity, or wrap around to form a closed curve, or follow a never-ending (possibly chaotic) path. Magnetic field lines exit a magnet near its north pole and enter near its south pole, but inside the magnet B-field lines continue through the magnet from the south pole back to the north. If a B-field line enters a magnet somewhere it has to leave somewhere else; it is not allowed to have an end point.\nMore formally, since all the magnetic field lines that enter any given region must also leave that region, subtracting the \"number\" of field lines that enter the region from the number that exit gives identically zero. Mathematically this is equivalent to Gauss's law for magnetism:\n\n  \n    \n      \n        \n          \u222e\n          \n            S\n          \n        \n        \n          B\n        \n        \u22c5\n        \n          d\n        \n        \n          A\n        \n        =\n        0\n      \n    \n    {\\displaystyle \\oint _{S}\\mathbf {B} \\cdot \\mathrm {d} \\mathbf {A} =0}\n  \n\nwhere the integral is a surface integral over the closed surface S (a closed surface is one that completely surrounds a region with no holes to let any field lines escape). Since dA points outward, the dot product in the integral is positive for B-field pointing out and negative for B-field pointing in.\n\n\n=== Faraday's Law ===\n\nA changing magnetic field, such as a magnet moving through a conducting coil, generates an electric field (and therefore tends to drive a current in such a coil). This is known as Faraday's law and forms the basis of many electrical generators and electric motors. Mathematically, Faraday's law is:\n\n  \n    \n      \n        \n          \n            E\n          \n        \n        =\n        \u2212\n        \n          \n            \n              \n                d\n              \n              \u03a6\n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {E}}=-{\\frac {\\mathrm {d} \\Phi }{\\mathrm {d} t}}}\n  \n\nwhere \n  \n    \n      \n        \n          \n            E\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {E}}}\n  \n is the electromotive force (or EMF, the voltage generated around a closed loop) and \u03a6 is the magnetic flux\u2014the product of the area times the magnetic field normal to that area. (This definition of magnetic flux is why B is often referred to as magnetic flux density.):\u200a210\u200a The negative sign represents the fact that any current generated by a changing magnetic field in a coil produces a magnetic field that opposes the change in the magnetic field that induced it. This phenomenon is known as Lenz's law. This integral formulation of Faraday's law can be converted into a differential form, which applies under slightly different conditions.\n\n  \n    \n      \n        \u2207\n        \u00d7\n        \n          E\n        \n        =\n        \u2212\n        \n          \n            \n              \u2202\n              \n                B\n              \n            \n            \n              \u2202\n              t\n            \n          \n        \n      \n    \n    {\\displaystyle \\nabla \\times \\mathbf {E} =-{\\frac {\\partial \\mathbf {B} }{\\partial t}}}\n  \n\n\n=== Amp\u00e8re's Law and Maxwell's correction ===\n\nSimilar to the way that a changing magnetic field generates an electric field, a changing electric field generates a magnetic field. This fact is known as Maxwell's correction to Amp\u00e8re's law and is applied as an additive term to Ampere's law as given above. This additional term is proportional to the time rate of change of the electric flux and is similar to Faraday's law above but with a different and positive constant out front. (The electric flux through an area is proportional to the area times the perpendicular part of the electric field.)\nThe full law including the correction term is known as the Maxwell\u2013Amp\u00e8re equation. It is not commonly given in integral form because the effect is so small that it can typically be ignored in most cases where the integral form is used.\nThe Maxwell term is critically important in the creation and propagation of electromagnetic waves. Maxwell's correction to Amp\u00e8re's Law together with Faraday's law of induction describes how mutually changing electric and magnetic fields interact to sustain each other and thus to form electromagnetic waves, such as light: a changing electric field generates a changing magnetic field, which generates a changing electric field again. These, though, are usually described using the differential form of this equation given below.\n\n  \n    \n      \n        \u2207\n        \u00d7\n        \n          B\n        \n        =\n        \n          \u03bc\n          \n            0\n          \n        \n        \n          J\n        \n        +\n        \n          \u03bc\n          \n            0\n          \n        \n        \n          \u03b5\n          \n            0\n          \n        \n        \n          \n            \n              \u2202\n              \n                E\n              \n            \n            \n              \u2202\n              t\n            \n          \n        \n      \n    \n    {\\displaystyle \\nabla \\times \\mathbf {B} =\\mu _{0}\\mathbf {J} +\\mu _{0}\\varepsilon _{0}{\\frac {\\partial \\mathbf {E} }{\\partial t}}}\n  \n\nwhere J is the complete microscopic current density, and \u03b50 is the vacuum permittivity.\nAs discussed above, materials respond to an applied electric E field and an applied magnetic B field by producing their own internal \"bound\" charge and current distributions that contribute to E and B but are difficult to calculate. To circumvent this problem, H and D fields are used to re-factor Maxwell's equations in terms of the free current density Jf:\n\n  \n    \n      \n        \u2207\n        \u00d7\n        \n          H\n        \n        =\n        \n          \n            J\n          \n          \n            \n              f\n            \n          \n        \n        +\n        \n          \n            \n              \u2202\n              \n                D\n              \n            \n            \n              \u2202\n              t\n            \n          \n        \n      \n    \n    {\\displaystyle \\nabla \\times \\mathbf {H} =\\mathbf {J} _{\\mathrm {f} }+{\\frac {\\partial \\mathbf {D} }{\\partial t}}}\n  \n\nThese equations are not any more general than the original equations (if the \"bound\" charges and currents in the material are known). They also must be supplemented by the relationship between B and H as well as that between E and D. On the other hand, for simple relationships between these quantities this form of Maxwell's equations can circumvent the need to calculate the bound charges and currents.\n\n\n== Formulation in special relativity and quantum electrodynamics ==\n\n\n=== Relativistic electrodynamics ===\n\n\n==== As different aspects of the same phenomenon ====\nAccording to the special theory of relativity, the partition of the electromagnetic force into separate electric and magnetic components is not fundamental, but varies with the observational frame of reference: An electric force perceived by one observer may be perceived by another (in a different frame of reference) as a magnetic force, or a mixture of electric and magnetic forces.\nThe magnetic field existing as electric field in other frames can be shown by consistency of equations obtained from Lorentz transformation of four force from Coulomb's Law in particle's rest frame with Maxwell's laws considering definition of fields from Lorentz force and for non accelerating condition. The form of magnetic field hence obtained by Lorentz transformation of four-force from the form of Coulomb's law in source's initial frame is given by:\n  \n    \n      \n        \n          B\n        \n        =\n        \n          \n            q\n            \n              4\n              \u03c0\n              \n                \u03b5\n                \n                  0\n                \n              \n              \n                r\n                \n                  3\n                \n              \n            \n          \n        \n        \n          \n            \n              1\n              \u2212\n              \n                \u03b2\n                \n                  2\n                \n              \n            \n            \n              (\n              1\n              \u2212\n              \n                \u03b2\n                \n                  2\n                \n              \n              \n                sin\n                \n                  2\n                \n              \n              \u2061\n              \u03b8\n              \n                )\n                \n                  3\n                  \n                    /\n                  \n                  2\n                \n              \n            \n          \n        \n        \n          \n            \n              \n                v\n              \n              \u00d7\n              \n                r\n              \n            \n            \n              c\n              \n                2\n              \n            \n          \n        \n        =\n        \n          \n            \n              \n                v\n              \n              \u00d7\n              \n                E\n              \n            \n            \n              c\n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {B} ={\\frac {q}{4\\pi \\varepsilon _{0}r^{3}}}{\\frac {1-\\beta ^{2}}{(1-\\beta ^{2}\\sin ^{2}\\theta )^{3/2}}}{\\frac {\\mathbf {v} \\times \\mathbf {r} }{c^{2}}}={\\frac {\\mathbf {v} \\times \\mathbf {E} }{c^{2}}}}\n  \nwhere \n  \n    \n      \n        q\n      \n    \n    {\\displaystyle q}\n  \n is the charge of the point source, \n  \n    \n      \n        \n          \u03b5\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\varepsilon _{0}}\n  \n is the vacuum permittivity, \n  \n    \n      \n        \n          r\n        \n      \n    \n    {\\displaystyle \\mathbf {r} }\n  \n is the position vector from the point source to the point in space, \n  \n    \n      \n        \n          v\n        \n      \n    \n    {\\displaystyle \\mathbf {v} }\n  \n is the velocity vector of the charged particle, \n  \n    \n      \n        \u03b2\n      \n    \n    {\\displaystyle \\beta }\n  \n is the ratio of speed of the charged particle divided by the speed of light and \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n  \n is the angle between \n  \n    \n      \n        \n          r\n        \n      \n    \n    {\\displaystyle \\mathbf {r} }\n  \n and \n  \n    \n      \n        \n          v\n        \n      \n    \n    {\\displaystyle \\mathbf {v} }\n  \n. This form of magnetic field can be shown to satisfy maxwell's laws within the constraint of particle being non accelerating. The above reduces to Biot-Savart law for non relativistic stream of current (\n  \n    \n      \n        \u03b2\n        \u226a\n        1\n      \n    \n    {\\displaystyle \\beta \\ll 1}\n  \n).\nFormally, special relativity combines the electric and magnetic fields into a rank-2 tensor, called the electromagnetic tensor. Changing reference frames mixes these components. This is analogous to the way that special relativity mixes space and time into spacetime, and mass, momentum, and energy into four-momentum. Similarly, the energy stored in a magnetic field is mixed with the energy stored in an electric field in the electromagnetic stress\u2013energy tensor.\n\n\n==== Magnetic vector potential ====\n\nIn advanced topics such as quantum mechanics and relativity it is often easier to work with a potential formulation of electrodynamics rather than in terms of the electric and magnetic fields. In this representation, the magnetic vector potential A, and the electric scalar potential \u03c6, are defined using gauge fixing such that:\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  B\n                \n              \n              \n                \n                =\n                \u2207\n                \u00d7\n                \n                  A\n                \n                ,\n              \n            \n            \n              \n                \n                  E\n                \n              \n              \n                \n                =\n                \u2212\n                \u2207\n                \u03c6\n                \u2212\n                \n                  \n                    \n                      \u2202\n                      \n                        A\n                      \n                    \n                    \n                      \u2202\n                      t\n                    \n                  \n                \n                .\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\mathbf {B} &=\\nabla \\times \\mathbf {A} ,\\\\\\mathbf {E} &=-\\nabla \\varphi -{\\frac {\\partial \\mathbf {A} }{\\partial t}}.\\end{aligned}}}\n  \n\nThe vector potential, A given by this form may be interpreted as a generalized potential momentum per unit charge  just as \u03c6 is interpreted as a generalized potential energy per unit charge. There are multiple choices one can make for the potential fields that satisfy the above condition. However, the choice of potentials is represented by its respective gauge condition.\nMaxwell's equations when expressed in terms of the potentials in Lorenz gauge can be cast into a form that agrees with special relativity. In relativity, A together with \u03c6 forms a four-potential regardless of the gauge condition, analogous to the four-momentum that combines the momentum and energy of a particle. Using the four potential instead of the electromagnetic tensor has the advantage of being much simpler\u2014and it can be easily modified to work with quantum mechanics.\n\n\n==== Propagation of Electric and Magnetic fields ====\nSpecial theory of relativity imposes the condition for events related by cause and effect to be time-like separated, that is that causal efficacy propagates no faster than light. Maxwell's equations for electromagnetism are found to be in favor of this as electric and magnetic disturbances are found to travel at the speed of light in space. Electric and magnetic fields from classical electrodynamics obey the principle of locality in physics and are expressed in terms of retarded time or the time at which the cause of a measured field originated given that the influence of field travelled at speed of light. The retarded time for a point particle is given as solution of:\n\n  \n    \n      \n        \n          t\n          \n            r\n          \n        \n        =\n        \n          t\n        \n        \u2212\n        \n          \n            \n              \n                |\n              \n              \n                r\n              \n              \u2212\n              \n                \n                  r\n                \n                \n                  s\n                \n              \n              (\n              \n                t\n                \n                  r\n                \n              \n              )\n              \n                |\n              \n            \n            c\n          \n        \n      \n    \n    {\\displaystyle t_{r}=\\mathbf {t} -{\\frac {|\\mathbf {r} -\\mathbf {r} _{s}(t_{r})|}{c}}}\n  \n\nwhere \n  \n    \n      \n        \n          \n            t\n            \n              r\n            \n          \n        \n      \n    \n    {\\textstyle {t_{r}}}\n  \n is retarded time or the time at which the source's contribution of the field originated, \n  \n    \n      \n        \n          \n            r\n          \n          \n            s\n          \n        \n        (\n        t\n        )\n      \n    \n    {\\textstyle {r}_{s}(t)}\n  \n is the position vector of the particle as function of time, \n  \n    \n      \n        \n          r\n        \n      \n    \n    {\\textstyle \\mathbf {r} }\n  \n is the point in space, \n  \n    \n      \n        \n          t\n        \n      \n    \n    {\\textstyle \\mathbf {t} }\n  \n is the time at which fields are measured and \n  \n    \n      \n        c\n      \n    \n    {\\textstyle c}\n  \n is the speed of light. The equation subtracts the time taken for light to travel from particle to the point in space from the time of measurement to find time of origin of the fields. The uniqueness of solution for \n  \n    \n      \n        \n          \n            t\n            \n              r\n            \n          \n        \n      \n    \n    {\\textstyle {t_{r}}}\n  \n for given \n  \n    \n      \n        \n          t\n        \n      \n    \n    {\\displaystyle \\mathbf {t} }\n  \n, \n  \n    \n      \n        \n          r\n        \n      \n    \n    {\\displaystyle \\mathbf {r} }\n  \n and \n  \n    \n      \n        \n          r\n          \n            s\n          \n        \n        (\n        t\n        )\n      \n    \n    {\\displaystyle r_{s}(t)}\n  \n is valid for charged particles moving slower than speed of light.\n\n\n==== Magnetic field of arbitrary moving point charge ====\n\nThe solution of maxwell's equations for electric and magnetic field of a point charge is expressed in terms of retarded time or the time at which the particle in the past causes the field at the point, given that the influence travels across space at the speed of light.\nAny arbitrary motion of point charge causes electric and magnetic fields found by solving maxwell's equations using green's function for retarded potentials and hence finding the fields to be as follows:\n\n  \n    \n      \n        \n          A\n        \n        (\n        \n          r\n        \n        ,\n        \n          t\n        \n        )\n        =\n        \n          \n            \n              \n                \u03bc\n                \n                  0\n                \n              \n              c\n            \n            \n              4\n              \u03c0\n            \n          \n        \n        \n          \n            (\n            \n              \n                \n                  q\n                  \n                    \n                      \u03b2\n                    \n                    \n                      s\n                    \n                  \n                \n                \n                  (\n                  1\n                  \u2212\n                  \n                    \n                      n\n                    \n                    \n                      s\n                    \n                  \n                  \u22c5\n                  \n                    \n                      \u03b2\n                    \n                    \n                      s\n                    \n                  \n                  )\n                  \n                    |\n                  \n                  \n                    r\n                  \n                  \u2212\n                  \n                    \n                      r\n                    \n                    \n                      s\n                    \n                  \n                  \n                    |\n                  \n                \n              \n            \n            )\n          \n          \n            t\n            =\n            \n              t\n              \n                r\n              \n            \n          \n        \n        =\n        \n          \n            \n              \n                \n                  \u03b2\n                \n                \n                  s\n                \n              \n              (\n              \n                t\n                \n                  r\n                \n              \n              )\n            \n            c\n          \n        \n        \u03c6\n        (\n        \n          r\n        \n        ,\n        \n          t\n        \n        )\n      \n    \n    {\\displaystyle \\mathbf {A} (\\mathbf {r} ,\\mathbf {t} )={\\frac {\\mu _{0}c}{4\\pi }}\\left({\\frac {q{\\boldsymbol {\\beta }}_{s}}{(1-\\mathbf {n} _{s}\\cdot {\\boldsymbol {\\beta }}_{s})|\\mathbf {r} -\\mathbf {r} _{s}|}}\\right)_{t=t_{r}}={\\frac {{\\boldsymbol {\\beta }}_{s}(t_{r})}{c}}\\varphi (\\mathbf {r} ,\\mathbf {t} )}\n  \n\n  \n    \n      \n        \n          B\n        \n        (\n        \n          r\n        \n        ,\n        \n          t\n        \n        )\n        =\n        \n          \n            \n              \u03bc\n              \n                0\n              \n            \n            \n              4\n              \u03c0\n            \n          \n        \n        \n          \n            (\n            \n              \n                \n                  \n                    q\n                    c\n                    (\n                    \n                      \n                        \u03b2\n                      \n                      \n                        s\n                      \n                    \n                    \u00d7\n                    \n                      \n                        n\n                      \n                      \n                        s\n                      \n                    \n                    )\n                  \n                  \n                    \n                      \u03b3\n                      \n                        2\n                      \n                    \n                    (\n                    1\n                    \u2212\n                    \n                      \n                        n\n                      \n                      \n                        s\n                      \n                    \n                    \u22c5\n                    \n                      \n                        \u03b2\n                      \n                      \n                        s\n                      \n                    \n                    \n                      )\n                      \n                        3\n                      \n                    \n                    \n                      |\n                    \n                    \n                      r\n                    \n                    \u2212\n                    \n                      \n                        r\n                      \n                      \n                        s\n                      \n                    \n                    \n                      \n                        |\n                      \n                      \n                        2\n                      \n                    \n                  \n                \n              \n              +\n              \n                \n                  \n                    q\n                    \n                      \n                        n\n                      \n                      \n                        s\n                      \n                    \n                    \u00d7\n                    \n                      \n                        (\n                      \n                    \n                    \n                      \n                        n\n                      \n                      \n                        s\n                      \n                    \n                    \u00d7\n                    \n                      \n                        (\n                      \n                    \n                    (\n                    \n                      \n                        n\n                      \n                      \n                        s\n                      \n                    \n                    \u2212\n                    \n                      \n                        \u03b2\n                      \n                      \n                        s\n                      \n                    \n                    )\n                    \u00d7\n                    \n                      \n                        \n                          \n                            \n                              \u03b2\n                            \n                            \n                              s\n                            \n                          \n                          \u02d9\n                        \n                      \n                    \n                    \n                      \n                        )\n                      \n                    \n                    \n                      \n                        )\n                      \n                    \n                  \n                  \n                    (\n                    1\n                    \u2212\n                    \n                      \n                        n\n                      \n                      \n                        s\n                      \n                    \n                    \u22c5\n                    \n                      \n                        \u03b2\n                      \n                      \n                        s\n                      \n                    \n                    \n                      )\n                      \n                        3\n                      \n                    \n                    \n                      |\n                    \n                    \n                      r\n                    \n                    \u2212\n                    \n                      \n                        r\n                      \n                      \n                        s\n                      \n                    \n                    \n                      |\n                    \n                  \n                \n              \n            \n            )\n          \n          \n            t\n            =\n            \n              t\n              \n                r\n              \n            \n          \n        \n        =\n        \n          \n            \n              \n                \n                  n\n                \n                \n                  s\n                \n              \n              (\n              \n                t\n                \n                  r\n                \n              \n              )\n            \n            c\n          \n        \n        \u00d7\n        \n          E\n        \n        (\n        \n          r\n        \n        ,\n        \n          t\n        \n        )\n      \n    \n    {\\displaystyle \\mathbf {B} (\\mathbf {r} ,\\mathbf {t} )={\\frac {\\mu _{0}}{4\\pi }}\\left({\\frac {qc({\\boldsymbol {\\beta }}_{s}\\times \\mathbf {n} _{s})}{\\gamma ^{2}(1-\\mathbf {n} _{s}\\cdot {\\boldsymbol {\\beta }}_{s})^{3}|\\mathbf {r} -\\mathbf {r} _{s}|^{2}}}+{\\frac {q\\mathbf {n} _{s}\\times {\\Big (}\\mathbf {n} _{s}\\times {\\big (}(\\mathbf {n} _{s}-{\\boldsymbol {\\beta }}_{s})\\times {\\dot {{\\boldsymbol {\\beta }}_{s}}}{\\big )}{\\Big )}}{(1-\\mathbf {n} _{s}\\cdot {\\boldsymbol {\\beta }}_{s})^{3}|\\mathbf {r} -\\mathbf {r} _{s}|}}\\right)_{t=t_{r}}={\\frac {\\mathbf {n} _{s}(t_{r})}{c}}\\times \\mathbf {E} (\\mathbf {r} ,\\mathbf {t} )}\n  \n\nwhere \n  \n    \n      \n        \u03c6\n        (\n        \n          r\n        \n        ,\n        \n          t\n        \n        )\n      \n    \n    {\\textstyle \\varphi (\\mathbf {r} ,\\mathbf {t} )}\n  \nand \n  \n    \n      \n        \n          A\n        \n        (\n        \n          r\n        \n        ,\n        \n          t\n        \n        )\n      \n    \n    {\\textstyle \\mathbf {A} (\\mathbf {r} ,\\mathbf {t} )}\n  \n are electric scalar potential and magnetic vector potential in Lorentz gauge, \n  \n    \n      \n        q\n      \n    \n    {\\displaystyle q}\n  \n is the charge of the point source, \n  \n    \n      \n        \n          \n            n\n          \n          \n            s\n          \n        \n        (\n        \n          r\n        \n        ,\n        t\n        )\n      \n    \n    {\\textstyle {n}_{s}(\\mathbf {r} ,t)}\n  \n is a unit vector pointing from charged particle to the point in space, \n  \n    \n      \n        \n          \n            \u03b2\n          \n          \n            s\n          \n        \n        (\n        t\n        )\n      \n    \n    {\\textstyle {\\boldsymbol {\\beta }}_{s}(t)}\n  \n is the velocity of the particle divided by the speed of light and \n  \n    \n      \n        \u03b3\n        (\n        t\n        )\n      \n    \n    {\\textstyle \\gamma (t)}\n  \n is the corresponding Lorentz factor. Hence by the principle of superposition, the fields of a system of charges also obey principle of locality.\n\n\n=== Quantum electrodynamics ===\n\nThe classical electromagnetic field incorporated into quantum mechanics forms what is known as the semi-classical theory of radiation. However, it is not able to make experimentally observed predictions such as spontaneous emission process or Lamb shift implying the need for quantization of fields. In modern physics, the electromagnetic field is understood to be not a classical field, but rather a quantum field; it is represented not as a vector of three numbers at each point, but as a vector of three quantum operators at each point. The most accurate modern description of the electromagnetic interaction (and much else) is quantum electrodynamics (QED), which is incorporated into a more complete theory known as the Standard Model of particle physics.\nIn QED, the magnitude of the electromagnetic interactions between charged particles (and their antiparticles) is computed using perturbation theory. These rather complex formulas produce a remarkable pictorial representation as Feynman diagrams in which virtual photons are exchanged.\nPredictions of QED agree with experiments to an extremely high degree of accuracy: currently about 10\u221212 (and limited by experimental errors); for details see precision tests of QED. This makes QED one of the most accurate physical theories constructed thus far.\nAll equations in this article are in the classical approximation, which is less accurate than the quantum description mentioned here. However, under most everyday circumstances, the difference between the two theories is negligible.\n\n\n== Uses and examples ==\n\n\n=== Earth's magnetic field ===\n\nThe Earth's magnetic field is produced by convection of a liquid iron alloy in the outer core. In a dynamo process, the movements drive a feedback process in which electric currents create electric and magnetic fields that in turn act on the currents.\nThe field at the surface of the Earth is approximately the same as if a giant bar magnet were positioned at the center of the Earth and tilted at an angle of about 11\u00b0 off the rotational axis of the Earth (see the figure). The north pole of a magnetic compass needle points roughly north, toward the North Magnetic Pole. However, because a magnetic pole is attracted to its opposite, the North Magnetic Pole is actually the south pole of the geomagnetic field. This confusion in terminology arises because the pole of a magnet is defined by the geographical direction it points.\nEarth's magnetic field is not constant\u2014the strength of the field and the location of its poles vary. Moreover, the poles periodically reverse their orientation in a process called geomagnetic reversal. The most recent reversal occurred 780,000 years ago.\n\n\n=== Rotating magnetic fields ===\n\nThe rotating magnetic field is a key principle in the operation of alternating-current motors. A permanent magnet in such a field rotates so as to maintain its alignment with the external field.\nMagnetic torque is used to drive electric motors. In one simple motor design, a magnet is fixed to a freely rotating shaft and subjected to a magnetic field from an array of electromagnets. By continuously switching the electric current through each of the electromagnets, thereby flipping the polarity of their magnetic fields, like poles are kept next to the rotor; the resultant torque is transferred to the shaft.\nA rotating magnetic field can be constructed using two orthogonal coils with 90 degrees phase difference in their AC currents. However, in practice such a system would be supplied through a three-wire arrangement with unequal currents.\nThis inequality would cause serious problems in standardization of the conductor size and so, to overcome it, three-phase systems are used where the three currents are equal in magnitude and have 120 degrees phase difference. Three similar coils having mutual geometrical angles of 120 degrees create the rotating magnetic field in this case. The ability of the three-phase system to create a rotating field, utilized in electric motors, is one of the main reasons why three-phase systems dominate the world's electrical power supply systems.\nSynchronous motors use DC-voltage-fed rotor windings, which lets the excitation of the machine be controlled\u2014and induction motors use short-circuited rotors (instead of a magnet) following the rotating magnetic field of a multicoiled stator. The short-circuited turns of the rotor develop eddy currents in the rotating field of the stator, and these currents in turn move the rotor by the Lorentz force.\nThe Italian physicist Galileo Ferraris and the Serbian-American electrical engineer Nikola Tesla independently researched the use of rotating magnetic fields in electric motors. In 1888, Ferraris published his research in a paper to the Royal Academy of Sciences in Turin and Tesla gained U.S. patent 381,968 for his work.\n\n\n=== Hall effect ===\n\nThe charge carriers of a current-carrying conductor placed in a transverse magnetic field experience a sideways Lorentz force; this results in a charge separation in a direction perpendicular to the current and to the magnetic field. The resultant voltage in that direction is proportional to the applied magnetic field. This is known as the Hall effect.\nThe Hall effect is often used to measure the magnitude of a magnetic field. It is used as well to find the sign of the dominant charge carriers in materials such as semiconductors (negative electrons or positive holes).\n\n\n=== Magnetic circuits ===\n\nAn important use of H is in magnetic circuits where B = \u03bcH inside a linear material. Here, \u03bc is the magnetic permeability of the material. This result is similar in form to Ohm's law J = \u03c3E, where J is the current density, \u03c3 is the conductance and E is the electric field. Extending this analogy, the counterpart to the macroscopic Ohm's law (I = V\u2044R) is:\n\n  \n    \n      \n        \u03a6\n        =\n        \n          \n            \n              F\n              R\n            \n          \n          \n            \n              m\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle \\Phi ={\\frac {F}{R}}_{\\mathrm {m} },}\n  \n\nwhere \n  \n    \n      \n        \u03a6\n        =\n        \u222b\n        \n          B\n        \n        \u22c5\n        \n          d\n        \n        \n          A\n        \n      \n    \n    {\\textstyle \\Phi =\\int \\mathbf {B} \\cdot \\mathrm {d} \\mathbf {A} }\n  \n is the magnetic flux in the circuit, \n  \n    \n      \n        F\n        =\n        \u222b\n        \n          H\n        \n        \u22c5\n        \n          d\n        \n        \n          \u2113\n        \n      \n    \n    {\\textstyle F=\\int \\mathbf {H} \\cdot \\mathrm {d} {\\boldsymbol {\\ell }}}\n  \n is the magnetomotive force applied to the circuit, and Rm is the reluctance of the circuit. Here the reluctance Rm is a quantity similar in nature to resistance for the flux. Using this analogy it is straightforward to calculate the magnetic flux of complicated magnetic field geometries, by using all the available techniques of circuit theory.\n\n\n=== Largest magnetic fields ===\n\nAs of October 2018, the largest magnetic field produced over a macroscopic volume outside a lab setting is 2.8 kT (VNIIEF in Sarov, Russia, 1998). As of October 2018, the largest magnetic field produced in a laboratory over a macroscopic volume was 1.2 kT by researchers at the University of Tokyo in 2018.\nThe largest magnetic fields produced in a laboratory occur in particle accelerators, such as RHIC, inside the collisions of heavy ions, where microscopic fields reach 1014 T. Magnetars have the strongest known magnetic fields of any naturally occurring object, ranging from 0.1 to 100 GT (108 to 1011 T).\n\n\n== Common formul\u00e6 ==\n\nAdditional magnetic field values can be found through the magnetic field of a finite beam, for example, that the magnetic field of an arc of angle \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n  \n and radius \n  \n    \n      \n        R\n      \n    \n    {\\displaystyle R}\n  \n at the center is \n  \n    \n      \n        B\n        =\n        \n          \n            \n              \n                \u03bc\n                \n                  0\n                \n              \n              \u03b8\n              I\n            \n            \n              4\n              \u03c0\n              R\n            \n          \n        \n      \n    \n    {\\displaystyle B={\\mu _{0}\\theta I \\over 4\\pi R}}\n  \n, or that the magnetic field at the center of a N-sided regular polygon of side \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n is \n  \n    \n      \n        B\n        =\n        \n          \n            \n              \n                \u03bc\n                \n                  0\n                \n              \n              N\n              I\n            \n            \n              \u03c0\n              a\n            \n          \n        \n        sin\n        \u2061\n        \n          \n            \u03c0\n            N\n          \n        \n        tan\n        \u2061\n        \n          \n            \u03c0\n            N\n          \n        \n      \n    \n    {\\displaystyle B={\\mu _{0}NI \\over \\pi a}\\sin {\\pi  \\over N}\\tan {\\pi  \\over N}}\n  \n, both outside of the plane with proper directions as inferred by right hand thumb rule.\n\n\n== History ==\n\n\n=== Early developments ===\nWhile magnets and some properties of magnetism were known to ancient societies, the research of magnetic fields began in 1269 when French scholar Petrus Peregrinus de Maricourt mapped out the magnetic field on the surface of a spherical magnet using iron needles. Noting the resulting field lines crossed at two points he named those points \"poles\" in analogy to Earth's poles. He also articulated the principle that magnets always have both a north and south pole, no matter how finely one slices them.\nAlmost three centuries later, William Gilbert of Colchester replicated Petrus Peregrinus' work and was the first to state explicitly that Earth is a magnet.:\u200a34\u200a Published in 1600, Gilbert's work, De Magnete, helped to establish magnetism as a science.\n\n\n=== Mathematical development ===\n\nIn 1750, John Michell stated that magnetic poles attract and repel in accordance with an inverse square law:\u200a56\u200a Charles-Augustin de Coulomb experimentally verified this in 1785 and stated explicitly that north and south poles cannot be separated.:\u200a59\u200a Building on this force between poles, Sim\u00e9on Denis Poisson (1781\u20131840) created the first successful model of the magnetic field, which he presented in 1824.:\u200a64\u200a In this model, a magnetic H-field is produced by magnetic poles and magnetism is due to small pairs of north\u2013south magnetic poles.\nThree discoveries in 1820 challenged this foundation of magnetism. Hans Christian \u00d8rsted demonstrated that a current-carrying wire is surrounded by a circular magnetic field. Then Andr\u00e9-Marie Amp\u00e8re showed that parallel wires with currents attract one another if the currents are in the same direction and repel if they are in opposite directions.:\u200a87\u200a Finally, Jean-Baptiste Biot and F\u00e9lix Savart announced empirical results about the forces that a current-carrying long, straight wire exerted on a small magnet, determining the forces were inversely proportional to the perpendicular distance from the wire to the magnet.:\u200a86\u200a Laplace later deduced a law of force based on the differential action of a differential section of the wire, which became known as the Biot\u2013Savart law, as Laplace did not publish his findings.\nExtending these experiments, Amp\u00e8re published his own successful model of magnetism in 1825. In it, he showed the equivalence of electrical currents to magnets:\u200a88\u200a and proposed that magnetism is due to perpetually flowing loops of current instead of the dipoles of magnetic charge in Poisson's model. Further, Amp\u00e8re derived both Amp\u00e8re's force law describing the force between two currents and Amp\u00e8re's law, which, like the Biot\u2013Savart law, correctly described the magnetic field generated by a steady current. Also in this work, Amp\u00e8re introduced the term electrodynamics to describe the relationship between electricity and magnetism.:\u200a88\u201392\u200a\nIn 1831, Michael Faraday discovered electromagnetic induction when he found that a changing magnetic field generates an encircling electric field, formulating what is now known as Faraday's law of induction.:\u200a189\u2013192\u200a Later, Franz Ernst Neumann proved that, for a moving conductor in a magnetic field, induction is a consequence of Amp\u00e8re's force law.:\u200a222\u200a In the process, he introduced the magnetic vector potential, which was later shown to be equivalent to the underlying mechanism proposed by Faraday.:\u200a225\u200a\nIn 1850, Lord Kelvin, then known as William Thomson, distinguished between two magnetic fields now denoted H and B. The former applied to Poisson's model and the latter to Amp\u00e8re's model and induction.:\u200a224\u200a Further, he derived how H and B relate to each other and coined the term permeability.:\u200a245\u200a\nBetween 1861 and 1865, James Clerk Maxwell developed and published Maxwell's equations, which explained and united all of classical electricity and magnetism. The first set of these equations was published in a paper entitled On Physical Lines of Force in 1861. These equations were valid but incomplete. Maxwell completed his set of equations in his later 1865 paper A Dynamical Theory of the Electromagnetic Field and demonstrated the fact that light is an electromagnetic wave. Heinrich Hertz published papers in 1887 and 1888 experimentally confirming this fact.\n\n\n=== Modern developments ===\nIn 1887, Tesla developed an induction motor that ran on alternating current. The motor used polyphase current, which generated a rotating magnetic field to turn the motor (a principle that Tesla claimed to have conceived in 1882). Tesla received a patent for his electric motor in May 1888. In 1885, Galileo Ferraris independently researched rotating magnetic fields and subsequently published his research in a paper to the Royal Academy of Sciences in Turin, just two months before Tesla was awarded his patent, in March 1888.\nThe twentieth century showed that classical electrodynamics is already consistent with special relativity, and extended classical electrodynamics to work with quantum mechanics. Albert Einstein, in his paper of 1905 that established relativity, showed that both the electric and magnetic fields are part of the same phenomena viewed from different reference frames. Finally, the emergent field of quantum mechanics was merged with electrodynamics to form quantum electrodynamics, which first formalized the notion that electromagnetic field energy is quantized in the form of photons.\n\n\n== See also ==\n\n\n=== General ===\nMagnetohydrodynamics \u2013 the study of the dynamics of electrically conducting fluids\nMagnetic hysteresis \u2013 application to ferromagnetism\nMagnetic nanoparticles \u2013 extremely small magnetic particles that are tens of atoms wide\nMagnetic reconnection \u2013 an effect that causes solar flares and auroras\nMagnetic scalar potential\nSI electromagnetism units \u2013 common units used in electromagnetism\nOrders of magnitude (magnetic field) \u2013 list of magnetic field sources and measurement devices from smallest magnetic fields to largest detected\nUpward continuation\nMoses Effect\n\n\n=== Mathematics ===\nMagnetic helicity \u2013 extent to which a magnetic field wraps around itself\n\n\n=== Applications ===\nDynamo theory \u2013 a proposed mechanism for the creation of the Earth's magnetic field\nHelmholtz coil \u2013 a device for producing a region of nearly uniform magnetic field\nMagnetic field viewing film \u2013 Film used to view the magnetic field of an area\nMagnetic pistol \u2013 a device on torpedoes or naval mines that detect the magnetic field of their target\nMaxwell coil \u2013 a device for producing a large volume of an almost constant magnetic field\nStellar magnetic field \u2013 a discussion of the magnetic field of stars\nTeltron tube \u2013 device used to display an electron beam and demonstrates effect of electric and magnetic fields on moving charges\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Further reading ==\n\n\n== External links ==\n Media related to Magnetic fields at Wikimedia Commons\nCrowell, B., \"Electromagnetism Archived 30 April 2010 at the Wayback Machine\".\nNave, R., \"Magnetic Field\". HyperPhysics.\n\"Magnetism\", The Magnetic Field (archived 9 July 2006). theory.uwinnipeg.ca.\nHoadley, Rick, \"What do magnetic fields look like?\" 17 July 2005.",
        "unit": "magnetic field",
        "url": "https://en.wikipedia.org/wiki/Magnetic_field"
    },
    {
        "_id": "Bit",
        "clean": "Bit",
        "text": "The bit is the most basic unit of information in computing and digital communication. The name is a portmanteau of binary digit. The bit represents a logical state with one of two possible values. These values are most commonly represented as either \"1\" or \"0\", but other representations such as true/false, yes/no, on/off, or +/\u2212 are also widely used.\nThe relation between these values and the physical states of the underlying storage or device is a matter of convention, and different assignments may be used even within the same device or program. It may be physically implemented with a two-state device.\nA contiguous group of binary digits is commonly called a bit string, a bit vector, or a single-dimensional (or multi-dimensional) bit array.\nA group of eight bits is called one byte, but historically the size of the byte is not strictly defined. Frequently, half, full, double and quadruple words consist of a number of bytes which is a low power of two. A string of four bits is usually a nibble.\nIn information theory, one bit is the information entropy of a random binary variable that is 0 or 1 with equal probability, or the information that is gained when the value of such a variable becomes known. As a unit of information, the bit is also known as a shannon, named after Claude E. Shannon.\nThe symbol for the binary digit is either \"bit\", per the IEC 80000-13:2008 standard, or the lowercase character \"b\", per the IEEE 1541-2002 standard. Use of the latter may create confusion with the capital \"B\" which is the international standard symbol for the byte.\n\n\n== History ==\nThe encoding of data by discrete bits was used in the punched cards invented by Basile Bouchon and Jean-Baptiste Falcon (1732), developed by Joseph Marie Jacquard (1804), and later adopted by Semyon Korsakov, Charles Babbage, Herman Hollerith, and early computer manufacturers like IBM. A variant of that idea was the perforated paper tape. In all those systems, the medium (card or tape) conceptually carried an array of hole positions; each position could be either punched through or not, thus carrying one bit of information. The encoding of text by bits was also used in Morse code (1844) and early digital communications machines such as teletypes and stock ticker machines (1870).\nRalph Hartley suggested the use of a logarithmic measure of information in 1928. Claude E. Shannon first used the word \"bit\" in his seminal 1948 paper \"A Mathematical Theory of Communication\". He attributed its origin to John W. Tukey, who had written a Bell Labs memo on 9 January 1947 in which he contracted \"binary information digit\" to simply \"bit\".\n\n\n== Physical representation ==\nA bit can be stored by a digital device or other physical system that exists in either of two possible distinct states. These may be the two stable states of a flip-flop, two positions of an electrical switch, two distinct voltage or current levels allowed by a circuit, two distinct levels of light intensity, two directions of magnetization or polarization, the orientation of reversible double stranded DNA, etc.\nBits can be implemented in several forms. In most modern computing devices, a bit is usually represented by an electrical voltage or current pulse, or by the electrical state of a flip-flop circuit.\nFor devices using positive logic, a digit value of 1 (or a logical value of true) is represented by a more positive voltage relative to the representation of 0. Different logic families require different voltages, and variations are allowed to account for component aging and noise immunity. For example, in transistor\u2013transistor logic (TTL) and compatible circuits, digit values 0 and 1 at the output of a device are represented by no higher than 0.4 V and no lower than 2.6 V, respectively; while TTL inputs are specified to recognize 0.8 V or below as 0 and 2.2 V or above as 1.\n\n\n=== Transmission and processing ===\nBits are transmitted one at a time in serial transmission, and by a multiple number of bits in parallel transmission. A bitwise operation optionally processes bits one at a time. Data transfer rates are usually measured in decimal SI multiples of the unit bit per second (bit/s), such as kbit/s.\n\n\n=== Storage ===\nIn the earliest non-electronic information processing devices, such as Jacquard's loom or Babbage's Analytical Engine, a bit was often stored as the position of a mechanical lever or gear, or the presence or absence of a hole at a specific point of a paper card or tape. The first electrical devices for discrete logic (such as elevator and traffic light control circuits, telephone switches, and Konrad Zuse's computer) represented bits as the states of electrical relays which could be either \"open\" or \"closed\". When relays were replaced by vacuum tubes, starting in the 1940s, computer builders experimented with a variety of storage methods, such as pressure pulses traveling down a mercury delay line, charges stored on the inside surface of a cathode-ray tube, or opaque spots printed on glass discs by photolithographic techniques.\nIn the 1950s and 1960s, these methods were largely supplanted by magnetic storage devices such as magnetic-core memory, magnetic tapes, drums, and disks, where a bit was represented by the polarity of magnetization of a certain area of a ferromagnetic film, or by a change in polarity from one direction to the other. The same principle was later used in the magnetic bubble memory developed in the 1980s, and is still found in various magnetic strip items such as metro tickets and some credit cards.\nIn modern semiconductor memory, such as dynamic random-access memory, the two values of a bit may be represented by two levels of electric charge stored in a capacitor. In certain types of programmable logic arrays and read-only memory, a bit may be represented by the presence or absence of a conducting path at a certain point of a circuit. In optical discs, a bit is encoded as the presence or absence of a microscopic pit on a reflective surface. In one-dimensional bar codes, bits are encoded as the thickness of alternating black and white lines.\n\n\n== Unit and symbol ==\nThe bit is not defined in the International System of Units (SI). However, the International Electrotechnical Commission issued standard IEC 60027, which specifies that the symbol for binary digit should be 'bit', and this should be used in all multiples, such as 'kbit', for kilobit. However, the lower-case letter 'b' is widely used as well and was recommended by the IEEE 1541 Standard (2002). In contrast, the upper case letter 'B' is the standard and customary symbol for byte.\n\n\n=== Multiple bits ===\n\nMultiple bits may be expressed and represented in several ways. For convenience of representing commonly reoccurring groups of bits in information technology, several units of information have traditionally been used. The most common is the unit byte, coined by Werner Buchholz in June 1956, which historically was used to represent the group of bits used to encode a single character of text (until UTF-8 multibyte encoding took over) in a computer and for this reason it was used as the basic addressable element in many computer architectures. The trend in hardware design converged on the most common implementation of using eight bits per byte, as it is widely used today. However, because of the ambiguity of relying on the underlying hardware design, the unit octet was defined to explicitly denote a sequence of eight bits.\nComputers usually manipulate bits in groups of a fixed size, conventionally named \"words\". Like the byte, the number of bits in a word also varies with the hardware design, and is typically between 8 and 80 bits, or even more in some specialized computers. In the early 21st century, retail personal or server computers have a word size of 32 or 64 bits.\nThe International System of Units defines a series of decimal prefixes for multiples of standardized units which are commonly also used with the bit and the byte. The prefixes kilo (103) through yotta (1024) increment by multiples of one thousand, and the corresponding units are the kilobit (kbit) through the yottabit (Ybit).\n\n\n== Information capacity and information compression ==\n\nWhen the information capacity of a storage system or a communication channel is presented in bits or bits per second, this often refers to binary digits, which is a computer hardware capacity to store binary data (0 or 1, up or down, current or not, etc.). Information capacity of a storage system is only an upper bound to the quantity of information stored therein. If the two possible values of one bit of storage are not equally likely, that bit of storage contains less than one bit of information. If the value is completely predictable, then the reading of that value provides no information at all (zero entropic bits, because no resolution of uncertainty occurs and therefore no information is available). If a computer file that uses n bits of storage contains only m < n bits of information, then that information can in principle be encoded in about m bits, at least on the average. This principle is the basis of data compression technology. Using an analogy, the hardware binary digits refer to the amount of storage space available (like the number of buckets available to store things), and the information content the filling, which comes in different levels of granularity (fine or coarse, that is, compressed or uncompressed information). When the granularity is finer\u2014when information is more compressed\u2014the same bucket can hold more.\nFor example, it is estimated that the combined technological capacity of the world to store information provides 1,300 exabytes of hardware digits. However, when this storage space is filled and the corresponding content is optimally compressed, this only represents 295 exabytes of information. When optimally compressed, the resulting carrying capacity approaches Shannon information or information entropy.\n\n\n== Bit-based computing ==\nCertain bitwise computer processor instructions (such as bit set) operate at the level of manipulating bits rather than manipulating data interpreted as an aggregate of bits.\nIn the 1980s, when bitmapped computer displays became popular, some computers provided specialized bit block transfer instructions to set or copy the bits that corresponded to a given rectangular area on the screen.\nIn most computers and programming languages, when a bit within a group of bits, such as a byte or word, is referred to, it is usually specified by a number from 0 upwards corresponding to its position within the byte or word. However, 0 can refer to either the most or least significant bit depending on the context.\n\n\n== Other information units ==\n\nSimilar to torque and energy in physics; information-theoretic information and data storage size have the same dimensionality of units of measurement, but there is in general no meaning to adding, subtracting or otherwise combining the units mathematically, although one may act as a bound on the other.\nUnits of information used in information theory include the shannon (Sh), the natural unit of information (nat) and the hartley (Hart). One shannon is the maximum amount of information needed to specify the state of one bit of storage. These are related by 1 Sh \u2248 0.693 nat \u2248 0.301 Hart.\nSome authors also define a binit as an arbitrary information unit equivalent to some fixed but unspecified number of bits.\n\n\n== See also ==\nBinary numeral system\nBit rate and baud rate\nBitstream\nByte\nEntropy (information theory)\nFuzzy bit\nInteger (computer science)\nNibble\nPrimitive data type\nP-bit (probabilistic bit)\nQubit (quantum bit)\nShannon (unit)\nTernary numeral system\nTrit (Trinary digit)\n\n\n== References ==\n\n\n== External links ==\n\nBit Calculator \u2013 a tool providing conversions between bit, byte, kilobit, kilobyte, megabit, megabyte, gigabit, gigabyte\nBitXByteConverter Archived 2016-04-06 at the Wayback Machine \u2013 a tool for computing file sizes, storage capacity, and digital information in various units",
        "unit": "bit",
        "url": "https://en.wikipedia.org/wiki/Bit"
    },
    {
        "_id": "Barn_(unit)",
        "clean": "Barn (unit)",
        "text": "A barn (symbol: b) is a metric unit of area equal to 10\u221228 m2 (100 fm2). Originally used in nuclear physics for expressing the cross sectional area of nuclei and nuclear reactions, today it is also used in all fields of high-energy physics to express the cross sections of any scattering process, and is best understood as a measure of the probability of interaction between small particles. A barn is approximately the cross-sectional area of a uranium nucleus. The barn is also the unit of area used in nuclear quadrupole resonance and nuclear magnetic resonance to quantify the interaction of a nucleus with an electric field gradient. While the barn never was an SI unit, the SI standards body acknowledged it in the 8th SI Brochure (superseded in 2019) due to its use in particle physics.\n\n\n== Etymology ==\nDuring Manhattan Project research on the atomic bomb during World War II, American physicists Marshall Holloway and Charles P. Baker were working at Purdue University on a project using a particle accelerator to measure the cross sections of certain nuclear reactions. According to an account of theirs from a couple years later, they were dining in a cafeteria in December 1942 and discussing their work. They \"lamented\" that there was no name for the unit of cross section and challenged themselves to develop one. They initially tried to find the name of \"some great man closely associated with the field\" that they could name the unit after, but struggled to find one that was appropriate. They considered \"Oppenheimer\" too long (in retrospect, they considered an \"Oppy\" to perhaps have been allowable), and considered \"Bethe\" to be too easily confused with the commonly-used Greek letter beta. They then considered naming it after John Manley, another scientist associated with their work, but considered \"Manley\" too long and \"John\" too closely associated with toilets. But this latter association, combined with the \"rural background\" of one of the scientists, suggested to them the term \"barn\", which also worked because the unit was \"really as big as a barn.\" According to the authors, the first published use of the term was in a (secret) Los Alamos report from late June 1943, on which the two originators were co-authors.\n\n\n== Commonly used prefixed versions ==\nThe unit symbol for the barn (b) is also the IEEE standard symbol for bit. In other words, 1 Mb can mean one megabarn or one megabit.\n\n\n== Conversions ==\nCalculated cross sections are often given in terms of inverse squared gigaelectronvolts (GeV\u22122), via the conversion \u01272c2/GeV2 = 0.3894 mb = 38940 am2.\nIn natural units (where \u0127 = c = 1), this simplifies to GeV\u22122 = 0.3894 mb = 38940 am2.\n\n\n=== SI units with prefix ===\nIn SI, one can use units such as square femtometers (fm2). The most common SI prefixed unit for the barn is the femtobarn, which is equal to a tenth of a square zeptometer. Many scientific papers discussing high-energy physics mention quantities of fractions of femtobarn level.\n\n\n== Inverse femtobarn ==\nThe inverse femtobarn (fb\u22121) is the unit typically used to measure the number of particle collision events per femtobarn of target cross-section, and is the conventional unit for time-integrated luminosity. Thus if a detector has accumulated 100 fb\u22121 of integrated luminosity, one expects to find 100 events per femtobarn of cross-section within these data.\nConsider a particle accelerator where two streams of particles, with cross-sectional areas measured in femtobarns, are directed to collide over a period of time. The total number of collisions will be directly proportional to the luminosity of the collisions measured over this time. Therefore, the collision count can be calculated by multiplying the integrated luminosity by the sum of the cross-section for those collision processes. This count is then expressed as inverse femtobarns for the time period (e.g., 100 fb\u22121 in nine months). Inverse femtobarns are often quoted as an indication of particle collider productivity.\nFermilab produced 10 fb\u22121 in the first decade of the 21st century.  Fermilab's Tevatron took about 4 years to reach 1 fb\u22121 in 2005, while two of CERN's LHC experiments, ATLAS and CMS, reached over 5 fb\u22121 of proton\u2013proton data in 2011 alone. In April 2012 the LHC achieved the collision energy of 8 TeV with a luminosity peak of 6760 inverse microbarns per second; by May 2012 the LHC delivered 1 inverse femtobarn of data per week to each detector collaboration. A record of over 23 fb\u22121 was achieved during 2012. As of November 2016, the LHC had achieved 40 fb\u22121 over that year, significantly exceeding the stated goal of 25 fb\u22121. In total, the second run of the LHC has delivered around 150 fb\u22121 to both ATLAS and CMS in 2015\u20132018.\n\n\n=== Usage example ===\nAs a simplified example, if a beamline runs for 8 hours (28 800 seconds) at an instantaneous luminosity of 300\u00d71030 cm\u22122\u22c5s\u22121 = 300 \u03bcb\u22121\u22c5s\u22121, then it will gather data totaling an integrated luminosity of 8640000 \u03bcb\u22121 = 8.64 pb\u22121 = 0.00864 fb\u22121 during this period. If this is multiplied by the cross-section, then a dimensionless number is obtained equal to the number of expected scattering events.\n\n\n== See also ==\n\"Shake\", a unit of time created by the same people at the same time as the barn\nOrders of magnitude (area)\nList of unusual units of measurement\nList of humorous units of measurement\n\n\n== References ==\n\n\n== External links ==\nIUPAC citation for this usage of \"barn\"",
        "unit": "barn",
        "url": "https://en.wikipedia.org/wiki/Barn_(unit)"
    },
    {
        "_id": "Japanese_yen",
        "clean": "Japanese yen",
        "text": "The yen (Japanese: \u5186, symbol: \u00a5; code: JPY) is the official currency of Japan. It is the third-most traded currency in the foreign exchange market, after the United States dollar and the euro. It is also widely used as a third reserve currency after the US dollar and the euro.\nThe New Currency Act of 1871 introduced Japan's modern currency system, with the yen defined as 1.5 g (0.048 troy ounces) of gold, or 24.26 g (0.780 troy ounces) of silver, and divided decimally into 100 sen or 1,000 rin. The yen replaced the previous Tokugawa coinage as well as the various hansatsu paper currencies issued by feudal han (fiefs). The Bank of Japan was founded in 1882 and given a monopoly on controlling the money supply.\nFollowing World War II, the yen lost much of its pre-war value. To stabilize the Japanese economy, the exchange rate of the yen was fixed at \u00a5360 per US$ as part of the Bretton Woods system. When that system was abandoned in 1971, the yen became undervalued and was allowed to float. The yen had appreciated to a peak of \u00a5271 per US$ in 1973, then underwent periods of depreciation and appreciation due to the 1973 oil crisis, arriving at a value of \u00a5227 per US$ by 1980.\nSince 1973, the government of Japan has maintained a policy of currency intervention, so the yen is under a managed float regime. The Japanese government focused on a competitive export market, and tried to ensure a low exchange rate for the yen through a trade surplus. The Plaza Accord of 1985 temporarily changed this situation; the exchange rate fell from its average of \u00a5239 per dollar in 1985 to \u00a5128 in 1988 and led to a peak rate of \u00a580 against the US$ in 1995, effectively increasing the value of Japan's GDP in dollar terms to almost that of the United States.\nSince that time, however, the world price of the yen has greatly decreased, falling to an average of almost \u00a5158 per dollar and \u00a5171 per euro in July 2024. The Bank of Japan maintains a policy of zero to near-zero interest rates and the Japanese government has previously had a strict anti-inflation policy. From late 2020 to first half 2024, the yen depreciated against the dollar by about 60%, giving rise to serious concern in Japan about long-term prospects for the currency. The sharp fall in the value of the currency has led some companies, including Modec, to stop presenting their financial statements in Japanese yen. However, this weakness has had some benefits for Japan's tourism industry, as the low exchange rate makes its purchasing power attractive for travellers, particularly those from foreign nations.\n\n\n== Pronunciation and etymology ==\nThe name, \"Yen\", derives from the Japanese word \u5713 (en, [e\u0274]; \"round\"), which borrows its phonetic reading from Chinese yuan, similar to North Korean won and South Korean won. Originally, the Chinese had traded silver in mass called sycees, and when Spanish and Mexican silver coins arrived from the Philippines, the Chinese called them \"silver rounds\" (Chinese: \u9280\u5713; pinyin: y\u00ednyu\u00e1n) for their circular shapes. The coins and the name also appeared in Japan. While the Chinese eventually replaced \u5706; \u5713 with \u5143, the Japanese continued to use the same word, which was given the shinjitai form \u5186 in reforms at the end of World War II.\nThe spelling and pronunciation \"yen\" is standard in English, because when Japan was first encountered by Europeans around the 16th century, Japanese /e/ (\u3048) and /we/ (\u3091) both had been pronounced [je] and Portuguese missionaries had spelled them \"ye\". By the middle of the 18th century, /e/ and /we/ came to be pronounced [e] as in modern Japanese, although some regions retain the [je] pronunciation. Walter Henry Medhurst, who had neither been to Japan nor met any Japanese people, having consulted mainly a Japanese-Dutch dictionary, spelled some \"e\"s as \"ye\" in his An English and Japanese, and Japanese and English Vocabulary (1830). In the early Meiji era, James Curtis Hepburn, following Medhurst, spelled all \"e\"s as \"ye\" in his A Japanese and English dictionary (1867); in Japanese, e and i are slightly palatalized, somewhat as in Russian. That was the first full-scale Japanese-English/English-Japanese dictionary, which had a strong influence on Westerners in Japan and probably prompted the spelling \"yen\". Hepburn revised most \"ye\"s to \"e\" in the 3rd edition (1886) to mirror the contemporary pronunciation, except \"yen\".\n\n\n== History ==\n\n\n=== Early history (1868\u20131876) ===\n\nAlthough the Edo Shogunate collapsed with the Meiji Restoration and a new government was born, the monetary system still took over that of the former entity. During this unstable period, the confusion caused by this form of exchange caused economic turmoil. The gold (counting money) system of eastern Japan and the silver (weighing money) system of the western Japan were not unified, and the difference in the gold-silver ratio caused a large amount of gold to flow overseas at the end of the Tokugawa shogunate. Emperor Meiji responded to this by appointing \u014ckuma Shigenobu as head of Japan's monetary reform program. He worked with Inoue Kaoru, It\u014d Hirobumi, and Shibusawa Eiichi to run the Ministry of Finance, seeking to introduce a modern monetary system into Japan. \u014ckuma eventually proposed that coins, which were previously square, be made into circles, and that the names of the traditional currencies, ry\u014d (\u4e21), bu (\u5206) and shu (\u6731), be unified into yen (\u5186), which was accepted by the government. Other rejected proposals included physical weight units of \"Fun\" and \"Momme\" which never made it past the pattern stage.\nThe first gold yen coins consisted of 2, 5, and 20 yen coins which were struck throughout 1870. Five yen coins were first struck in gold for the Japanese government in 1870 at the San Francisco Mint. During this time a new mint was being established at Osaka, which did not receive the gold bullion needed for coinage until the following year. Gold bullion was delivered from private Japanese citizens, foreigners, and the Japanese government. Initially the government opted for silver, which would become the standard unit of value leaving gold coinage as a subsidiary. While gold coinage couldn't be produced domestically in 1870, the mint at Osaka could produce silver coins which included denominations of 5, 10, 20, and 50 sen. None of these coins dated \"1870\" circulated until the Meiji government officially adopted the \"yen\" as Japan's modern unit of currency on June 27, 1871. This Act formally stipulated the adoption of the decimal accounting system of yen (1, \u5713), sen (1\u2044100, \u9322), and rin (1\u20441000, \u5398). The new currency was gradually introduced beginning from July of that year.\nJapanese yen denominated paper currency was also conceived with the coins in 1870 as Meiji Tsuho notes by Italian engraver Edoardo Chiossone. These were released as fiat currency in denominations of 1, 2, 5, 10, 50, and 100 yen along with subsidiary notes of 10, 20, and 50 sen in 1872. Almost concurrently, the government established a series of national banks modeled after the system in the United States which issued national bank notes.\n\n\n=== Satsuma Rebellion and aftermath (1877\u20131887) ===\nMassive inflation from the Satsuma Rebellion in 1877 caused a glut of non-redeemable fiat currency notes. The issuance of national fiat banknotes was ultimately suspended in 1880 by then prime minister Matsukata Masayoshi. New policies were put into place which included the establishment of a centralized banking system. \nThe Bank of Japan hence commenced operations on October 10, 1882, with the authority to print banknotes that could be exchanged for the old Government and National Bank Notes. By May 1883, another act provided the redemption and retirement of national bank notes. The National Bank Act was amended again in March 1896, providing for the dissolution of the national banks on the expiration of their charters. This amendment also prohibited national bank notes from circulating after December 31, 1899. In that year, Japan adopted a gold exchange standard, defining the yen as 0.75 g fine gold or US$0.4985.\nThis exchange rate remained in place until Japan left the gold standard in December 1931, after which the yen fell to $0.30 by July 1932 and to $0.20 by 1933.\nIt remained steady at around $0.30 until the start of the Pacific War on December 7, 1941, at which time it fell to $0.23.\nThe sen and the rin were eventually taken out of circulation at the end of 1953.\n\n\n=== Fixed rate of the yen to the U.S. dollar ===\nNo true exchange rate existed for the yen between December 7, 1941, and April 25, 1949; wartime inflation reduced the yen to a fraction of its prewar value.\nAfter a period of instability, on April 25, 1949, the U.S. occupation government fixed the value of the yen at \u00a5360 per USD through a United States plan, which was part of the Bretton Woods system, to stabilize prices in the Japanese economy.\nThat exchange rate was maintained until 1971, when the United States abandoned the gold standard, ending a key element of the Bretton Woods system, and setting in motion changes that eventually led to floating exchange rates in 1973.\n\n\n=== Yen and major currencies float ===\nBy 1971, the yen had become undervalued. Japanese exports were costing too little in international markets, and imports from abroad were costing the Japanese too much. This undervaluation was reflected in the current account balance, which had risen from the deficits of the early 1960s, to a then-large surplus of US$5.8 billion in 1971. The belief that the yen, and several other major currencies, were undervalued motivated the United States' actions in 1971.\nFollowing the United States' measures to devalue the dollar in the summer of 1971, the Japanese government agreed to a new, fixed exchange rate as part of the Smithsonian Agreement, signed at the end of the year. This agreement set the exchange rate at \u00a5308 per US$. However, the new fixed rates of the Smithsonian Agreement were difficult to maintain in the face of supply and demand pressures in the foreign-exchange market. In early 1973, the rates were abandoned, and the major nations of the world allowed their currencies to float.\n\n\n=== Yen adoption in Okinawa ===\nAfter World War II the United States-administered Okinawa issued a higher-valued currency called the B yen from 1946 to 1958, which was then replaced by the U.S. dollar at the rate of $1 = 120 B yen. Upon the reversion of Okinawa to Japan in 1972 the Japanese yen then replaced the dollar. In light of the dollar's reduction in value from \u00a5360 to \u00a5308 just before the reversion, an unannounced \"currency confirmation\" took place on October 9, 1971, wherein residents disclosed their dollar holdings in cash and bank accounts; dollars held that day amounting to US$60 million were entitled for conversion in 1972 at a higher rate of \u00a5360.\n\n\n=== Japanese government intervention in the currency market ===\nIn the 1970s, Japanese government and business people were very concerned that a rise in the value of the yen would hurt export growth by making Japanese products less competitive and would damage the industrial base. The government, therefore, continued to intervene heavily in foreign-exchange marketing (buying or selling dollars), even after the 1973 decision to allow the yen to float.\nDespite intervention, market pressures caused the yen to continue climbing in value, peaking temporarily at an average of \u00a5271 per US$ in 1973, before the impact of the 1973 oil crisis was felt (this was retroactively called endaka, although the term was only coined in 1985). The increased costs of imported oil caused the yen to depreciate to a range of \u00a5290 per US$ to \u00a5300 per US$ between 1974 and 1976. The re-emergence of trade surpluses drove the yen back up to \u00a5211 in 1978. This currency strengthening was again reversed by the second oil shock in 1979, with the yen dropping to \u00a5227 per US$ by 1980.\n\n\n=== Yen in the early 1980s ===\nDuring the first half of the 1980s, the yen failed to rise in value, though current account surpluses returned and grew quickly. From \u00a5221 per US$ in 1981, the average value of the yen actually dropped to \u00a5239 per US$ in 1985. The rise in the current account surplus generated stronger demand for yen in foreign-exchange markets, but this trade-related demand for yen was offset by other factors. A wide differential in interest rates, with United States interest rates much higher than those in Japan, and the continuing moves to deregulate the international flow of capital, led to a large net outflow of capital from Japan. This capital flow increased the supply of yen in foreign-exchange markets, as Japanese investors changed their yen for other currencies (mainly dollars) to invest overseas. This kept the yen weak relative to the dollar and fostered the rapid rise in the Japanese trade surplus that took place in the 1980s.\n\n\n=== Effect of the Plaza Accord ===\n\nIn 1985, a dramatic change began. Finance officials from major nations signed an agreement (the Plaza Accord) affirming that the dollar was overvalued (and, therefore, the yen undervalued). This agreement, and shifting supply and demand pressures in the markets, led to a rapid rise in the value of the yen. From its average of \u00a5239 per US$ in 1985, the yen rose to a peak of \u00a5128 in 1988, virtually doubling its value relative to the dollar. After declining somewhat in 1989 and 1990, it reached a new high of \u00a5123 to US$ in December 1992. In April 1995, the yen hit a peak of under 80 yen/US$, temporarily making Japan's economy nearly the size of that of the US.\n\n\n=== Post-bubble years ===\nThe yen declined during the Japanese asset price bubble and continued to do so afterwards, reaching a low of \u00a5134 to US$ in February 2002. The Bank of Japan's policy of zero interest rates has discouraged yen investments, with the carry trade of investors borrowing yen and investing in better-paying currencies (thus further pushing down the yen) estimated to be as large as $1 trillion. In February 2007, The Economist estimated that the yen was 15% undervalued against the dollar, and as much as 40% undervalued against the euro.\n\n\n=== After the global economic crisis of 2008 ===\n\nHowever, this trend of depreciation reversed after the global economic crisis of 2008. Other major currencies, except the Swiss franc, have been declining relative to the yen.\nOn April 4, 2013, the Bank of Japan announced that they would expand their asset purchase program by $1.4 trillion in two years. The Bank of Japan hopes to bring Japan from deflation to inflation, aiming for 2% inflation. The number of purchases is so large that it is expected to double the money supply, but this move has sparked concerns that the authorities in Japan are deliberately devaluing the yen to boost exports. However, the commercial sector in Japan worried that the devaluation would trigger an increase in import prices, especially for energy and raw materials.\n\n\n=== A period of rapid global inflation from 2022 onwards ===\n\nSince 2022, the yen has depreciated significantly against its peers, due to a variety of factors. Firstly, Japan's prolonged low-interest-rate policy (to tackle domestic deflation) has created a yield differential with other countries\u2014notably the US\u2014that have high interest rates (to tackle domestic inflation), prompting investors to seek higher returns in foreign currencies. This interest rate differential directly affects the price  of the Yen and serves as one of the drivers behind its depreciation. Widely held expectations of yen depreciation can become self-fulfilling prophecies, affecting the currency's exchange rate. To counter this, the BOJ conducted currency interventions of more than JPY 9 trillion selling the dollar and buying the yen in the September\u2013October 2022 and April\u2013May 2024 periods respectively.\n\n\n=== Redenomination proposals ===\nNumerous proposals have been made since the 1990s to redenominate the yen by introducing a new unit or new yen, equal to 100 yen, and nearly worth one U.S. dollar. This has not happened to date, since the yen remains trusted globally despite its low unit value, and due to the huge costs of reissuing new currency and updating currency-reading hardware. The negative impact of postponing upgrades to various computer software until redenomination occurs, in particular, was also cited.\n\n\n== Coins ==\n \nThe Japan Mint has issued legal tender coins from 1871 to the present.\n\n\n=== Currently circulating coinage ===\n\nThe obverse side of all coins shows the coin's value in kanji characters as well as the country name (through 1945, Dai Nippon (\u5927\u65e5\u672c, \"Great Japan\"); after 1945, Nippon-koku (\u65e5\u672c\u56fd, \"State of Japan\") (except for the current 5-yen coin with the country name on the reverse). The reverse side of all coins shows the year of mintage, which is not shown in Gregorian calendar years, but instead in the regnal year of the current emperor's reign, with the first year of an era called gannen (\u5143\u5e74). Imperial portraits have never appeared on Japanese coins, as the image of the emperor remains sacred.\n\nExamples\nCoins minted in 1900 bear the year \u660e\u6cbb (Meiji) 33, the 33rd year of Emperor Meiji's reign\nCoins minted in 1920 bear the year \u5927\u6b63 (Taish\u014d) 9, the 9th year of Emperor Taish\u014d's reign\nCoins minted in 1980 bear the year \u662d\u548c (Sh\u014dwa) 55, the 55th year of Emperor Sh\u014dwa's reign\nCoins minted in 2000 bear the year \u5e73\u6210 (Heisei) 12, the 12th year of Emperor Akihito's reign\nCoins minted in 2020 bear the year \u4ee4\u548c (Reiwa) 2, the 2nd year of Emperor Naruhito's reign\n\nIn 1897, the silver 1 yen coin was demonetized and the sizes of the gold coins were reduced by 50%, with 5, 10 and 20 yen coins issued.\nAfter the war, brass 50 sen, 1 and 5 yen were introduced between 1946 and 1948. The current-type holed brass 5 yen was introduced in 1949, the bronze 10 yen in 1951, and the aluminum 1 yen in 1955.\nIn 1955 the first unholed, nickel 50 yen was introduced. In 1957, silver 100 yen pieces were introduced, followed by the holed 50 yen coin in 1959. These were replaced in 1967 by the current cupro-nickel 100 yen along with a smaller 50 yen.\nIn 1982, the first cupronickel 500 yen coin was introduced. Alongside the 5 Swiss franc coin, the 500 yen coin is one of the highest-valued coin to be used regularly in the world, with a value of US$4.42 as of December 2016. Because of its high face value, the 500 yen coin has been a favorite target for counterfeiters, resulting in the issuance in 2000 of the second nickel-brass 500 yen coin with added security features. Continued counterfeiting of the latter resulted in the issuance in 2021 of the third bi-metallic 500 yen coin with more improvements in security features.\nDue to the great differences in style, size, weight and the pattern present on the edge of the coin they are easy for people with visual impairments to tell apart from one another.\n\nCommemorative coins have been minted on various occasions in base metal, silver and gold. The first of these were silver \u00a5100 and \u00a51,000 Summer Olympic coins issued for the 1964 games. The largest issuance by denomination and total face value were 10 million gold coins of \u00a5100,000 denomination for the 60th anniversary of reign of the Sh\u014dwa Emperor in 1986, totalling \u00a51 trillion and utilizing 200,000 kg fine gold. \u00a5500 commemorative coins have been regularly issued since 1985. In 2008 commemorative \u00a5500 and \u00a51,000 coins were issued featuring Japan's 47 prefectures. Even though all commemorative coins can be spent like ordinary (non-commemorative) coins, they do not normally circulate, and \u00a5100,000 coins are treated with caution due to the discovery of counterfeits.\nThe 1 yen coin is made out of 100% aluminum and can float on water if placed correctly.\n\n\n=== Formerly circulated coinage ===\n\n\n==== Sen ====\n\nSubsidiary coins of \"sen\" (one hundredth of a yen) were initially introduced in 1870 with a silver alloy in denominations of 5, 10, 20 and 50 sen. Copper sen coins in denominations of half, 1, and 2 came three years later, as Japan acquired the technology needed to mint them. The removal of silver from sen coinage began in 1889, when Cupronickel 5 sen coins were introduced. By 1920, this included cupro-nickel 10 sen and reduced-size silver 50 sen coins. Production of the latter ceased in 1938, after which a variety of base metals were used to produce 1, 5 and 10 sen coins during the Second World War. While clay 5 and 10 sen coins were produced in 1945, they were not issued for circulation. As with the Rin, coins in denominations of less than 1 yen became invalid at the end of 1953 and were demonetized due to inflation.\n\n\n==== Rin ====\n\nBronze coins worth one-one thousandth of a yen called \"rin\" were first introduced in 1873. One rin coins were very small, measuring 15.75 mm in diameter and 0.3 mm in thickness, and co-circulated with mon coins of the old currency system. Their small size was eventually their undoing, and the rin was abandoned in 1884 due to unpopularity. Five rin coins worth one-two hundredth of a yen also used a bronze alloy. These were successor coins to the equally valued half sen coin which had been previously minted until 1888. The decision to bring back an equally valued coin was in response to rising inflation caused by World War I which led to an overall shortage of subsidiary coins. The mintage period for five rin coins was brief as they were discontinued after only four years of production due to their sharp decline in monetary value. The overall demand for subsidiary coinage ended as Japan slipped into a post-war recession. Coins worth 1 and 5 rin were eventually officially taken out of circulation at the end of 1953 and demonetized.\n\n\n== Banknotes ==\n\nThe issuance of yen banknotes began in 1872, two years after the currency was introduced. Denominations have ranged from 1 yen to 10,000 yen; since 1984, the lowest-valued banknote is the 1,000 yen note. Before and during World War II, various bodies issued banknotes in yen, such as the Ministry of Finance and the Imperial Japanese National Bank. The Allied forces also issued some notes shortly after the war. Since then, the Bank of Japan has been the exclusive note issuing authority. The bank has issued five series after World War II.\nJapan is generally considered a cash-based society, with 38% of payments in Japan made by cash in 2014. Possible explanations are that cash payments protect one's privacy, merchants do not have to wait for payment, and it does not carry any negative connotation like credit.\nAt present, portraits of people from the Meiji period and later are printed on Japanese banknotes. The reason for this is that from the viewpoint of preventing forgery, it is desirable to use a precise photograph as an original rather than a painting for a portrait.\nSeries E banknotes were introduced in 2004 in \u00a51000, \u00a55000, and \u00a510,000 denominations.\nSeries F banknotes were introduced on 3 July 2024. They were announced on 9 April 2019 by Finance Minister Tar\u014d As\u014d. The \u00a51000 bill features Kitasato Shibasabur\u014d and The Great Wave off Kanagawa, the \u00a55000 bill features Tsuda Umeko and Wisteria flowers, and the \u00a510,000 bill features Shibusawa Eiichi and Tokyo Station. The Ministry decided to not redesign the \u00a52000 note due to low circulation.\nThe EURion constellation pattern is present in the Series D, E and F banknotes.\n\n\n== Determinants of value ==\nBeginning in December 1931, Japan gradually shifted from the gold standard system to the managed currency system.\nThe relative value of the yen is determined in foreign exchange markets by the economic forces of supply and demand. The supply of the yen in the market is governed by the desire of yen holders to exchange their yen for other currencies to purchase goods, services, or assets. The demand for the yen is governed by the desire of foreigners to buy goods and services in Japan and by their interest in investing in Japan (buying yen-denominated real and financial assets).\nSince the 1990s, the Bank of Japan, the country's central bank, has kept interest rates low to spur economic growth. Short-term lending rates have responded to this monetary relaxation and fell from 3.7% to 1.3% between 1993 and 2008. Low interest rates and low inflation, combined with a ready liquidity, prompted investors to borrow yen in Japan and invest it in other countries (such as the US) with significantly higher bank rates, a practice known as the carry trade. This has helped to keep the exchange rate of the yen low compared to other currencies, particularly the US dollar.\n\n\n== International reserve currency ==\n\nThe special drawing rights (SDR) valuation is an IMF basket of the world's major reserve currencies, including the Japanese yen. Its share of 8.33% as of 2016 has declined from 18% as of 2000.\nThe percental composition of currencies of official foreign exchange reserves from 1995 to 2022.\n\n\n== Historical exchange rate ==\nBefore the war commenced, the yen traded on an average of 3.6 yen to the dollar. After the war the yen went as low as 600 yen per USD in 1947, as a result of currency overprinting in order to fund the war, and afterwards to fund the reconstruction.\nWhen MacArthur and the US forces entered Japan in 1945, they decreed an official conversion rate of 15 yen to the USD. Within 1945\u20131946: the rate tanked to 50 yen to the USD because of the ongoing inflation. During the first half of 1946, the rate fluctuated to 66 yen to the USD and eventually plummeting to 600 yen to the dollar by 1947 because of the failure of the economic remedies. Eventually, the peg was officially moved to 270 yen to the dollar in 1948 before being adjusted again from 1949 to 1971 to 360 yen to the dollar.\nBeginning in 2022 the yen/dollar rate has become increasingly weaker with each passing month. By July 2024, the price fell to upper \u00a5161 per $1, marking the lowest exchange rate for the yen in 37.5 years on a nominal effective exchange rate and the lowest real effective exchange rate since the start of statistics by the Bank of Japan in 1970.  One of the main reasons behind this fall was the US moving towards higher interest rates, while Japan remained \"ultra-low\". Other factors include the strength of the US economy and its labor market, while Japan continues to lag behind its peers to bring its economy back to its pre-pandemic size. Japan's trade balance staying in the red is also likely feeding into the weaker yen. Interviewed by Asahi Shimbun Digital in September 2022, Izuru Kato, chief economist at Totan Research, expressed concern about the sharp fall in its value since 2022. and Moneypost reported that the exchange rate instability had made it impossible to exchange the currency for Japanese yen at some exchange offices. With the rate returning to \u00a5150 in November 2023, concern deepened as Nikkei sources expected the yen to depreciate further in the future. The Economist (London) expressed concern over the performance of the Bank of Japan, suggested that a systemic risk is posed to the world financial system. Other reports of a weakening of the Japanese yen include the Noto Peninsula earthquake in January 2024, where previous earthquakes had seen a temporary appreciation of the yen against the US dollar, but this earthquake reversed the trend and weakened the yen against the US dollar.\nGoldman Sachs expects the Japanese yen to remain very weak in the coming months.\n\n\n=== Average spot rates v USD ===\nThe table below shows the monthly average of the U.S. dollar\u2013yen spot rate (JPY per USD) at 17:00 JST:\n\n\n=== Average monthly real effective exchange rate ===\nThe table below shows monthly averages of real effective exchange rates. A higher figure indicates a stronger yen, while a lower figure indicates a weaker yen with the value of 100 being the 2020 average. Data from 1970 onwards are presented in the Broad range, while data from 1969 and earlier are presented in the Narrow range.:\n\n\n== See also ==\nJapan Mint\nJapanese military currency\nEconomy of Japan\nCapital flows in Japan\nMonetary and fiscal policy of Japan\nBalance of payments accounts of Japan (1960\u201390)\nList of countries by leading trade partners\nList of the largest trading partners of Japan\nKorean Empire won (1902\u20131910)\n\n\n=== Older currency ===\nJapanese mon (currency)\nKoban (coin)\nRy\u014d (Japanese coin)\nWad\u014dkaichin\n\n\n== Notes ==\n\n\n== References ==\n\n\n=== Citations ===\n\n\n=== Sources ===\n\n\n== Further reading ==\n\n\n== External links ==\n\nJapanese currency FAQ in Currency Museum, Bank of Japan\nBank of Japan Notes and Coins Currently Issued - Bank of Japan\nCatalog of the coins of Japan (Numista)\nChart: US dollar in yen) (in German)\nChart: 100 yen in euros (in German)\nHistorical Currency Converter Estimates the historical value of the yen into other currencies\nHistorical and current banknotes of Japan (in English and German)",
        "unit": "japanese yen",
        "url": "https://en.wikipedia.org/wiki/Japanese_yen"
    },
    {
        "_id": "Nucleotide",
        "clean": "Nucleotide",
        "text": "Nucleotides are organic molecules composed of a nitrogenous base, a pentose sugar and a phosphate. They serve as monomeric units of the nucleic acid polymers \u2013 deoxyribonucleic acid (DNA) and ribonucleic acid (RNA),  both of which are essential biomolecules within all life-forms on Earth. Nucleotides are obtained in the diet and are also synthesized from common nutrients by the liver.\nNucleotides are composed of three subunit molecules: a nucleobase, a five-carbon sugar (ribose or deoxyribose), and a phosphate group consisting of one to three phosphates.  The four nucleobases in DNA are guanine, adenine, cytosine, and thymine; in RNA, uracil is used in place of thymine.\nNucleotides also play a central role in metabolism at a fundamental, cellular level. They provide chemical energy\u2014in the form of the nucleoside triphosphates, adenosine triphosphate (ATP), guanosine triphosphate (GTP), cytidine triphosphate (CTP), and uridine triphosphate (UTP)\u2014throughout the cell for the many cellular functions that demand energy, including: amino acid, protein and cell membrane synthesis, moving the cell and cell parts (both internally and intercellularly), cell division, etc.. In addition, nucleotides participate in cell signaling (cyclic guanosine monophosphate or cGMP and cyclic adenosine monophosphate or cAMP) and are incorporated into important cofactors of enzymatic reactions (e.g., coenzyme A, FAD, FMN, NAD, and NADP+).\nIn experimental biochemistry, nucleotides can be radiolabeled using radionuclides to yield radionucleotides.\n5-nucleotides are also used in flavour enhancers as food additive to enhance the umami taste, often in the form of a yeast extract.\n\n\n== Structure ==\n\nA nucleotide is composed of three distinctive chemical sub-units: a five-carbon sugar molecule, a nucleobase (the two of which together are called a nucleoside), and one phosphate group. With all three joined, a nucleotide is also termed a \"nucleoside monophosphate\", \"nucleoside diphosphate\" or \"nucleoside triphosphate\", depending on how many phosphates make up the phosphate group.\nIn nucleic acids, nucleotides contain either a purine or a pyrimidine base\u2014i.e., the nucleobase molecule, also known as a nitrogenous base\u2014and are termed ribonucleotides if the sugar is ribose, or deoxyribonucleotides if the sugar is deoxyribose. Individual phosphate molecules repetitively connect the sugar-ring molecules in two adjacent nucleotide monomers, thereby connecting the nucleotide monomers of a nucleic acid end-to-end into a long chain. These chain-joins of sugar and phosphate molecules create a 'backbone' strand for a single- or double helix. In any one strand, the chemical orientation (directionality) of the chain-joins runs from the 5'-end to the 3'-end (read: 5 prime-end to 3 prime-end)\u2014referring to the five carbon sites on sugar molecules in adjacent nucleotides. In a double helix, the two strands are oriented in opposite directions, which permits base pairing and complementarity between the base-pairs, all which is essential for replicating or transcribing the encoded information found in DNA.\nNucleic acids then are polymeric macromolecules assembled from nucleotides, the monomer-units of nucleic acids. The purine bases adenine and guanine and pyrimidine base cytosine occur in both DNA and RNA, while the pyrimidine bases thymine (in DNA) and uracil (in RNA) occur in just one. Adenine forms a base pair with thymine with two hydrogen bonds, while guanine pairs with cytosine with three hydrogen bonds.\nIn addition to being building blocks for the construction of nucleic acid polymers, singular nucleotides play roles in cellular energy storage and provision, cellular signaling, as a source of phosphate groups used to modulate the activity of proteins and other signaling molecules, and as enzymatic cofactors, often carrying out redox reactions.  Signaling cyclic nucleotides are formed by binding the phosphate group twice to the same sugar molecule, bridging the 5'- and 3'- hydroxyl groups of the sugar.  Some signaling nucleotides differ from the standard single-phosphate group configuration, in having multiple phosphate groups attached to different positions on the sugar.  Nucleotide cofactors include a wider range of chemical groups attached to the sugar via the glycosidic bond, including nicotinamide and flavin, and in the latter case, the ribose sugar is linear rather than forming the ring seen in other nucleotides.\n\n\n== Synthesis ==\nNucleotides can be synthesized by a variety of means, both in vitro and in vivo.\nIn vitro, protecting groups may be used during laboratory production of nucleotides. A purified nucleoside is protected to create a phosphoramidite, which can then be used to obtain analogues not found in nature and/or to synthesize an oligonucleotide.\nIn vivo, nucleotides can be synthesized de novo or recycled through salvage pathways. The components used in de novo nucleotide synthesis are derived from biosynthetic precursors of carbohydrate and amino acid metabolism, and from ammonia and carbon dioxide. Recently it has been also demonstrated that cellular bicarbonate metabolism can be regulated by mTORC1 signaling. The liver is the major organ of de novo synthesis of all four nucleotides. De novo synthesis of pyrimidines and purines follows two different pathways. Pyrimidines are synthesized first from aspartate and carbamoyl-phosphate in the cytoplasm to the common precursor ring structure orotic acid, onto which a phosphorylated ribosyl unit is covalently linked. Purines, however, are first synthesized from the sugar template onto which the ring synthesis occurs. For reference, the syntheses of the purine and pyrimidine nucleotides are carried out by several enzymes in the cytoplasm of the cell, not within a specific organelle. Nucleotides undergo breakdown such that useful parts can be reused in synthesis reactions to create new nucleotides.\n\n\n=== Pyrimidine ribonucleotide synthesis ===\n\nThe synthesis of the pyrimidines CTP and UTP occurs in the cytoplasm and starts with the formation of carbamoyl phosphate from glutamine and CO2. Next, aspartate carbamoyltransferase catalyzes a condensation reaction between aspartate and carbamoyl phosphate to form carbamoyl aspartic acid, which is cyclized into 4,5-dihydroorotic acid by dihydroorotase. The latter is converted to orotate by dihydroorotate oxidase. The net reaction is:\n\n(S)-Dihydroorotate + O2 \u2192 Orotate + H2O2\nOrotate is covalently linked with a phosphorylated ribosyl unit. The covalent linkage between the ribose and pyrimidine occurs at position C1 of the ribose unit, which contains a pyrophosphate, and N1 of the pyrimidine ring. Orotate phosphoribosyltransferase (PRPP transferase) catalyzes the net reaction yielding orotidine monophosphate (OMP):\n\nOrotate + 5-Phospho-\u03b1-D-ribose 1-diphosphate (PRPP)  \u2192  Orotidine 5'-phosphate + Pyrophosphate\nOrotidine 5'-monophosphate is decarboxylated by orotidine-5'-phosphate decarboxylase to form uridine monophosphate (UMP). PRPP transferase catalyzes both the ribosylation and decarboxylation reactions, forming UMP from orotic acid in the presence of PRPP. It is from UMP that other pyrimidine nucleotides are derived.  UMP is phosphorylated by two kinases to uridine triphosphate (UTP) via two sequential reactions with ATP.  First, the diphosphate from UDP is produced, which in turn is phosphorylated to UTP. Both steps are fueled by ATP hydrolysis:\n\nATP + UMP \u2192 ADP + UDP\nUDP + ATP \u2192 UTP + ADP\nCTP is subsequently formed by the amination of UTP by the catalytic activity of CTP synthetase. Glutamine is the NH3 donor and the reaction is fueled by ATP hydrolysis, too:\n\nUTP + Glutamine + ATP + H2O \u2192 CTP + ADP + Pi\nCytidine monophosphate (CMP) is derived from cytidine triphosphate (CTP) with subsequent loss of two phosphates.\n\n\n=== Purine ribonucleotide synthesis ===\n\nThe atoms that are used to build the purine nucleotides come from a variety of sources:\n\nThe de novo synthesis of purine nucleotides by which these precursors are incorporated into the purine ring proceeds by a 10-step pathway to the branch-point intermediate IMP, the nucleotide of the base hypoxanthine. AMP and GMP are subsequently synthesized from this intermediate via separate, two-step pathways. Thus, purine moieties are initially formed as part of the ribonucleotides rather than as free bases.\nSix enzymes take part in IMP synthesis. Three of them are multifunctional:\n\nGART (reactions 2, 3, and 5)\nPAICS (reactions 6, and 7)\nATIC (reactions 9, and 10)\nThe pathway starts with the formation of PRPP. PRPS1 is the enzyme that activates R5P, which is formed primarily by the pentose phosphate pathway, to PRPP by reacting it with ATP. The reaction is unusual in that a pyrophosphoryl group is directly transferred from ATP to C1 of R5P and that the product has the \u03b1 configuration about C1. This reaction is also shared with the pathways for the synthesis of Trp, His, and the pyrimidine nucleotides. Being on a major metabolic crossroad and requiring much energy, this reaction is highly regulated.\nIn the first reaction unique to purine nucleotide biosynthesis, PPAT catalyzes the displacement of PRPP's pyrophosphate group (PPi) by an amide nitrogen donated from either glutamine (N), glycine (N&C), aspartate (N), folic acid (C1), or CO2. This is the committed step in purine synthesis. The reaction occurs with the inversion of configuration about ribose C1, thereby forming \u03b2-5-phosphorybosylamine (5-PRA) and establishing the anomeric form of the future nucleotide.\nNext, a glycine is incorporated fueled by ATP hydrolysis, and the carboxyl group forms an amine bond to the NH2 previously introduced. A one-carbon unit from folic acid coenzyme N10-formyl-THF is then added to the amino group of the substituted glycine followed by the closure of the imidazole ring. Next, a second NH2 group is transferred from glutamine to the first carbon of the glycine unit. A carboxylation of the second carbon of the glycin unit is concomitantly added. This new carbon is modified by the addition of a third NH2 unit, this time transferred from an aspartate residue. Finally, a second one-carbon unit from formyl-THF is added to the nitrogen group and the ring is covalently closed to form the common purine precursor inosine monophosphate (IMP).\nInosine monophosphate is converted to adenosine monophosphate in two steps. First, GTP hydrolysis fuels the addition of aspartate to IMP by adenylosuccinate synthase, substituting the carbonyl oxygen for a nitrogen and forming the intermediate adenylosuccinate. Fumarate is then cleaved off forming adenosine monophosphate. This step is catalyzed by adenylosuccinate lyase.\nInosine monophosphate is converted to guanosine monophosphate by the oxidation of IMP forming xanthylate, followed by the insertion of an amino group at C2.  NAD+ is the electron acceptor in the oxidation reaction. The amide group transfer from glutamine is fueled by ATP hydrolysis.\n\n\n=== Pyrimidine and purine degradation ===\nIn humans, pyrimidine rings (C, T, U) can be degraded completely to CO2 and NH3 (urea excretion). That having been said, purine rings (G, A) cannot. Instead, they are degraded to the metabolically inert uric acid which is then excreted from the body. Uric acid is formed when GMP is split into the base guanine and ribose. Guanine is deaminated to xanthine which in turn is oxidized to uric acid. This last reaction is irreversible. Similarly, uric acid can be formed when AMP is deaminated to IMP from which the ribose unit is removed to form hypoxanthine. Hypoxanthine is oxidized to xanthine and finally to uric acid. Instead of uric acid secretion, guanine and IMP can be used for recycling purposes and nucleic acid synthesis in the presence of PRPP and aspartate (NH3 donor).\n\n\n== Prebiotic synthesis of nucleotides ==\nTheories about the origin of life require knowledge of chemical pathways that permit formation of life's key building blocks under plausible prebiotic conditions.  The RNA world hypothesis holds that in the primordial soup there existed free-floating ribonucleotides, the fundamental molecules that combine in series to form RNA.  Complex molecules like RNA must have arisen from small molecules whose reactivity was governed by physico-chemical processes.  RNA is composed of purine and pyrimidine nucleotides, both of which are necessary for reliable information transfer, and thus Darwinian evolution.  Becker et al. showed how pyrimidine nucleosides can be synthesized from small molecules and ribose, driven solely by wet-dry cycles.  Purine nucleosides can be synthesized by a similar pathway.  5'-mono- and di-phosphates also form selectively from phosphate-containing minerals, allowing concurrent formation of polyribonucleotides with both the purine and pyrimidine bases.  Thus a reaction network towards the purine and pyrimidine RNA building blocks can be established starting from simple atmospheric or volcanic molecules.\n\n\n== Unnatural base pair (UBP) ==\n\nAn unnatural base pair (UBP) is a designed subunit (or nucleobase) of DNA which is created in a laboratory and does not occur in nature. Examples include d5SICS and dNaM. These artificial nucleotides bearing hydrophobic nucleobases, feature two fused aromatic rings that form a (d5SICS\u2013dNaM) complex or base pair in DNA. E. coli have been induced to replicate a plasmid containing UBPs through multiple generations. This is the first known example of a living organism passing along an expanded genetic code to subsequent generations.\n\n\n== Medical applications of synthetic nucleotides ==\nThe applications of synthetic nucleotides vary widely and include disease diagnosis, treatment, or precision medicine. \n\nAntiviral or Antiretroviral agents: several nucleotide derivatives have been used in the treatment against infection with Hepatitis and HIV. Examples of direct nucleoside analog reverse-transcriptase inhibitors (NRTIs) include Tenofovir disoproxil, Tenofovir alafenamide, and Sofosbuvir. On the other hand, agents such as Mericitabine, Lamivudine, Entecavir and Telbivudine must first undergo metabolization via phosphorylation to become activated.\nAntisense oligonucleotides (ASO): synthetic oligonucleotides have been used in the treatment of rare heritable diseases since they can bind specific RNA transcripts and ultimately modulate protein expression. Spinal muscular atrophy, amyotrophic lateral sclerosis, homozygous familial hypercholesterolemia, and primary hyperoxaluria type 1 are all amenable to ASO-based therapy. The application of oligonucleotides is a new frontier in precision medicine and management of conditions which are untreatable.\nSynthetic guide RNA (gRNA): synthetic nucleotides can be used to design gRNA which are essential for the proper function of gene-editing technologies such as CRISPR-Cas9.\n\n\n== Length unit ==\nNucleotide (abbreviated \"nt\") is a common unit of length for single-stranded nucleic acids, similar to how base pair is a unit of length for double-stranded nucleic acids.\n\n\n== Abbreviation codes for degenerate bases ==\n\nThe IUPAC has designated the symbols for nucleotides. Apart from the five (A, G, C, T/U) bases, often degenerate bases are used especially for designing PCR primers. These nucleotide codes are listed here.  Some primer sequences may also include the character \"I\", which codes for the non-standard nucleotide inosine.  Inosine occurs in tRNAs and will pair with adenine, cytosine, or thymine.  This character does not appear in the following table, however, because it does not represent a degeneracy.  While inosine can serve a similar function as the degeneracy \"D\", it is an actual nucleotide, rather than a representation of a mix of nucleotides that covers each possible pairing needed.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Further reading ==",
        "unit": "nucleotide",
        "url": "https://en.wikipedia.org/wiki/Nucleotide"
    },
    {
        "_id": "Inch",
        "clean": "Inch",
        "text": "The inch (symbol: in or \u2033) is a unit of length in the British Imperial and the United States customary systems of measurement. It is equal to \u20601/36\u2060 yard or \u20601/12\u2060 of a foot. Derived from the Roman uncia (\"twelfth\"), the word inch is also sometimes used to translate similar units in other measurement systems, usually understood as deriving from the width of the human thumb.\nStandards for the exact length of an inch have varied in the past, but since the adoption of the international yard during the 1950s and 1960s the inch has been based on the metric system and defined as exactly 25.4 mm.\n\n\n== Name ==\nThe English word \"inch\" (Old English: ynce) was an early borrowing from Latin uncia (\"one-twelfth; Roman inch; Roman ounce\"). The vowel change from Latin /u/ to Old English /y/ (which became Modern English /\u026a/) is known as umlaut. The consonant change from the Latin /k/ (spelled c) to English /t\u0283/ is palatalisation. Both were features of Old English phonology; see Phonological history of Old English \u00a7 Palatalization and Germanic umlaut \u00a7 I-mutation in Old English for more information.\n\"Inch\" is cognate with \"ounce\" (Old English: ynse), whose separate pronunciation and spelling reflect its reborrowing in Middle English from Anglo-Norman unce and ounce.\nIn many other European languages, the word for \"inch\" is the same as or derived from the word for \"thumb\", as a man's thumb is about an inch wide (and this was even sometimes used to define the inch). In the Dutch language a term for inch is engelse duim (english thumb). Examples include Catalan: polzada (\"inch\") and polze (\"thumb\"); Czech: palec (\"thumb\"); Danish and Norwegian: tomme (\"inch\") tommel (\"thumb\"); Dutch: duim (whence Afrikaans: duim and Russian: \u0434\u044e\u0439\u043c); French: pouce; Georgian: \u10d3\u10e3\u10d8\u10db\u10d8, Hungarian: h\u00fcvelyk; Italian: pollice; Portuguese: polegada (\"inch\") and polegar (\"thumb\"); (\"duim\"); Slovak: palec (\"thumb\"); Spanish: pulgada (\"inch\") and pulgar (\"thumb\"); and Swedish: tum (\"inch\") and tumme (\"thumb\").\n\n\n== Usage ==\n\n\n=== Imperial or hybrid countries ===\nThe inch is a commonly used customary unit of length in the United States, Canada, and the United Kingdom. For the United Kingdom, guidance on public sector use states that, since 1 October 1995, without time limit, the inch (along with the foot) is to be used as a primary unit for road signs and related measurements of distance (with the possible exception of clearance heights and widths) and may continue to be used as a secondary or supplementary indication following a metric measurement for other purposes.\n\n\n=== Worldwide ===\nInches are used for display screens (e.g. televisions and computer monitors) worldwide. It is the official Japanese standard for electronic parts, especially display screens, and is the industry standard throughout continental Europe for display screens (Germany being one of few countries to supplement it with centimetres in most stores).\nInches are commonly used to specify the diameter of vehicle wheel rims, and the corresponding inner diameter of tyres in tyre codes.\n\n\n=== SI countries ===\nBoth inch-based and millimeter-based hex keys are widely available for sale in Europe.\n\n\n=== Technical details ===\nThe international standard symbol for inch is in (see ISO 31-1, Annex A) but traditionally the inch is denoted by a double prime, which is often approximated by a double quote symbol, and the foot by a prime, which is often approximated by an apostrophe. For example; three feet, two inches can be written as 3\u2032 2\u2033. (This is akin to how the first and second \"cuts\" of the hour are likewise indicated by prime and double prime symbols, and also the first and second cuts of the degree.)\nSubdivisions of an inch are typically written using dyadic fractions with odd number numerators; for example, two and three-eighths of an inch would be written as \u20602+3/8\u2060\u2033 and not as 2.375\u2033 nor as \u20602+6/16\u2060\u2033.  However, for engineering purposes fractions are commonly given to three or four places of decimals and have been for many years.\n\n\n=== Equivalents ===\n1 international inch is equal to:\n\n\n== History ==\n\nThe earliest known reference to the inch in England is from the Laws of \u00c6thelberht dating to the early 7th century, surviving in a single manuscript, the Textus Roffensis from 1120. Paragraph LXVII sets out the fine for wounds of various depths: one inch, one shilling; two inches, two shillings, etc.\nAn Anglo-Saxon unit of length was the barleycorn. After 1066, 1 inch was equal to 3 barleycorns, which continued to be its legal definition for several centuries, with the barleycorn being the base unit. One of the earliest such definitions is that of 1324, where the legal definition of the inch was set out in a statute of Edward II of England, defining it as \"three grains of barley, dry and round, placed end to end, lengthwise\".\nSimilar definitions are recorded in both English and Welsh medieval law tracts. One, dating from the first half of the 10th century, is contained in the Laws of Hywel Dda which superseded those of Dyfnwal, an even earlier definition of the inch in Wales. Both definitions, as recorded in Ancient Laws and Institutes of Wales (vol i., pp. 184, 187, 189), are that \"three lengths of a barleycorn is the inch\".\nKing David I of Scotland in his Assize of Weights and Measures (c. 1150) is said to have defined the Scottish inch as the width of an average man's thumb at the base of the nail, even including the requirement to calculate the average of a small, a medium, and a large man's measures. However, the oldest surviving manuscripts date from the early 14th century and appear to have been altered with the inclusion of newer material.\nIn 1814, Charles Butler, a mathematics teacher at Cheam School, recorded the old legal definition of the inch to be \"three grains of sound ripe barley being taken out the middle of the ear, well dried, and laid end to end in a row\", and placed the barleycorn, not the inch, as the base unit of the English Long Measure system, from which all other units were derived. John Bouvier similarly recorded in his 1843 law dictionary that the barleycorn was the fundamental measure. Butler observed, however, that \"[a]s the length of the barley-corn cannot be fixed, so the inch according to this method will be uncertain\", noting that a standard inch measure was now [i.e. by 1843] kept in the Exchequer chamber, Guildhall, and that was the legal definition of the inch.\nThis was a point also made by George Long in his 1842 Penny Cyclop\u00e6dia, observing that standard measures had since surpassed the barleycorn definition of the inch, and that to recover the inch measure from its original definition, in case the standard measure were destroyed, would involve the measurement of large numbers of barleycorns and taking their average lengths. He noted that this process would not perfectly recover the standard, since it might introduce errors of anywhere between one hundredth and one tenth of an inch in the definition of a yard.\nBefore the adoption of the international yard and pound, various definitions were in use. In the United Kingdom and most countries of the British Commonwealth, the inch was defined in terms of the Imperial Standard Yard. The United States adopted the conversion factor 1 metre = 39.37 inches by an act in 1866. In 1893, Mendenhall ordered the physical realization of the inch to be based on the international prototype metres numbers 21 and 27, which had been received from the CGPM, together with the previously adopted conversion factor.\nAs a result of the definitions above, the U.S. inch was effectively defined as 25.4000508 mm (with a reference temperature of 68 degrees Fahrenheit) and the UK inch at 25.399977 mm (with a reference temperature of 62 degrees Fahrenheit). When Carl Edvard Johansson started manufacturing gauge blocks in inch sizes in 1912, Johansson's compromise was to manufacture gauge blocks with a nominal size of 25.4mm, with a reference temperature of 20 degrees Celsius, accurate to within a few parts per million of both official definitions. Because Johansson's blocks were so popular, his blocks became the de facto standard for manufacturers internationally, with other manufacturers of gauge blocks following Johansson's definition by producing blocks designed to be equivalent to his.\nIn 1930, the British Standards Institution adopted an inch of exactly 25.4 mm. The American Standards Association followed suit in 1933. By 1935, industry in 16 countries had adopted the \"industrial inch\" as it came to be known, effectively endorsing Johansson's pragmatic choice of conversion ratio.\nIn 1946, the Commonwealth Science Congress recommended a yard of exactly 0.9144 metres for adoption throughout the British Commonwealth. This was adopted by Canada in 1951; the United States on 1 July 1959; Australia in 1961, effective 1 January 1964; and the United Kingdom in 1963, effective on 1 January 1964. The new standards gave an inch of exactly 25.4 mm, 1.7 millionths of an inch longer than the old imperial inch and 2 millionths of an inch shorter than the old US inch.\n\n\n== Related units ==\n\n\n=== US survey inches ===\nThe United States retained the \u20601/39.37\u2060-metre definition for surveying, producing a 2 millionth part difference between standard and US survey inches. This is approximately \u20601/8\u2060 inch per mile; 12.7 kilometres is exactly 500,000 standard inches and exactly 499,999 survey inches. This difference is substantial when doing calculations in State Plane Coordinate Systems with coordinate values in the hundreds of thousands or millions of feet.\nIn 2020, the National Institute of Standards and Technology announced that the U.S. survey foot would \"be phased out\" on 1 January 2023 and be superseded by the international foot (also known as the foot) equal to 0.3048 metres exactly, for all further applications. This implies that the survey inch was replaced by the international inch.\n\n\n=== Continental inches ===\n\nBefore the adoption of the metric system, several European countries had customary units whose name translates into \"inch\". The French pouce measured roughly 27.0 mm, at least when applied to describe the calibre of artillery pieces. The Amsterdam foot (voet) consisted of 11 Amsterdam inches (duim). The Amsterdam foot is about 8% shorter than an English foot.\n\n\n=== Scottish inch ===\nThe now obsolete Scottish inch (Scottish Gaelic: \u00f2irleach), \u20601/12\u2060 of a Scottish foot, was about 1.0016 imperial inches (about 25.44 mm).\n\n\n== See also ==\n\n\n== Notes ==\n\n\n== References ==\n\n\n=== Citations ===\n\n\n=== Bibliography ===\nAttenborough, F. L. (1922), The Laws of the Earliest English Kings (Llanerch Press Facsimile Reprint 2000 ed.), Cambridge: Cambridge University Press, ISBN 978-1-86143-101-1, retrieved 11 July 2018",
        "unit": "inch",
        "url": "https://en.wikipedia.org/wiki/Inch"
    },
    {
        "_id": "Statcoulomb",
        "clean": "Statcoulomb",
        "text": "The statcoulomb (statC), franklin (Fr), or electrostatic unit of charge (esu) is the unit of measurement for electrical charge used in the centimetre\u2013gram\u2013second electrostatic units variant (CGS-ESU) and Gaussian systems of units. It is a derived unit given by\n\nThat is, it is defined so that the CGS-ESU quantity that the proportionality constant in Coulomb's law is a dimensionless quantity equal to 1.\nIt can be converted to the corresponding SI quantity using\n\nThe International System of Units uses the coulomb (C) as its unit of electric charge. The conversion between the units coulomb and the statcoulomb depends on the context. The most common contexts are:\n\nFor electric charge:\nFor electric flux (\u03a6D): \nThe symbol \"\u2258\" ('corresponds to') is used instead of \"=\" because the two sides are not interchangeable, as discussed below. The numerical part of the conversion factor of 2997924580 statC/C is very close to 10 times the numeric value of the speed of light when expressed in the unit metre/second, with a small uncertainty.  In the context of electric flux, the SI and CGS units for an electric displacement field (D) are related by:\n\ndue to the relation between the metre and the centimetre.  The coulomb is an extremely large charge rarely encountered in electrostatics, while the statcoulomb is closer to everyday charges.\n\n\n== Definition and relation to CGS base units ==\nThe statcoulomb is defined such that if two stationary spherically symmetric objects each carry a charge of 1 statC and are 1 cm apart, the force of mutual electrical repulsion will be 1 dyne. This repulsion is governed by Coulomb's law, which in the CGS-Gaussian system states:\n\n  \n    \n      \n        F\n        =\n        \n          \n            \n              \n                q\n                \n                  1\n                \n                \n                  G\n                \n              \n              \n                q\n                \n                  2\n                \n                \n                  G\n                \n              \n            \n            \n              r\n              \n                2\n              \n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle F={\\frac {q_{1}^{\\text{G}}q_{2}^{\\text{G}}}{r^{2}}},}\n  \n\nwhere F is the force, qG1 and qG2 are the two charges, and r is the distance between the charges. Performing dimensional analysis on Coulomb's law, the dimension of electrical charge in CGS must be [mass]1/2 [length]3/2 [time]\u22121. (This statement is not true in the International System of Quantities upon which the SI is based; see below.) We can be more specific in light of the definition above: Substituting F = 1 dyn, qG1 = qG2 = 1 statC, and r = 1 cm, we get:\n\nas expected.\n\n\n== Dimensional relation between statcoulomb and coulomb ==\n\n\n=== General incompatibility ===\nCoulomb's law in the Gaussian unit system and the SI are respectively:\n\nSince \u03b50, the vacuum permittivity, is not dimensionless, the coulomb is not dimensionally equivalent to [mass]1/2 [length]3/2 [time]\u22121, unlike the statcoulomb. In fact, it is impossible to express the coulomb in terms of mass, length, and time alone.\nConsequently, a conversion equation like \"1 C = n statC\" is misleading: the units on the two sides are not consistent. One cannot freely switch between coulombs and statcoulombs within a formula or equation, as one would freely switch between centimetres and metres. One can, however, find a correspondence between coulombs and statcoulombs in different contexts. As described below, \"1 C corresponds to 3.00\u00d7109 statC\" when describing the charge of objects. In other words, if a physical object has a charge of 1 C, it also has a charge of 3.00\u00d7109 statC. Likewise, \"1 C corresponds to 3.77\u00d71010 statC\" when describing an electric displacement field flux.\n\n\n=== As a unit of charge ===\nThe statcoulomb is defined as follows: If two stationary objects each carry a charge of 1 statC and are 1 cm apart in vacuum, they will electrically repel each other with a force of 1 dyne.  From this definition, it is straightforward to find an equivalent charge in coulombs. Using the SI equation\n\nand setting F = 1 dyn = 10\u22125 N and r = 1 cm = 10\u22122 m, and then solving for q = qSI1 = qSI2, the result is\n\nTherefore, an object with a CGS charge of 1 statC has a charge of approximately 3.34\u00d710\u221210 C.\n\n\n=== As a unit of electric displacement field or flux ===\nAn electric flux (specifically, a flux of the electric displacement field D) has units of charge: statC in CGS and coulombs in SI. The conversion factor can be derived from Gauss's law:\n\n  \n    \n      \n        \n          \u03a6\n          \n            \n              D\n            \n          \n          \n            G\n          \n        \n        =\n        4\n        \u03c0\n        \n          Q\n          \n            G\n          \n        \n      \n    \n    {\\displaystyle \\Phi _{\\mathbf {D} }^{\\text{G}}=4\\pi Q^{\\text{G}}}\n  \n\n  \n    \n      \n        \n          \u03a6\n          \n            \n              D\n            \n          \n          \n            SI\n          \n        \n        =\n        \n          Q\n          \n            SI\n          \n        \n      \n    \n    {\\displaystyle \\Phi _{\\mathbf {D} }^{\\text{SI}}=Q^{\\text{SI}}}\n  \n\nwhere\n\n  \n    \n      \n        \n          \u03a6\n          \n            \n              D\n            \n          \n        \n        \u2261\n        \n          \u222b\n          \n            S\n          \n        \n        \n          D\n        \n        \u22c5\n        \n          d\n        \n        \n          A\n        \n      \n    \n    {\\displaystyle \\Phi _{\\mathbf {D} }\\equiv \\int _{S}\\mathbf {D} \\cdot \\mathrm {d} \\mathbf {A} }\n  \n\nTherefore, the conversion factor for flux and the conversion factor for charge differ by a ratio of 4\u03c0:\n\n  \n    \n      \n        \n          1\n           \n          C\n        \n         \n        \n          \n            =\n            \u2322\n          \n        \n         \n        \n          3.7673\n          \u00d7\n          \n            10\n            \n              10\n            \n          \n           \n          s\n          t\n          a\n          t\n          C\n        \n        \n        \n          (as unit of \n        \n        \n          \u03a6\n          \n            \n              D\n            \n          \n        \n        \n          ).\n        \n      \n    \n    {\\displaystyle \\mathrm {1~C} ~{\\overset {\\frown }{=}}~\\mathrm {3.7673\\times 10^{10}~statC} \\qquad {\\text{(as unit of }}\\Phi _{\\mathbf {D} }{\\text{).}}}\n  \n\n  \n    \n      \n        \n          1\n           \n          C\n        \n        \u00d7\n        \n          \n            \n              \n                4\n                \u03c0\n                \u00d7\n                \n                  10\n                  \n                    9\n                  \n                \n              \n              \n                \u03f5\n                \n                  0\n                \n              \n            \n          \n        \n        =\n        \n          3.7673\n          \u00d7\n          \n            10\n            \n              10\n            \n          \n           \n          s\n          t\n          a\n          t\n          C\n        \n        \n        \n          (as unit of \n        \n        \n          \u03a6\n          \n            \n              D\n            \n          \n        \n        \n          ).\n        \n      \n    \n    {\\displaystyle \\mathrm {1~C} \\times {\\sqrt {\\frac {4\\pi \\times 10^{9}}{\\epsilon _{0}}}}=\\mathrm {3.7673\\times 10^{10}~statC} \\qquad {\\text{(as unit of }}\\Phi _{\\mathbf {D} }{\\text{).}}}\n  \n\n\n== Notes ==\n\n\n== References ==",
        "unit": "statcoulomb",
        "url": "https://en.wikipedia.org/wiki/Statcoulomb"
    },
    {
        "_id": "TNT_equivalent",
        "clean": "TNT equivalent",
        "text": "TNT equivalent is a convention for expressing energy, typically used to describe the energy released in an explosion. The ton of TNT is a unit of energy defined by convention to be 4.184 gigajoules (1 gigacalorie), which is the approximate energy released in the detonation of a metric ton (1,000 kilograms) of TNT. In other words, for each gram of TNT exploded, 4.184 kilojoules (or 4184 joules) of energy are released.\nThis convention intends to compare the destructiveness of an event with that of conventional explosive materials, of which TNT is a typical example, although other conventional explosives such as dynamite contain more energy.\n\n\n== Kiloton and megaton ==\nThe \"kiloton (of TNT equivalent)\" is a unit of energy equal to 4.184 terajoules (4.184\u00d71012 J). A kiloton of TNT can be visualized as a cube of TNT 8.46 metres (27.8 ft) on a side.\nThe \"megaton (of TNT equivalent)\" is a unit of energy equal to 4.184 petajoules (4.184\u00d71015 J).\nThe kiloton and megaton of TNT equivalent have traditionally been used to describe the energy output, and hence the destructive power, of a nuclear weapon. The TNT equivalent appears in various nuclear weapon control treaties, and has been used to characterize the energy released in asteroid impacts.\n\n\n== Historical derivation of the value ==\nAlternative values for TNT equivalency can be calculated according to which property is being compared and when in the two detonation processes the values are measured.\nWhere for example the comparison is by energy yield, an explosive's energy is normally expressed for chemical purposes as the thermodynamic work produced by its detonation. For TNT this has been accurately measured as 4,686 J/g from a large sample of air blast experiments, and theoretically calculated to be 4,853 J/g.\nHowever even on this basis, comparing the actual energy yields of a large nuclear device and an explosion of TNT can be slightly inaccurate. Small TNT explosions, especially in the open, don't tend to burn the carbon-particle and hydrocarbon products of the explosion. Gas-expansion and pressure-change effects tend to \"freeze\" the burn rapidly. A large open explosion of TNT may maintain fireball temperatures high enough so that some of those products do burn up with atmospheric oxygen.\nSuch differences can be substantial.  For safety purposes a range as wide as 2,673\u20136,702 J has been stated for a gram of TNT upon explosion. Thus one can state that a nuclear bomb has a yield of 15 kt (6.3\u00d71013 J), but the explosion of an actual 15,000 ton pile of TNT may yield (for example) 8\u00d71013 J due to additional carbon/hydrocarbon oxidation not present with small open-air charges.\nThese complications have been sidestepped by convention. The energy released by one gram of TNT was arbitrarily defined as a matter of convention to be 4,184 J, which is exactly one kilocalorie.\n\n\n== Conversion to other units ==\n1 ton of TNT equivalent is approximately:\n\n1.0\u00d7109 calories\n4.184\u00d7109 joules\n3.96831\u00d7106 British thermal units\n3.086\u00d7109 foot-pounds\n1.162\u00d7103 kilowatt-hours\n2.611\u00d71028 electronvolts\n\n\n== Examples ==\n\n\n== Relative effectiveness factor ==\nThe relative effectiveness factor (RE factor) relates an explosive's demolition power to that of TNT, in units of the TNT equivalent/kg (TNTe/kg). The RE factor is the relative mass of TNT to which an explosive is equivalent: The greater the RE, the more powerful the explosive.\nThis enables engineers to determine the proper masses of different explosives when applying blasting formulas developed specifically for TNT. For example, if a timber-cutting formula calls for a charge of 1 kg of TNT, then based on octanitrocubane's RE factor of 2.38, it would take only 1.0/2.38 (or 0.42) kg of it to do the same job. Using PETN, engineers would need 1.0/1.66 (or 0.60) kg to obtain the same effects as 1 kg of TNT. With ANFO or ammonium nitrate, they would require 1.0/0.74 (or 1.35) kg or 1.0/0.32 (or 3.125) kg, respectively.\nCalculating a single RE factor for an explosive is, however, impossible. It depends on the specific case or use. Given a pair of explosives, one can produce 2\u00d7 the shockwave output (this depends on the distance of measuring instruments) but the difference in direct metal cutting ability may be 4\u00d7 higher for one type of metal and 7\u00d7 higher for another type of metal. The relative differences between two explosives with shaped charges will be even greater. The table below should be taken as an example and not as a precise source of data.\n\n\n=== Nuclear examples ===\n\n\n== See also ==\nBrisance\nNet explosive quantity\nNuclear weapon yield\nOrders of magnitude (energy)\nTable of explosive detonation velocities\nTonne of oil equivalent, a unit of energy almost exactly 10 tonnes of TNT\n\n\n== References ==\n\n\n=== Footnotes ===\n\n\n=== Citations ===\n\n\n== External links ==\nThompson, A.; Taylor, B.N. (July 2008). \"Guide for the Use of the International System of Units (SI)\". NIST. NIST Special Publication. 811. National Institute of Standards and Technology. Version 3.2.\nNuclear Weapons FAQ Part 1.3\nRhodes, Richard (2012). The Making of the Atomic Bomb (25th Anniversary ed.). Simon & Schuster. ISBN 978-1-4516-7761-4.\nCooper, Paul W. (1996), Explosives Engineering, New York: Wiley-VCH, ISBN 978-0-471-18636-6\nHQ Department of the Army (2004) [1967], Field Manual 5-25: Explosives and Demolitions, Washington, D.C.: Pentagon Publishing, pp. 83\u201384, ISBN 978-0-9759009-5-6\nUrba\u0144ski, Tadeusz (1985) [1984], Chemistry and Technology of Explosives, Volumes I\u2013IV (second ed.), Oxford: Pergamon\nMathieu, J\u00f6rg; Stucki, Hans (2004), \"Military High Explosives\", CHIMIA International Journal for Chemistry, 58 (6): 383\u2013389, doi:10.2533/000942904777677669, ISSN 0009-4293\n\"3. Thermobaric Explosives\". Advanced Energetic Materials. The National Academies Press, nap.edu. 2004. doi:10.17226/10918. ISBN 978-0-309-09160-2.",
        "unit": "kiloton",
        "url": "https://en.wikipedia.org/wiki/TNT_equivalent"
    },
    {
        "_id": "Kelvin",
        "clean": "Kelvin",
        "text": "The kelvin (symbol: K) is the base unit for temperature in the International System of Units (SI). The Kelvin scale is an absolute temperature scale that starts at the lowest possible temperature (absolute zero), taken to be 0 K. By definition, the Celsius scale (symbol \u00b0C) and the Kelvin scale have the exact same magnitude; that is, a rise of 1 K is equal to a rise of 1 \u00b0C and vice versa, and any temperature in degrees Celsius can be converted to kelvin by adding 273.15.\nThe 19th century British scientist Lord Kelvin first developed and proposed the scale. It was often called the \"absolute Celsius\" scale in the early 20th century. The kelvin was formally added to the International System of Units in 1954, defining 273.16 K to be the triple point of water. The Celsius, Fahrenheit, and Rankine scales were redefined in terms of the Kelvin scale using this definition. The 2019 revision of the SI now defines the kelvin in terms of energy by setting the Boltzmann constant to exactly 1.380649\u00d710\u221223 joules per kelvin; every 1 K change of thermodynamic temperature corresponds to a thermal energy change of exactly 1.380649\u00d710\u221223 J.\n\n\n== History ==\n\n\n=== Precursors ===\n\nDuring the 18th century, multiple temperature scales were developed, notably Fahrenheit and centigrade (later Celsius). These scales predated much of the modern science of thermodynamics, including atomic theory and the kinetic theory of gases which underpin the concept of absolute zero. Instead, they chose defining points within the range of human experience that could be reproduced easily and with reasonable accuracy, but lacked any deep significance in thermal physics. In the case of the Celsius scale (and the long since defunct Newton scale and R\u00e9aumur scale) the melting point of ice served as such a starting point, with Celsius being defined (from the 1740s to the 1940s) by calibrating a thermometer such that:\n\nWater's freezing point is 0 \u00b0C.\nWater's boiling point is 100 \u00b0C.\nThis definition assumes pure water at a specific pressure chosen to approximate the natural air pressure at sea level. Thus, an increment of 1 \u00b0C  equals \u20601/100\u2060 of the temperature difference between the melting and boiling points. The same temperature interval was later used for the Kelvin scale.\n\n\n=== Charles's law ===\nFrom 1787 to 1802, it was determined by Jacques Charles (unpublished), John Dalton, and Joseph Louis Gay-Lussac that, at constant pressure, ideal gases expanded or contracted their volume linearly (Charles's law) by about 1/273 parts per degree Celsius of temperature's change up or down, between 0 \u00b0C and 100 \u00b0C. Extrapolation of this law suggested that a gas cooled to about \u2212273 \u00b0C would occupy zero volume.\n\n\n=== Lord Kelvin ===\n\n\n==== First absolute scale ====\nIn 1848, William Thomson, who was later ennobled as Lord Kelvin, published a paper On an Absolute Thermometric Scale. The scale proposed in the paper turned out to be unsatisfactory, but the principles and formulas upon which the scale was based were correct. For example, in a footnote, Thomson derived the value of \u2212273 \u00b0C for absolute zero by calculating the negative reciprocal of 0.00366\u2014the coefficient of thermal expansion of an ideal gas per degree Celsius relative to the ice point. This derived value agrees with the currently accepted value of \u2212273.15 \u00b0C, allowing for the precision and uncertainty involved in the calculation.\nThe scale was designed on the principle that \"a unit of heat descending from a body A at the temperature T\u00b0 of this scale, to a body B at the temperature (T \u2212 1)\u00b0, would give out the same mechanical effect, whatever be the number T.\" Specifically, Thomson expressed the amount of work necessary to produce a unit of heat (the thermal efficiency) as \n  \n    \n      \n        \u03bc\n        (\n        t\n        )\n        (\n        1\n        +\n        E\n        t\n        )\n        \n          /\n        \n        E\n      \n    \n    {\\displaystyle \\mu (t)(1+Et)/E}\n  \n, where \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n is the temperature in Celsius, \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n  \n is the coefficient of thermal expansion, and \n  \n    \n      \n        \u03bc\n        (\n        t\n        )\n      \n    \n    {\\displaystyle \\mu (t)}\n  \n was \"Carnot's function\", a substance-independent quantity depending on temperature, motivated by an obsolete version of Carnot's theorem. The scale is derived by finding a change of variables \n  \n    \n      \n        \n          T\n          \n            1848\n          \n        \n        =\n        f\n        (\n        T\n        )\n      \n    \n    {\\displaystyle T_{1848}=f(T)}\n  \n of temperature \n  \n    \n      \n        T\n      \n    \n    {\\displaystyle T}\n  \n such that \n  \n    \n      \n        d\n        \n          T\n          \n            1848\n          \n        \n        \n          /\n        \n        d\n        T\n      \n    \n    {\\displaystyle dT_{1848}/dT}\n  \n is proportional to \n  \n    \n      \n        \u03bc\n      \n    \n    {\\displaystyle \\mu }\n  \n.\n\nWhen Thomson published his paper in 1848, he only considered Regnault's experimental measurements of \n  \n    \n      \n        \u03bc\n        (\n        t\n        )\n      \n    \n    {\\displaystyle \\mu (t)}\n  \n. That same year, James Prescott Joule suggested to Thomson that the true formula for Carnot's function was\n\n  \n    \n      \n        \u03bc\n        (\n        t\n        )\n        =\n        J\n        \n          \n            E\n            \n              1\n              +\n              E\n              t\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle \\mu (t)=J{\\frac {E}{1+Et}},}\n  \n\nwhere \n  \n    \n      \n        J\n      \n    \n    {\\displaystyle J}\n  \n is \"the mechanical equivalent of a unit of heat\", now referred to as the specific heat capacity of water, approximately 771.8 foot-pounds force per degree Fahrenheit per pound (4,153 J/K/kg). Thomson was initially skeptical of the deviations of Joule's formula from experiment, stating \"I think it will be generally admitted that there can be no such inaccuracy in Regnault's part of the data, and there remains only the uncertainty regarding the density of saturated steam\". Thomson referred to the correctness of Joule's formula as \"Mayer's hypothesis\", on account of it having been first assumed by Mayer. Thomson arranged numerous experiments in coordination with Joule, eventually concluding by 1854 that Joule's formula was correct and the effect of temperature on the density of saturated steam accounted for all discrepancies with Regnault's data. Therefore, in terms of the modern Kelvin scale \n  \n    \n      \n        T\n      \n    \n    {\\displaystyle T}\n  \n, the first scale could be expressed as follows:\n\n  \n    \n      \n        \n          T\n          \n            1848\n          \n        \n        =\n        100\n        \u00d7\n        \n          \n            \n              log\n              \u2061\n              (\n              T\n              \n                /\n              \n              \n                273 K\n              \n              )\n            \n            \n              log\n              \u2061\n              (\n              \n                373 K\n              \n              \n                /\n              \n              \n                273 K\n              \n              )\n            \n          \n        \n      \n    \n    {\\displaystyle T_{1848}=100\\times {\\frac {\\log(T/{\\text{273 K}})}{\\log({\\text{373 K}}/{\\text{273 K}})}}}\n  \n\nThe parameters of the scale were arbitrarily chosen to coincide with the Celsius scale at 0\u00b0 and 100 \u00b0C or 273 and 373 K (the melting and boiling points of water). On this scale, an increase of approximately 222 degrees corresponds to a doubling of Kelvin temperature, regardless of the starting temperature, and \"infinite cold\" (absolute zero) has a numerical value of negative infinity.\n\n\n==== Modern absolute scale ====\nThomson understood that with Joule's proposed formula for \n  \n    \n      \n        \u03bc\n      \n    \n    {\\displaystyle \\mu }\n  \n, the relationship between work and heat for a perfect thermodynamic engine was simply the constant \n  \n    \n      \n        J\n      \n    \n    {\\displaystyle J}\n  \n. In 1854, Thomson and Joule thus formulated a second absolute scale that was more practical and convenient, agreeing with air thermometers for most purposes. Specifically, \"the numerical measure of temperature shall be simply the mechanical equivalent of the thermal unit divided by Carnot's function.\"\nTo explain this definition, consider a reversible Carnot cycle engine, where \n  \n    \n      \n        \n          Q\n          \n            H\n          \n        \n      \n    \n    {\\displaystyle Q_{H}}\n  \n is the amount of heat energy transferred into the system, \n  \n    \n      \n        \n          Q\n          \n            C\n          \n        \n      \n    \n    {\\displaystyle Q_{C}}\n  \n is the heat leaving the system, \n  \n    \n      \n        W\n      \n    \n    {\\displaystyle W}\n  \n is the work done by the system (\n  \n    \n      \n        \n          Q\n          \n            H\n          \n        \n        \u2212\n        \n          Q\n          \n            C\n          \n        \n      \n    \n    {\\displaystyle Q_{H}-Q_{C}}\n  \n), \n  \n    \n      \n        \n          t\n          \n            H\n          \n        \n      \n    \n    {\\displaystyle t_{H}}\n  \n is the temperature of the hot reservoir in Celsius, and \n  \n    \n      \n        \n          t\n          \n            C\n          \n        \n      \n    \n    {\\displaystyle t_{C}}\n  \n is the temperature of the cold reservoir in Celsius. The Carnot function is defined as \n  \n    \n      \n        \u03bc\n        =\n        W\n        \n          /\n        \n        \n          Q\n          \n            H\n          \n        \n        \n          /\n        \n        (\n        \n          t\n          \n            H\n          \n        \n        \u2212\n        \n          t\n          \n            C\n          \n        \n        )\n      \n    \n    {\\displaystyle \\mu =W/Q_{H}/(t_{H}-t_{C})}\n  \n, and the absolute temperature as \n  \n    \n      \n        \n          T\n          \n            H\n          \n        \n        =\n        J\n        \n          /\n        \n        \u03bc\n      \n    \n    {\\displaystyle T_{H}=J/\\mu }\n  \n. One finds the relationship \n  \n    \n      \n        \n          T\n          \n            H\n          \n        \n        =\n        J\n        \u00d7\n        \n          Q\n          \n            H\n          \n        \n        \u00d7\n        (\n        \n          t\n          \n            H\n          \n        \n        \u2212\n        \n          t\n          \n            C\n          \n        \n        )\n        \n          /\n        \n        W\n      \n    \n    {\\displaystyle T_{H}=J\\times Q_{H}\\times (t_{H}-t_{C})/W}\n  \n. By supposing \n  \n    \n      \n        \n          T\n          \n            H\n          \n        \n        \u2212\n        \n          T\n          \n            C\n          \n        \n        =\n        J\n        \u00d7\n        (\n        \n          t\n          \n            H\n          \n        \n        \u2212\n        \n          t\n          \n            c\n          \n        \n        )\n      \n    \n    {\\displaystyle T_{H}-T_{C}=J\\times (t_{H}-t_{c})}\n  \n, one obtains the general principle of an absolute thermodynamic temperature scale for the Carnot engine, \n  \n    \n      \n        \n          Q\n          \n            H\n          \n        \n        \n          /\n        \n        \n          T\n          \n            H\n          \n        \n        =\n        \n          Q\n          \n            C\n          \n        \n        \n          /\n        \n        \n          T\n          \n            C\n          \n        \n      \n    \n    {\\displaystyle Q_{H}/T_{H}=Q_{C}/T_{C}}\n  \n. The definition can be shown to correspond to the thermometric temperature of the ideal gas laws.\nThis definition by itself is not sufficient. Thomson specified that the scale should have two properties:\n\nThe absolute values of two temperatures are to one another in the proportion of the heat taken in to the heat rejected in a perfect thermodynamic engine working with a source and refrigerator at the higher and lower of the temperatures respectively.\nThe difference of temperatures between the freezing- and boiling-points of water under standard atmospheric pressure shall be called 100 degrees. (The same increment as the Celsius scale) Thomson's best estimates at the time were that the temperature of freezing water was 273.7 K and the temperature of boiling water was 373.7 K.\nThese two properties would be featured in all future versions of the Kelvin scale, although it was not yet known by that name. In the early decades of the 20th century, the Kelvin scale was often called the \"absolute Celsius\" scale, indicating Celsius degrees counted from absolute zero rather than the freezing point of water, and using the same symbol for regular Celsius degrees, \u00b0C.\n\n\n=== Triple point standard ===\n\nIn 1873, William Thomson's older brother James coined the term triple point to describe the combination of temperature and pressure at which the solid, liquid, and gas phases of a substance were capable of coexisting in thermodynamic equilibrium. While any two phases could coexist along a range of temperature-pressure combinations (e.g. the boiling point of water can be affected quite dramatically by raising or lowering the pressure), the triple point condition for a given substance can occur only at a single pressure and only at a single temperature. By the 1940s, the triple point of water had been experimentally measured to be about 0.6% of standard atmospheric pressure and very close to 0.01 \u00b0C per the historical definition of Celsius then in use.\nIn 1948, the Celsius scale was recalibrated by assigning the triple point temperature of water the value of 0.01 \u00b0C exactly and allowing the melting point at standard atmospheric pressure to have an empirically determined value (and the actual melting point at ambient pressure to have a fluctuating value) close to 0 \u00b0C. This was justified on the grounds that the triple point was judged to give a more accurately reproducible reference temperature than the melting point. The triple point could be measured with \u00b10.0001 \u00b0C accuracy, while the melting point just to \u00b10.001 \u00b0C.\nIn 1954, with absolute zero having been experimentally determined to be about \u2212273.15 \u00b0C per the definition of \u00b0C then in use, Resolution 3 of the 10th General Conference on Weights and Measures (CGPM) introduced a new internationally standardized Kelvin scale which defined the triple point as exactly 273.15 + 0.01 = 273.16 degrees Kelvin.\nIn 1967/1968, Resolution 3 of the 13th CGPM renamed the unit increment of thermodynamic temperature \"kelvin\", symbol K, replacing \"degree Kelvin\", symbol \u00b0K. The 13th CGPM also held in Resolution 4 that \"The kelvin, unit of thermodynamic temperature, is equal to the fraction \u20601/273.16\u2060 of the thermodynamic temperature of the triple point of water.\"\nAfter the 1983 redefinition of the metre, this left the kelvin, the second, and the kilogram as the only SI units not defined with reference to any other unit.\nIn 2005, noting that the triple point could be influenced by the isotopic ratio of the hydrogen and oxygen making up a water sample and that this was \"now one of the major sources of the observed variability between different realizations of the water triple point\", the International Committee for Weights and Measures (CIPM), a committee of the CGPM, affirmed that for the purposes of delineating the temperature of the triple point of water, the definition of the kelvin would refer to water having the isotopic composition specified for Vienna Standard Mean Ocean Water.\n\n\n=== 2019 redefinition ===\n\nIn 2005, the CIPM began a programme to redefine the kelvin (along with other SI base units) using a more experimentally rigorous method. In particular, the committee proposed redefining the kelvin such that the Boltzmann constant (kB) would take the exact value 1.3806505\u00d710\u221223 J/K. The committee hoped the program would be completed in time for its adoption by the CGPM at its 2011 meeting, but at the 2011 meeting the decision was postponed to the 2014 meeting when it would be considered part of a larger program. A challenge was to avoid degrading the accuracy of measurements close to the triple point. The redefinition was further postponed in 2014, pending more accurate measurements of the Boltzmann constant in terms of the current definition, but was finally adopted at the 26th CGPM in late 2018, with a value of kB = 1.380649\u00d710\u221223 J\u22c5K\u22121.\nFor scientific purposes, the redefinition's main advantage is in allowing more accurate measurements at very low and very high temperatures, as the techniques used depend on the Boltzmann constant. Independence from any particular substance or measurement is also a philosophical advantage. The kelvin now only depends on the Boltzmann constant and universal constants (see 2019 SI unit dependencies diagram), allowing the kelvin to be expressed exactly as:\n\n1 kelvin = \u20601.380649\u00d710\u221223/(6.62607015\u00d710\u221234)(9192631770)\u2060 \u2060h\u0394\u03bdCs/kB\u2060 = \u206013.80649/6.09110229711386655\u2060 \u2060h\u0394\u03bdCs/kB\u2060.\nFor practical purposes, the redefinition was unnoticed; enough digits were used for the Boltzmann constant to ensure that 273.16 K has enough significant digits to contain the uncertainty of water's triple point and water still normally freezes at 0 \u00b0C to a high degree of precision. But before the redefinition, the triple point of water was exact and the Boltzmann constant had a measured value of 1.38064903(51)\u00d710\u221223 J/K, with a relative standard uncertainty of 3.7\u00d710\u22127. Afterward, the Boltzmann constant is exact and the uncertainty is transferred to the triple point of water, which is now 273.1600(1) K.\nThe new definition officially came into force on 20 May 2019, the 144th anniversary of the Metre Convention.\n\n\n== Practical uses ==\n\n\n=== Colour temperature ===\n\nThe kelvin is often used as a measure of the colour temperature of light sources. Colour temperature is based upon the principle that a black body radiator emits light with a frequency distribution characteristic of its temperature. Black bodies at temperatures below about 4000 K appear reddish, whereas those above about 7500 K appear bluish. Colour temperature is important in the fields of image projection and photography, where a colour temperature of approximately 5600 K is required to match \"daylight\" film emulsions.\nIn astronomy, the stellar classification of stars and their place on the Hertzsprung\u2013Russell diagram are based, in part, upon their surface temperature, known as effective temperature. The photosphere of the Sun, for instance, has an effective temperature of 5772 K [1][2][3][4] as adopted by IAU 2015 Resolution B3.\nDigital cameras and photographic software often use colour temperature in K in edit and setup menus. The simple guide is that higher colour temperature produces an image with enhanced white and blue hues. The reduction in colour temperature produces an image more dominated by reddish, \"warmer\" colours.\n\n\n=== Kelvin as a unit of noise temperature ===\n\nFor electronics, the kelvin is used as an indicator of how noisy a circuit is in relation to an ultimate noise floor, i.e. the noise temperature. The Johnson\u2013Nyquist noise of resistors (which produces an associated kTC noise when combined with capacitors) is a type of thermal noise derived from the Boltzmann constant and can be used to determine the noise temperature of a circuit using the Friis formulas for noise.\n\n\n== Derived units and SI multiples ==\n\nThe only SI derived unit with a special name derived from the kelvin is the degree Celsius. Like other SI units, the kelvin can also be modified by adding a metric prefix that multiplies it by a power of 10:\n\n\n== Orthography ==\nAccording to SI convention, the kelvin is never referred to nor written as a degree. The word \"kelvin\" is not capitalized when used as a unit. It may be in plural form as appropriate (for example, \"it is 283 kelvins outside\", as for \"it is 50 degrees Fahrenheit\" and \"10 degrees Celsius\"). The unit's symbol K is a capital letter, per the SI convention to capitalize symbols of units derived from the name of a person. It is common convention to capitalize Kelvin when referring to Lord Kelvin or the Kelvin scale.\nThe unit symbol K is encoded in Unicode at code point U+212A K KELVIN SIGN. However, this is a compatibility character provided for compatibility with legacy encodings. The Unicode standard recommends using U+004B K LATIN CAPITAL LETTER K instead; that is, a normal capital K. \"Three letterlike symbols have been given canonical equivalence to regular letters: U+2126 \u03a9 OHM SIGN, U+212A K KELVIN SIGN, and U+212B \u00c5 ANGSTROM SIGN. In all three instances, the regular letter should be used.\"\n\n\n== See also ==\n\nComparison of temperature scales\nInternational Temperature Scale of 1990\nkT (energy) \u2013 product of the Boltzmann constant and temperature\nNegative temperature\nOutline of metrology and measurement\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Bibliography ==\nBureau International des Poids et Mesures (2019). \"The International System of Units (SI) Brochure\" (PDF). 9th Edition. International Committee for Weights and Measures. Retrieved 2022-04-28.\nThomson, William (Lord Kelvin) (1882). Mathematical and physical papers: Volume I. Cambridge University Press.\n\n\n== External links ==\n\nThomson, William (October 1848). \"On an Absolute Thermometric Scale founded on Carnot's Theory of the Motive Power of Heat, and calculated from Regnault's Observations\". zapatopi.net. Philosophical Magazine. Archived from the original on 2008-02-01. Retrieved 2022-02-21.\nThomson, William (March 1851). \"On the Dynamical Theory of Heat, with numerical results deduced from Mr Joule's equivalent of a Thermal Unit, and M. Regnault's Observations on Steam\". zapatopi.net. Transactions of the Royal Society of Edinburgh. Retrieved 2024-05-05.",
        "unit": "kelvin",
        "url": "https://en.wikipedia.org/wiki/Kelvin"
    },
    {
        "_id": "Cent_(currency)",
        "clean": "Cent (currency)",
        "text": "The cent is a monetary unit of many national currencies that equals a hundredth (1\u2044100) of the basic monetary unit. The word derives from the Latin centum, 'hundred'.\nThe cent sign is commonly a simple minuscule (lower case) letter c. In North America, the c is crossed by a diagonal or vertical stroke (depending on typeface), yielding the character \u00a2.\nThe United States one cent coin is generally known by the nickname \"penny\", alluding to the British coin and unit of that name. Australia ended production of their 1\u00a2 coin in 1992, as did Canada in 2012. Some Eurozone countries ended production of the 1 euro cent coin, most recently Slovakia in 2022.\n\n\n== Symbol ==\n\nThe cent may be represented by the cent sign, written in various ways according to the national convention and font choice. Most commonly seen forms are a minuscule letter c crossed by a diagonal stroke, a vertical line, a simple c, depending on the currency (see below). Cent amounts from 1 to 99 can be represented as one or two digits followed by the appropriate abbreviation (2\u00a2, 5c, 75\u00a2, 99c), or as a subdivision of the base unit ($0.75, \u20ac0.99). In some countries, longer abbreviations like \"ct.\" are used. Languages that use other alphabets have their own abbreviations and conventions.\nThe cent symbol has largely fallen into disuse since the mid-20th century as inflation has resulted in very few things being priced in cents in any currency. It was included on US typewriter keyboards, but has not been adopted on computers.\n\n\n=== North American cent sign ===\nThe cent sign appeared as the shift of the 6 keys on American manual typewriters, but the freestanding circumflex on computer keyboards has taken over that position. The character (offset 162) can still be created in most common code pages, including Unicode and Windows-1252:\n\nOn DOS- or Windows-based computers with a numeric keypad, Alt can be held while typing 0162 or 155 on the keypad. See Unicode input \u00a7 In Microsoft Windows for techniques involving the hexadecimal code point A2 that can be used when there is no numeric keypad, as on many laptops. For the US International keyboard Right Alt\u21e7 ShiftC can be typed.\nOn Mac systems, \u2325 Option can be held and 4 on the number row pressed.\nOn Unix/Linux systems with a compose key, Compose+|+C and Compose+/+C are typical sequences.\n\n\n=== Orthography ===\nWhen written in English and Mexican Spanish, the cent sign (\u00a2 or c) follows the amount (with no space between)\u2014for example, 2\u00a2 and $0.02, or 2c and \u20ac0.02. Conventions in other languages may vary.\n\n\n== Usage ==\n\n\n=== Minor currency units called cent or similar names ===\nExamples of currencies around the world featuring centesimal (1\u2044100) units called cent, or related words from the same root such as c\u00e9ntimo, cent\u00e9simo, centavo or sen, are:\n\nArgentine peso (as centavo)\nAruban florin, but all circulating coins are in multiples of 5 cents.\nAustralian dollar, but all circulating coins are in multiples of 5 cents.\nBarbadian dollar\nBahamian dollar, but all circulating coins are in multiples of 5 cents.\nBelize dollar\nBermudian dollar\nBolivian boliviano (as centavo), but all circulating coins are in multiples of 10 centavos\nBrazilian real (as centavo)\nBrunei dollar (as sen)\nCambodian riel (as sen)\nCanadian dollar\nCayman Islands dollar\nChilean peso (as centavo). Centavos officially exist and are considered in financial transactions, but there are no current centavo-denominated coins.\nColombian peso (as centavo)\nCook Islands dollar (cent, although some 50 cent coins are marked \"50 tene\")\nCuban peso (as centavo)\nEast Caribbean dollar, but all circulating coins are in multiples of 5 cents.\nEritrean nakfa\nEstonian kroon (as sent)\nEthiopian birr (as santim)\nEuro \u2013 the coins bear the text \"euro cent\". Greek coins have \u039b\u0395\u03a0\u03a4\u039f (\"lepto\") on the obverse of the one-cent coin and \u039b\u0395\u03a0\u03a4\u0391 (\"lepta\") on the obverse of the others. The actual usage varies depending on the language.\nFijian dollar\nGuyanese dollar, but there are no circulating coins with a value below one dollar.\nHong Kong dollar, but all circulating coins are in multiples of 10 cents.\nIndonesian rupiah (as sen; last coin minted was 50 cents in 1961, last cents printed as banknotes in 1964 which were demonetized in 1996 save for the 1 cent)\nJamaican dollar, but there are no circulating coins with a value below one dollar.\nKenyan shilling\nLesotho loti (as sente)\nLiberian dollar\nLithuanian litas (as centas)\nMacanese pataca (as avo), but all circulating coins are in multiples of 10 avos.\nMalaysian ringgit (as sen), but all circulating coins are in multiples of 5 sen.\nMauritian rupee\nMexican peso (as centavo)\nMoroccan dirham (as santim)\nNamibian dollar\nNetherlands Antillean guilder\nNew Zealand dollar, but all circulating coins are in multiples of 10 cents.\nPanamanian balboa (as cent\u00e9simo)\nPeruvian sol (as c\u00e9ntimo)\nPhilippine peso (as sentimo or centavo)\nSeychellois rupee\nSierra Leonean leone\nSingapore dollar, but all circulating coins are in multiples of 5 cents.\nSouth African rand, but all circulating coins are in multiples of 10 cents.\nSri Lankan rupee\nSurinamese dollar\nSwazi lilangeni\nNew Taiwan dollar, but all circulating coins are in multiples of 50 cents.\nTanzanian shilling\nTongan pa\u02bbanga (as seniti)\nTrinidad and Tobago dollar\nUnited States dollar\nUruguayan peso (as cent\u00e9simo)\nZimbabwean ZiG\n\n\n=== Minor currency units with other names ===\nExamples of currencies featuring centesimal (1\u2044100) units not called cent\n\n\n=== Obsolete centesimal currency units ===\nExamples of currencies which formerly featured centesimal (1\u2044100) units but now have no fractional denomination in circulation:\n\nExamples of currencies which use the cent symbol for other purposes:\n\nCosta Rican col\u00f3n \u2013 The common symbol '\u00a2' is frequently used locally to represent '\u20a1', the proper col\u00f3n designation\nGhanaian cedi \u2013 The common symbol '\u00a2' is sometimes used to represent '\u20b5', the proper cedi designation\n\n\n== See also ==\n\nCent (music)\n\n\n== References ==",
        "unit": "cent",
        "url": "https://en.wikipedia.org/wiki/Cent_(currency)"
    },
    {
        "_id": "Gray_(unit)",
        "clean": "Gray (unit)",
        "text": "The gray (symbol: Gy) is the unit of ionizing radiation dose in the International System of Units (SI), defined as the absorption of one joule of radiation energy per kilogram of matter.\nIt is used as a unit of the radiation quantity absorbed dose that measures the energy deposited by ionizing radiation in a unit mass of absorbing material, and is used for measuring the delivered dose in radiotherapy, food irradiation and radiation sterilization. It is important in predicting likely acute health effects, such as acute radiation syndrome  and is used to calculate equivalent dose using the sievert, which is a measure of the stochastic health effect  on the human body.\nThe gray is also used in radiation metrology as a unit of the radiation quantity kerma; defined as the sum of the initial kinetic energies of all the charged particles liberated by uncharged ionizing radiation in a sample of matter per unit mass. The unit was named after British physicist Louis Harold Gray, a pioneer in the measurement of X-ray and radium radiation and their effects on living tissue. \nThe gray was adopted as part of the International System of Units in 1975. The corresponding cgs unit to the gray is the rad (equivalent to 0.01 Gy), which remains common largely in the United States, though \"strongly discouraged\" in the style guide for U.S. National Institute of Standards and Technology.\n\n\n== Applications ==\n\nThe gray has a number of fields of application in measuring dose:\n\n\n=== Radiobiology ===\nThe measurement of absorbed dose in tissue is of fundamental importance in radiobiology and radiation therapy as it is the measure of the amount of energy the incident radiation deposits in the target tissue. The measurement of absorbed dose is a complex problem due to scattering and absorption, and many specialist dosimeters are available for these measurements, and can cover applications in 1-D, 2-D and 3-D.\nIn radiation therapy, the amount of radiation applied varies depending on the type and stage of cancer being treated. For curative cases, the typical dose for a solid epithelial tumor ranges from 60 to 80 Gy, while lymphomas are treated with 20 to 40 Gy. Preventive (adjuvant) doses are typically around 45\u201360 Gy in 1.8\u20132 Gy fractions (for breast, head, and neck cancers).\nThe average radiation dose from an abdominal X-ray is 0.7 millisieverts (0.0007 Sv), that from an abdominal CT scan is 8 mSv, that from a pelvic CT scan is 6 mGy, and that from a selective CT scan of the abdomen and the pelvis is 14 mGy.\n\n\n=== Radiation protection ===\n\nThe absorbed dose also plays an important role in radiation protection, as it is the starting point for calculating the stochastic health risk of low levels of radiation, which is defined as the probability of cancer induction and genetic damage. The gray measures the total absorbed energy of radiation, but the probability of stochastic damage also depends on the type and energy of the radiation and the types of tissues involved. This probability is related to the equivalent dose in sieverts (Sv), which has the same dimensions as the gray. It is related to the gray by weighting factors described in the articles on equivalent dose and effective dose.\nThe International Committee for Weights and Measures states: \"In order to avoid any risk of confusion between the absorbed dose D and the dose equivalent H, the special names for the respective units should be used, that is, the name gray should be used instead of joules per kilogram for the unit of absorbed dose D and the name sievert instead of joules per kilogram for the unit of dose equivalent H.\"\n\n  \n    \n      \n        1\n         \n        \n          G\n          y\n        \n        =\n        1\n         \n        \n          \n            \n              J\n            \n            \n              k\n              g\n            \n          \n        \n        =\n        1\n         \n        \n          \n            \n              \n                m\n              \n              \n                2\n              \n            \n            \n              \n                s\n              \n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle 1\\ \\mathrm {Gy} =1\\ {\\frac {\\mathrm {J} }{\\mathrm {kg} }}=1\\ {\\frac {\\mathrm {m} ^{2}}{\\mathrm {s} ^{2}}}}\n  \n\nThe accompanying diagrams show how absorbed dose (in grays) is first obtained by computational techniques, and from this value the equivalent doses are derived. For X-rays and gamma rays the gray is numerically the same value when expressed in sieverts, but for alpha particles one gray is equivalent to 20 sieverts, and a radiation weighting factor is applied accordingly.\n\n\n=== Radiation poisoning ===\n\nThe gray is conventionally used to express the severity of what are known as \"tissue effects\" from doses received in acute exposure to high levels of ionizing radiation. These are effects that are certain to happen, as opposed to the uncertain effects of low levels of radiation that have a probability of causing damage. A whole-body acute exposure to 5 grays or more of high-energy radiation usually leads to death within 14 days. LD1 is 2.5 Gy, LD50 is 5 Gy and LD99 is 8 Gy. The LD50 dose represents 375 joules for a 75 kg adult.\n\n\n=== Absorbed dose in matter ===\nThe gray is used to measure absorbed dose rates in non-tissue materials for processes such as radiation hardening, food irradiation and electron irradiation. Measuring and controlling the value of absorbed dose is vital to ensuring correct operation of these processes.\n\n\n=== Kerma ===\nKerma (\"kinetic energy released per unit mass\") is used in radiation metrology as a measure of the liberated energy of ionisation due to irradiation, and is expressed in grays. Importantly, kerma dose is different from absorbed dose, depending on the radiation energies involved, partially because ionization energy is not accounted for. Whilst roughly equal at low energies, kerma is much higher than absorbed dose at higher energies, because some energy escapes from the absorbing volume in the form of bremsstrahlung (X-rays) or fast-moving electrons. \nKerma, when applied to air, is equivalent to the legacy roentgen unit of radiation exposure, but there is a difference in the definition of these two units. The gray is defined independently of any target material, however, the roentgen was defined specifically by the ionisation effect in dry air, which did not necessarily represent the effect on other media.\n\n\n== Development of the absorbed dose concept and the gray ==\n\nWilhelm R\u00f6ntgen discovered X-rays on November 8, 1895, and their use spread very quickly for medical diagnostics, particularly broken bones and embedded foreign objects where they were a revolutionary improvement over previous techniques.\nDue to the wide use of X-rays and the growing realisation of the dangers of ionizing radiation, measurement standards became necessary for radiation intensity and various countries developed their own, but using differing definitions and methods. Eventually, in order to promote international standardisation, the first International Congress of Radiology (ICR) meeting in London in 1925, proposed a separate body to consider units of measure. This was called the International Commission on Radiation Units and Measurements, or ICRU, and came into being at the Second ICR in Stockholm in 1928, under the chairmanship of Manne Siegbahn.\nOne of the earliest techniques of measuring the intensity of X-rays was to measure their ionising effect in air by means of an air-filled ion chamber. At the first ICRU meeting it was proposed that one unit of X-ray dose should be defined as the quantity of X-rays that would produce one esu of charge in one cubic centimetre of dry air at 0 \u00b0C and 1 standard atmosphere of pressure. This unit of radiation exposure was named the roentgen in honour of Wilhelm R\u00f6ntgen, who had died five years previously. At the 1937 meeting of the ICRU, this definition was extended to apply to gamma radiation. This approach, although a great step forward in standardisation, had the disadvantage of not being a direct measure of the absorption of radiation, and thereby the ionisation effect, in various types of matter including human tissue, and was a measurement only of the effect of the X-rays in a specific circumstance; the ionisation effect in dry air.\nIn 1940, Louis Harold Gray, who had been studying the effect of neutron damage on human tissue, together with William Valentine Mayneord and the radiobiologist John Read, published a paper in which a new unit of measure, dubbed the gram roentgen (symbol: gr) was proposed, and defined as \"that amount of neutron radiation which produces an increment in energy in unit volume of tissue equal to the increment of energy produced in unit volume of water by one roentgen of radiation\".  This unit was found to be equivalent to 88 ergs in air, and made the absorbed dose, as it subsequently became known, dependent on the interaction of the radiation with the irradiated material, not just an expression of radiation exposure or intensity, which the roentgen represented. In 1953 the ICRU recommended the rad, equal to 100 erg/g, as the new unit of measure of absorbed radiation. The rad was expressed in coherent cgs units. \nIn the late 1950s, the CGPM invited the ICRU to join other scientific bodies to work on the development of the International System of Units, or SI. The CCU decided to define the SI unit of absorbed radiation as energy deposited by reabsorbed charged particles per unit mass of absorbent material, which is how the rad had been defined, but in MKS units it would be equivalent to the joule per kilogram. This was confirmed in 1975 by the 15th CGPM, and the unit was named the \"gray\" in honour of Louis Harold Gray, who had died in 1965. The gray was thus equal to 100 rad. Notably, the centigray (numerically equivalent to the rad) is still widely used to describe absolute absorbed doses in radiotherapy.\nThe adoption of the gray by the 15th General Conference on Weights and Measures as the unit of measure of the absorption of ionizing radiation, specific energy absorption, and of kerma in 1975 was the culmination of over half a century of work, both in the understanding of the nature of ionizing radiation and in the creation of coherent radiation quantities and units.\n\n\n== Radiation-related quantities ==\n\nThe following table shows radiation quantities in SI and non-SI units.\n\n\n== See also ==\nDose area product \u2013 Quantity used to assess X radiation risk (Gy\u00b7cm2)\nInternational System of Units \u2013 Modern form of the metric system (SI base units)\nOrders of magnitude (radiation) \u2013 Comparison of a wide range of radiation dosages\nOrder of magnitude (unit) \u2013 Scale of numbers with a fixed ratio\nRad (radiation unit) \u2013 Non-SI unit measuring absorbed dose of ionizing radiation\nRoentgen (unit) \u2013 Measurement of radiation exposure\nRoentgen equivalent man \u2013 Radiation unit\nSievert, SI derived unit of dose equivalent radiation\nSI derived unit \u2013 Measurement unit derived from basic metric value\n\n\n== Notes ==\n\n\n== References ==\n\n\n== External links ==\nBoyd, M.A. (March 1\u20135, 2009). The Confusing World of Radiation Dosimetry\u20149444 (PDF). WM2009 Conference (Waste Management Symposium). Phoenix, AZ. Archived from the original (PDF) on 2016-12-21. Retrieved 2014-07-07. An account of chronological differences between USA and ICRP dosimetry systems.",
        "unit": "gray",
        "url": "https://en.wikipedia.org/wiki/Gray_(unit)"
    },
    {
        "_id": "Ton",
        "clean": "Ton",
        "text": "Ton is any of several units of measure of mass, volume or force. It has a long history and has acquired several meanings and uses.\nAs a unit of mass, ton can mean:\n\nthe long ton, which is 2,240 pounds (1,016.0 kilograms)\nthe tonne, also called the metric ton, which is 1,000 kilograms (about 2,204.6 pounds) or 1 megagram.\nthe short ton, which is 2,000 pounds (907.2 kilograms)\nIts original use as a unit of volume has continued in the capacity of cargo ships and in units such as the freight ton and a number of other units, ranging from 35 to 100 cubic feet (0.99 to 2.83 m3) in size. Recent specialized uses include the ton as a means of truck classification. It can also be used as a unit of energy, or in refrigeration as a unit of power, sometimes called a ton of refrigeration.\nBecause the ton (of any system of measuring weight) is usually the heaviest unit named in colloquial speech, its name also has figurative uses, singular and plural, informally meaning a large amount or quantity, or to a great degree, as in \"There's a ton of bees in this hive,\" \"We have tons of homework,\" and \"I love you a ton.\"\n\n\n== History ==\nThe ton is derived from the tun, the term applied to a cask of the largest capacity. This could contain a volume between 175 and 213 imperial gallons (210 and 256 US gal; 800 and 970 L), which could weigh around 2,000 pounds (910 kg) and occupy some 60 cubic feet (1.7 m3) of space.\n\n\n== Units of mass/weight ==\nThere are several similar units of mass or volume called the ton:\n\nThe difference between the short ton and the other common forms (\"long\" and \"metric\") is about 10%, while the metric and long tons differ by less than 2%.\nThe metric tonne is usually distinguished by its spelling when written, but in the United States and United Kingdom, it is pronounced the same as ton, hence is often spoken as \"metric ton\" when it is necessary to make the distinction. In the United Kingdom the final \"e\" of \"tonne\" can also be pronounced (). In Australia, it is pronounced .\nIn Ireland and most members of the Commonwealth of Nations, a ton is defined as 2,240 pounds (1,016.04691 kg).\nIn the United States and Canada, a ton is defined as 2,000 pounds (907.18474 kg).\n\n\n=== Other units of mass/weight ===\nDeadweight ton (abbreviation 'DWT' or 'dwt') is a measure of a ship's carrying capacity, including bunker oil, fresh water, ballast water, crew, and provisions. It is expressed in tonnes (1,000 kilograms (2,205 lb)) or long tons (2,240 pounds (1,016 kg)). This measurement is also used in the U.S. tonnage of naval ships.\nIncreasingly, tonnes are being used rather than long tons in measuring the displacement of ships.\nHarbour ton, used in South Africa in the 20th century, was equivalent to (2,000 pounds (907 kg)) or 1 short ton.\n\nAssay ton (abbreviation 'AT') is not a unit of measurement but a standard quantity used in assaying ores of precious metals. A short assay ton is approximately 29.17 g (1.029 oz) and a long assay ton is approximately 32.67 g (1.152 oz). These amounts bear the same ratio to a milligram as a short or long ton bears to a troy ounce. Therefore, the number of milligrams of a particular metal found in a sample weighing one assay ton gives the number of troy ounces of metal contained in a ton of ore.\nIn documents that predate 1960 the word ton is sometimes spelled tonne, but in more recent documents tonne refers exclusively to the metric ton.\nIn nuclear power plants tHM and MTHM mean tonnes of heavy metals, and MTU means tonnes of uranium. In the steel industry, the abbreviation THM means 'tons/tonnes hot metal', which refers to the amount of liquid iron or steel that is produced, particularly in the context of blast furnace production or specific consumption.\nA dry ton or dry tonne has the same mass value, but the material (sludge, slurries, compost, and similar mixtures in which solid material is soaked with or suspended in water) has been dried to a relatively low, consistent moisture level (dry weight). If the material is in its natural, wet state, it is called a wet ton or wet tonne.\n\n\n=== Subdivisions ===\nBoth the UK definition of long ton and US definition of short ton have similar underlying bases. Each is equivalent to 20 hundredweight; however, they are long 51 kilograms (112 lb) or short hundredweight 45 kilograms (100 lb), respectively.\nBefore the 20th century there were several definitions. Prior to the 15th century in England, the ton was 20 hundredweight, each of 108 lb, giving a ton of 2,160 pounds (980 kg).  In the 19th century in different parts of Britain, definitions of 2,240, or 2,352, or 2,400 lb were used, with 2,000 lb for explosives; the legal ton was usually 2,240 lb.\nIn the United Kingdom, Canada, Australia, and other areas that had used the imperial system, the tonne is the form of ton legal in trade.\n\n\n== Units of volume ==\n\nThe displacement, essentially the weight, of a ship is traditionally expressed in long tons. To simplify measurement it is determined by measuring the volume, rather than weight, of water displaced, and calculating the weight from the volume and density.\nFor practical purposes the displacement ton (DT) is a unit of volume, 35 cubic feet (0.9911 m3), the approximate volume occupied by one ton of seawater (the actual volume varies with salinity and temperature). It is slightly less than the 224 imperial gallons (1.018 m3) of the water ton (based on distilled water).\nOne measurement ton or freight ton is equal to 40 cubic feet (1.133 m3), but historically it has had several different definitions. It is used to determine the amount of money to be charged in loading, unloading, or carrying different sorts of cargo. In general if a cargo is heavier than salt water, the actual weight is used. If it is lighter than salt water, e.g. feathers, freight is calculated in measurement tons of 40 cubic feet.\nGross tonnage and net tonnage are volumetric measures of the cargo-carrying capacity of a ship.\nThe Panama Canal/Universal Measurement System (PC/UMS) is based on net tonnage, modified for Panama Canal billing purposes. PC/UMS is based on a mathematical formula to calculate a vessel's total volume; a PC/UMS net ton is equivalent to 100 cubic feet of capacity.\nThe water ton is used chiefly in Great Britain, in statistics dealing with petroleum products, and is defined as 224 imperial gallons (35.96 cu ft; 1.018 m3), the volume occupied by 1 long ton (2,240 lb; 1,016 kg) of water under the conditions that define the imperial gallon.\n\n\n== Units of energy and power ==\n\n\n=== Ton of TNT ===\n\nA ton of TNT or tonne of TNT is a unit of energy equal to 109 (thermochemical) calories, also known as a gigacalorie (Gcal), equal to 4.184 gigajoules (GJ).\nA kiloton of TNT or kilotonne of TNT is a unit of energy equal to 1012 calories, also known as a teracalorie (Tcal), equal to 4.184 terajoules (TJ).\nA megaton of TNT (1,000,000 tonnes) or megatonne of TNT is a unit of energy equal to 1015 calories, also known (infrequently) as a petacalorie (Pcal), equal to 4.184 petajoules (PJ).\nThese are small calories (cal). The large or dietary calorie (Cal) is equal to one kilocalorie (kcal), and is gradually being replaced by the latter correct term.\nEarly values for the explosive energy released by trinitrotoluene (TNT) ranged from 900 to 1100 calories per gram. In order to standardise the use of the term TNT as a unit of energy, an arbitrary value was assigned based on 1,000 calories (1 kcal or 4.184 kJ) per gram. Thus there is no longer a direct connection to the chemical TNT itself. It is now merely a unit of energy that happens to be expressed using words normally associated with mass (e.g., kilogram, tonne, pound). The definition applies for both spellings: ton of TNT and tonne of TNT.\nMeasurements in tons of TNT have been used primarily to express nuclear weapon yields, though they have also been used since in seismology as well.\n\n\n=== Tonne of oil equivalent ===\nA tonne of oil equivalent (toe), sometimes ton of oil equivalent, is a conventional value, based on the amount of energy released by burning one tonne of crude oil. The unit is used, for example, by the International Energy Agency (IEA), for the reported world energy consumption as TPES in millions of toe (Mtoe).\n\nOther sources convert 1 toe into 1.28 tonne of coal equivalent (tce). 1 toe is also standardized as 7.33 barrel of oil equivalent (boe).\n\n\n=== Tonne of coal equivalent ===\nA tonne of coal equivalent (tce), sometimes ton of coal equivalent, is a conventional value, based on the amount of energy released by burning one tonne of coal. Plural name is tonnes of coal equivalent.\n\nPer the World Coal Association: 1 tonne of coal equivalent (tce) corresponds to 0.697 tonne of oil equivalent (toe)\nPer the International Energy Agency 1 tonne of coal equivalent (tce) corresponds to 0.700 tonne of oil equivalent (toe)\n\n\n=== Refrigeration ===\n\nThe unit ton is used in refrigeration and air conditioning to measure the rate of heat absorption. Prior to the introduction of mechanical refrigeration, cooling was accomplished by delivering ice.  Installing one ton of mechanical refrigeration capacity replaced the daily delivery of one ton of ice.\n\nIn North America, a standard ton of refrigeration is 12,000 BTU/h (3,517 W). \"The heat absorption per day is approximately the heat of fusion of 1 ton of ice at 32 \u00b0F (0 \u00b0C).\" This is approximately the power required to melt one short ton (2,000 lb or 907 kg) of ice at 0 \u00b0C (32 \u00b0F) in 24 hours, thus representing the delivery of 1 short ton (0.893 long tons; 0.907 t) of ice per day.\nA less common usage is the power required to cool 1 long ton (2,240 lb or 1,016 kg = 1 long ton or 1.120 short tons or 1.016 t) of water by 1 \u00b0F (0.56 \u00b0C) every 10 minutes = 13,440 BTU/h (3,939 W).\nThe refrigeration ton is commonly abbreviated as RT.\n\n\n== Colloquial English ==\nTon is also used informally, often as slang, to mean a large amount of something.\nIn Britain, a ton is colloquially used to refer to 100 of a given unit. Ton can thus refer to a speed of 100 miles per hour, and is prefixed by an indefinite article, e.g. \"Lee was doing a ton down the motorway\"; to money e.g. \"How much did you pay for that?\" \"A ton\" (\u00a3100); to 100 points in a game e.g. \"Eric just threw a ton in our darts game\" (in some games, e.g. cricket, more commonly called a century); or to a hundred of any other countable figure.\n\n\n== See also ==\n\n\n== References ==",
        "unit": "ton",
        "url": "https://en.wikipedia.org/wiki/Ton"
    },
    {
        "_id": "Minute_and_second_of_arc",
        "clean": "Minute and second of arc",
        "text": "A minute of arc, arcminute (arcmin), arc minute, or minute arc, denoted by the symbol \u2032, is a unit of angular measurement equal to \u20601/60\u2060 of one degree. Since one degree is \u20601/360\u2060 of a turn, or complete rotation, one arcminute is \u20601/21600\u2060 of a turn. The nautical mile (nmi) was originally defined as the arc length of a minute of latitude on a spherical Earth, so the actual Earth's circumference is very near 21600 nmi.  A minute of arc is \u2060\u03c0/10800\u2060 of a radian.\nA second of arc, arcsecond (arcsec), or arc second, denoted by the symbol \u2033, is \u20601/60\u2060 of an arcminute, \u20601/3600\u2060 of a degree, \u20601/1296000\u2060 of a turn, and \u2060\u03c0/648000\u2060 (about \u20601/206264.8\u2060) of a radian.\nThese units originated in Babylonian astronomy as sexagesimal (base 60) subdivisions of the degree; they are used in fields that involve very small angles, such as astronomy, optometry, ophthalmology, optics, navigation, land surveying, and marksmanship.\nTo express even smaller angles, standard SI prefixes can be employed; the milliarcsecond (mas) and microarcsecond (\u03bcas), for instance, are commonly used in astronomy. For a three-dimensional area such as on a sphere, square arcminutes or seconds may be used.\n\n\n== Symbols and abbreviations ==\nThe prime symbol \u2032 (U+2032) designates the arcminute, though a single quote ' (U+0027) is commonly used where only ASCII characters are permitted. One arcminute is thus written as 1\u2032. It is also abbreviated as arcmin or amin.\nSimilarly, double prime \u2033 (U+2033) designates the arcsecond, though a double quote \" (U+0022) is commonly used where only ASCII characters are permitted. One arcsecond is thus written as 1\u2033. It is also abbreviated as arcsec or asec.\n\nIn celestial navigation, seconds of arc are rarely used in calculations, the preference usually being for degrees, minutes, and decimals of a minute, for example, written as 42\u00b0 25.32\u2032 or 42\u00b0 25.322\u2032. This notation has been carried over into marine GPS and aviation GPS receivers, which normally display latitude and longitude in the latter format by default.\n\n\n== Common examples ==\nThe average apparent diameter of the full Moon is about 31 arcminutes, or 0.52\u00b0.\nOne arcminute is the approximate distance two contours can be separated by, and still be distinguished by, a person with 20/20 vision.\nOne arcsecond is the approximate angle subtended by a U.S. dime coin (18 mm) at a distance of 4 kilometres (about 2.5 mi). An arcsecond is also the angle subtended by\n\nan object of diameter 725.27 km at a distance of one astronomical unit,\nan object of diameter 45866916 km at one light-year,\nan object of diameter one astronomical unit (149597870.7 km) at a distance of one parsec, per the definition of the latter.\nOne milliarcsecond is about the size of a half dollar, seen from a distance equal to that between the Washington Monument and the Eiffel Tower.\nOne microarcsecond is about the size of a period at the end of a sentence in the Apollo mission manuals left on the Moon as seen from Earth.\nOne nanoarcsecond is about the size of a penny on Neptune's moon Triton as observed from Earth.\nAlso notable examples of size in arcseconds are:\n\nHubble Space Telescope has calculational resolution of 0.05 arcseconds and actual resolution of almost 0.1 arcseconds, which is close to the diffraction limit.\nAt crescent phase, Venus measures between 60.2 and 66 seconds of arc.\n\n\n== History ==\nThe concepts of degrees, minutes, and seconds\u2014as they relate to the measure of both angles and time\u2014derive from Babylonian astronomy and time-keeping. Influenced by the Sumerians, the ancient Babylonians divided the Sun's perceived motion across the sky over the course of one full day into 360 degrees. Each degree was subdivided into 60 minutes and each minute into 60 seconds. Thus, one Babylonian degree was equal to four minutes in modern terminology, one Babylonian minute to four modern seconds, and one Babylonian second to \u20601/15\u2060 (approximately 0.067) of a modern second.\n\n\n== Uses ==\n\n\n=== Astronomy ===\n\nSince antiquity, the arcminute and arcsecond have been used in astronomy: in the ecliptic coordinate system as latitude (\u03b2) and longitude (\u03bb); in the horizon system as altitude (Alt) and azimuth (Az); and in the equatorial coordinate system as declination (\u03b4). All are measured in degrees, arcminutes, and arcseconds. The principal exception is right ascension (RA) in equatorial coordinates, which is measured in time units of hours, minutes, and seconds.\nContrary to what one might assume, minutes and seconds of arc do not directly relate to minutes and seconds of time, in either the rotational frame of the Earth around its own axis (day), or the Earth's rotational frame around the Sun (year). The Earth's rotational rate around its own axis is 15 minutes of arc per minute of time (360 degrees / 24 hours in day); the Earth's rotational rate around the Sun (not entirely constant) is roughly 24 minutes of time per minute of arc (from 24 hours in day), which tracks the annual progression of the Zodiac. Both of these factor in what astronomical objects you can see from surface telescopes (time of year) and when you can best see them (time of day), but neither are in unit correspondence. For simplicity, the explanations given assume a degree/day in the Earth's annual rotation around the Sun, which is off by roughly 1%. The same ratios hold for seconds, due to the consistent factor of 60 on both sides.  \nThe arcsecond is also often used to describe small astronomical angles such as the angular diameters of planets (e.g. the angular diameter of Venus which varies between 10\u2033 and 60\u2033); the proper motion of stars; the separation of components of binary star systems; and parallax, the small change of position of a star or Solar System body as the Earth revolves about the Sun. These small angles may also be written in milliarcseconds (mas), or thousandths of an arcsecond. The unit of distance called the parsec, abbreviated from the parallax angle of one arc second, was developed for such parallax measurements. The distance from the Sun to a celestial object is the reciprocal of the angle, measured in arcseconds, of the object's apparent movement caused by parallax.\nThe European Space Agency's astrometric satellite Gaia, launched in 2013, can approximate star positions to 7 microarcseconds (\u03bcas).\nApart from the Sun, the star with the largest angular diameter from Earth is R Doradus, a red giant with a diameter of 0.05\u2033. Because of the effects of atmospheric blurring, ground-based telescopes will smear the image of a star to an angular diameter of about 0.5\u2033; in poor conditions this increases to 1.5\u2033 or even more. The dwarf planet Pluto has proven difficult to resolve because its angular diameter is about 0.1\u2033. Techniques exist for improving seeing on the ground. Adaptive optics, for example, can produce images around 0.05\u2033 on a 10 m class telescope.\nSpace telescopes are not affected by the Earth's atmosphere but are diffraction limited. For example, the Hubble Space Telescope can reach an angular size of stars down to about 0.1\u2033.\n\n\n=== Cartography ===\nMinutes (\u2032) and seconds (\u2033) of arc are also used in cartography and navigation. At sea level one minute of arc along the equator equals exactly one geographical mile (not to be confused with international mile or statute mile) along the Earth's equator or approximately one nautical mile (1,852 metres; 1.151 miles). A second of arc, one sixtieth of this amount, is roughly 30 metres (98 feet). The exact distance varies along meridian arcs or any other great circle arcs because the figure of the Earth is slightly oblate (bulges a third of a percent at the equator).\nPositions are traditionally given using degrees, minutes, and seconds of arcs for latitude, the arc north or south of the equator, and for longitude, the arc east or west of the Prime Meridian. Any position on or above the Earth's reference ellipsoid can be precisely given with this method. However, when it is inconvenient to use  base-60 for minutes and seconds, positions are frequently expressed as decimal fractional degrees to an equal amount of precision. Degrees given to three decimal places (\u20601/1000\u2060 of a degree) have about \u20601/4\u2060 the precision of degrees-minutes-seconds (\u20601/3600\u2060 of a degree) and specify locations within about 120 metres (390 feet).  For navigational purposes positions are given in degrees and decimal minutes, for instance The Needles lighthouse is at 50\u00ba 39.734\u2019N 001\u00ba 35.500\u2019W.\n\n\n=== Property cadastral surveying ===\nRelated to cartography, property boundary surveying using the metes and bounds system and cadastral surveying relies on fractions of a degree to describe property lines' angles in reference to cardinal directions. A boundary \"mete\" is described with a beginning reference point, the cardinal direction North or South followed by an angle less than 90 degrees and a second cardinal direction, and a linear distance. The boundary runs the specified linear distance from the beginning point, the direction of the distance being determined by rotating the first cardinal direction the specified angle toward the second cardinal direction. For example, North 65\u00b0 39\u2032 18\u2033 West 85.69 feet would describe a line running from the starting point 85.69 feet in a direction 65\u00b0 39\u2032 18\u2033 (or 65.655\u00b0) away from north toward the west.\n\n\n=== Firearms ===\n\nThe arcminute is commonly found in the firearms industry and literature, particularly concerning the precision of rifles, though the industry refers to it as minute of angle (MOA).  It is especially popular as a unit of measurement with shooters familiar with the imperial measurement system because 1 MOA subtends a circle with a diameter of 1.047 inches (which is often rounded to just 1 inch) at 100 yards (2.66 cm at 91 m or 2.908 cm at 100 m), a traditional distance on American target ranges.  The subtension is linear with the distance, for example, at 500 yards, 1 MOA subtends 5.235 inches, and at 1000 yards 1 MOA subtends 10.47 inches.\nSince many modern telescopic sights are adjustable in half (\u20601/2\u2060), quarter (\u20601/4\u2060) or eighth (\u20601/8\u2060) MOA increments, also known as clicks, zeroing and adjustments are made by counting 2, 4 and 8 clicks per MOA respectively.\nFor example, if the point of impact is 3 inches high and 1.5 inches left of the point of aim at 100 yards (which for instance could be measured by using a spotting scope with a calibrated reticle, or a  target delineated for such purposes), the scope needs to be adjusted 3 MOA down, and 1.5 MOA right. Such adjustments are trivial when the scope's adjustment dials have a MOA scale printed on them, and even figuring the right number of clicks is relatively easy on scopes that click in fractions of MOA. This makes zeroing and adjustments much easier:\n\nTo adjust a 1\u20442 MOA scope 3 MOA down and 1.5 MOA right, the scope needs to be adjusted 3 \u00d7 2 = 6 clicks down and 1.5 x 2 = 3 clicks right\nTo adjust a 1\u20444 MOA scope 3 MOA down and 1.5 MOA right, the scope needs to be adjusted 3 x 4 = 12 clicks down and 1.5 \u00d7 4 = 6 clicks right\nTo adjust a 1\u20448 MOA scope 3 MOA down and 1.5 MOA right, the scope needs to be adjusted 3 x 8 = 24 clicks down and 1.5 \u00d7 8 = 12 clicks right\n\nAnother common system of measurement in firearm scopes is the milliradian (mrad). Zeroing an mrad based scope is easy for users familiar with base ten systems. The most common adjustment value in mrad based scopes is \u20601/10\u2060 mrad (which approximates 1\u20443 MOA).\n\nTo adjust a \u20601/10\u2060 mrad scope 0.9 mrad down and 0.4 mrad right, the scope needs to be adjusted 9 clicks down and 4 clicks right (which equals approximately 3 and 1.5 MOA respectively).\nOne thing to be aware of is that some MOA scopes, including some higher-end models, are calibrated such that an adjustment of 1 MOA on the scope knobs corresponds to exactly 1 inch of impact adjustment on a target at 100 yards, rather than the mathematically correct 1.047 inches. This is commonly known as the Shooter's MOA (SMOA) or Inches Per Hundred Yards (IPHY). While the difference between one true MOA and one SMOA is less than half of an inch even at 1000 yards, this error compounds significantly on longer range shots that may require adjustment upwards of 20\u201330 MOA to compensate for the bullet drop. If a shot requires an adjustment of 20 MOA or more, the difference between true MOA and SMOA will add up to 1 inch or more. In competitive target shooting, this might mean the difference between a hit and a miss.\nThe physical group size equivalent to m minutes of arc can be calculated as follows: group size = tan(\u2060m/60\u2060) \u00d7 distance. In the example previously given, for 1 minute of arc, and substituting 3,600 inches for 100 yards, 3,600 tan(\u20601/60\u2060) \u2248 1.047 inches. In metric units 1 MOA at 100 metres \u2248 2.908 centimetres.\nSometimes, a precision-oriented firearm's performance will be measured in MOA.  This simply means that under ideal conditions (i.e. no wind, high-grade ammo, clean barrel, and a stable mounting platform such as a vise or a benchrest used to eliminate shooter error), the gun is capable of producing a group of shots whose center points (center-to-center) fit into a circle, the average diameter of circles in several groups can be subtended by that amount of arc.  For example, a 1 MOA rifle should be capable, under ideal conditions, of repeatably shooting 1-inch groups at 100 yards.  Most higher-end rifles are warrantied by their manufacturer to shoot under a given MOA threshold (typically 1 MOA or better) with specific ammunition and no error on the shooter's part.  For example, Remington's M24 Sniper Weapon System is required to shoot 0.8 MOA or better, or be rejected from sale by quality control.\nRifle manufacturers and gun magazines often refer to this capability as sub-MOA, meaning a gun consistently shooting groups under 1 MOA.  This means that a single group of 3 to 5 shots at 100 yards, or the average of several groups, will measure less than 1 MOA between the two furthest shots in the group, i.e. all shots fall within 1 MOA.  If larger samples are taken (i.e., more shots per group) then group size typically increases, however this will ultimately average out.  If a rifle was truly a 1 MOA rifle, it would be just as likely that two consecutive shots land exactly on top of each other as that they land 1 MOA apart.  For 5-shot groups, based on 95% confidence, a rifle that normally shoots 1 MOA can be expected to shoot groups between 0.58 MOA and 1.47 MOA, although the majority of these groups will be under 1 MOA. What this means in practice is if a rifle that shoots 1-inch groups on average at 100 yards shoots a group measuring 0.7 inches followed by a group that is 1.3 inches, this is not statistically abnormal.\nThe metric system counterpart of the MOA is the milliradian (mrad or 'mil'), being equal to 1\u20441000 of the target range, laid out on a circle that has the observer as centre and the target range as radius. The number of milliradians on a full such circle therefore always is equal to 2 \u00d7 \u03c0 \u00d7 1000, regardless the target range. Therefore, 1 MOA \u2248 0.2909 mrad. This means that an object which spans 1 mrad on the reticle is at a range that is in metres equal to the object's linear size in millimetres (e.g. an object of 100 mm subtending 1 mrad is 100 metres away). So there is no conversion factor required, contrary to the MOA system.  A reticle with markings (hashes or dots) spaced with a one mrad apart (or a fraction of a mrad) are collectively called a mrad reticle. If the markings are round they are called mil-dots.\nIn the table below conversions from mrad to metric values are exact (e.g. 0.1 mrad equals exactly 10 mm at 100 metres), while conversions of minutes of arc to both metric and imperial values are approximate.\n\n1\u2032 at 100 yards is about 1.047 inches\n1\u2032 \u2248 0.291 mrad (or 29.1 mm at 100 m, approximately  30 mm at 100 m)\n1 mrad \u2248 3.44\u2032, so \u20601/10\u2060 mrad \u2248 \u20601/3\u2060\u2032\n0.1 mrad equals exactly 1 cm at 100 m, or exactly 0.36 inches at 100 yards\n\n\n=== Human vision ===\nIn humans, 20/20 vision is the ability to resolve a spatial pattern separated by a visual angle of one minute of arc, from a distance of twenty feet.\nA 20/20 letter subtends 5 minutes of arc total.\n\n\n=== Materials ===\nThe deviation from parallelism between two surfaces, for instance in optical engineering, is usually measured in arcminutes or arcseconds.\nIn addition, arcseconds are sometimes used in rocking curve (\u03c9-scan) x ray diffraction measurements of high-quality epitaxial thin films.\n\n\n=== Manufacturing ===\nSome measurement devices make use of arcminutes and arcseconds to measure angles when the object being measured is too small for direct visual inspection. For instance, a toolmaker's optical comparator will often include an option to measure in \"minutes and seconds\".\n\n\n== See also ==\nGradian\nDegree (angle) \u00a7 Subdivisions\nSexagesimal \u00a7 Modern usage\nSquare minute\nSquare second\nSteradian\nMilliradian\nNautical mile\n\n\n== References ==\n\n\n== External links ==\nMOA/ mils By Robert Simeone\nA Guide to calculate distance using MOA Scope by Steve Coffman",
        "unit": "minute of arc",
        "url": "https://en.wikipedia.org/wiki/Minute_and_second_of_arc"
    },
    {
        "_id": "Torque",
        "clean": "Torque",
        "text": "In physics and mechanics, torque is the rotational analogue of linear force. It is also referred to as the moment of force (also abbreviated to moment). The symbol for torque is typically \n  \n    \n      \n        \n          \u03c4\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\tau }}}\n  \n, the lowercase Greek letter tau. When being referred to as moment of force, it is commonly denoted by M. Just as a linear force is a push or a pull applied to a body, a torque can be thought of as a twist applied to an object with respect to a chosen point; for example, driving a screw uses torque, which is applied by the screwdriver rotating around its axis. A force of three newtons applied two metres from the fulcrum, for example, exerts the same torque as a force of one newton applied six metres from the fulcrum.\n\n\n== History ==\n\nThe term torque (from Latin torqu\u0113re, 'to twist') is said to have been suggested by James Thomson and appeared in print in April, 1884. Usage is attested the same year by Silvanus P. Thompson in the first edition of Dynamo-Electric Machinery. Thompson motivates the term as follows:\n\nJust as the Newtonian definition of force is that which produces or tends to produce motion (along a line), so torque may be defined as that which produces or tends to produce torsion (around an axis). It is better to use a term which treats this action as a single definite entity than to use terms like \"couple\" and \"moment\", which suggest more complex ideas. The single notion of a twist applied to turn a shaft is better than the more complex notion of applying a linear force (or a pair of forces) with a certain leverage.\nToday, torque is referred to using different vocabulary depending on geographical location and field of study. This article follows the definition used in US physics in its usage of the word torque.\nIn the UK and in US mechanical engineering, torque is referred to as moment of force, usually shortened to moment. This terminology can be traced back to at least 1811 in Sim\u00e9on Denis Poisson's Trait\u00e9 de m\u00e9canique. An English translation of Poisson's work appears in 1842.\n\n\n== Definition and relation to other physical quantities ==\n\nA force applied perpendicularly to a lever multiplied by its distance from the lever's fulcrum (the length of the lever arm) is its torque. Therefore, torque is defined as the product of the magnitude of the perpendicular component of the force and the distance of the line of action of a force from the point around which it is being determined. In three dimensions, the torque is a pseudovector; for point particles, it is given by the cross product of the displacement vector and the force vector. The direction of the torque can be determined by using the right hand grip rule: if the fingers of the right hand are curled from the direction of the lever arm to the direction of the force, then the thumb points in the direction of the torque. It follows that the torque vector is perpendicular to both the position and force vectors and defines the plane in which the two vectors lie. The resulting torque vector direction is determined by the right-hand rule. Therefore any force directed parallel to the particle's position vector does not produce a torque. The magnitude of torque applied to a rigid body depends on three quantities: the force applied, the lever arm vector connecting the point about which the torque is being measured to the point of force application, and the angle between the force and lever arm vectors. In symbols:\n\n  \n    \n      \n        \n          \u03c4\n        \n        =\n        \n          r\n        \n        \u00d7\n        \n          F\n        \n        \n        \u27f9\n        \n        \u03c4\n        =\n        r\n        \n          F\n          \n            \u22a5\n          \n        \n        =\n        r\n        F\n        sin\n        \u2061\n        \u03b8\n      \n    \n    {\\displaystyle {\\boldsymbol {\\tau }}=\\mathbf {r} \\times \\mathbf {F} \\implies \\tau =rF_{\\perp }=rF\\sin \\theta }\n  \n\nwhere\n\n  \n    \n      \n        \n          \u03c4\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\tau }}}\n  \n is the torque vector and \n  \n    \n      \n        \u03c4\n      \n    \n    {\\displaystyle \\tau }\n  \n is the magnitude of the torque,\n\n  \n    \n      \n        \n          r\n        \n      \n    \n    {\\displaystyle \\mathbf {r} }\n  \n is the position vector (a vector from the point about which the torque is being measured to the point where the force is applied), and r is the magnitude of the position vector,\n\n  \n    \n      \n        \n          F\n        \n      \n    \n    {\\displaystyle \\mathbf {F} }\n  \n is the force vector, F is the magnitude of the force vector and F\u22a5 is the amount of force directed perpendicularly to the position of the particle,\n\n  \n    \n      \n        \u00d7\n      \n    \n    {\\displaystyle \\times }\n  \n denotes the cross product, which produces a vector that is perpendicular both to r and to F following the right-hand rule,\n\n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n  \n is the angle between the force vector and the lever arm vector.\nThe SI unit for torque is the newton-metre (N\u22c5m). For more on the units of torque, see \u00a7 Units.\n\n\n=== Relationship with the angular momentum ===\nThe net torque on a body determines the rate of change of the body's angular momentum,\n\n  \n    \n      \n        \n          \u03c4\n        \n        =\n        \n          \n            \n              \n                d\n              \n              \n                L\n              \n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\tau }}={\\frac {\\mathrm {d} \\mathbf {L} }{\\mathrm {d} t}}}\n  \n\nwhere L is the angular momentum vector and t is time. For the motion of a point particle,\n\n  \n    \n      \n        \n          L\n        \n        =\n        I\n        \n          \u03c9\n        \n        ,\n      \n    \n    {\\displaystyle \\mathbf {L} =I{\\boldsymbol {\\omega }},}\n  \n\nwhere \n  \n    \n      \n        I\n        =\n        m\n        \n          r\n          \n            2\n          \n        \n      \n    \n    {\\textstyle I=mr^{2}}\n  \n is the moment of inertia and \u03c9 is the orbital angular velocity pseudovector. It follows that\n\n  \n    \n      \n        \n          \n            \u03c4\n          \n          \n            \n              n\n              e\n              t\n            \n          \n        \n        =\n        \n          I\n          \n            1\n          \n        \n        \n          \n            \n              \n                \u03c9\n                \n                  1\n                \n              \n              \u02d9\n            \n          \n        \n        \n          \n            \n              \n                e\n                \n                  1\n                \n              \n              ^\n            \n          \n        \n        +\n        \n          I\n          \n            2\n          \n        \n        \n          \n            \n              \n                \u03c9\n                \n                  2\n                \n              \n              \u02d9\n            \n          \n        \n        \n          \n            \n              \n                e\n                \n                  2\n                \n              \n              ^\n            \n          \n        \n        +\n        \n          I\n          \n            3\n          \n        \n        \n          \n            \n              \n                \u03c9\n                \n                  3\n                \n              \n              \u02d9\n            \n          \n        \n        \n          \n            \n              \n                e\n                \n                  3\n                \n              \n              ^\n            \n          \n        \n        +\n        \n          I\n          \n            1\n          \n        \n        \n          \u03c9\n          \n            1\n          \n        \n        \n          \n            \n              d\n              \n                \n                  \n                    \n                      e\n                      \n                        1\n                      \n                    \n                    ^\n                  \n                \n              \n            \n            \n              d\n              t\n            \n          \n        \n        +\n        \n          I\n          \n            2\n          \n        \n        \n          \u03c9\n          \n            2\n          \n        \n        \n          \n            \n              d\n              \n                \n                  \n                    \n                      e\n                      \n                        2\n                      \n                    \n                    ^\n                  \n                \n              \n            \n            \n              d\n              t\n            \n          \n        \n        +\n        \n          I\n          \n            3\n          \n        \n        \n          \u03c9\n          \n            3\n          \n        \n        \n          \n            \n              d\n              \n                \n                  \n                    \n                      e\n                      \n                        3\n                      \n                    \n                    ^\n                  \n                \n              \n            \n            \n              d\n              t\n            \n          \n        \n        =\n        I\n        \n          \n            \n              \u03c9\n              \u02d9\n            \n          \n        \n        +\n        \n          \u03c9\n        \n        \u00d7\n        (\n        I\n        \n          \u03c9\n        \n        )\n      \n    \n    {\\displaystyle {\\boldsymbol {\\tau }}_{\\mathrm {net} }=I_{1}{\\dot {\\omega _{1}}}{\\hat {\\boldsymbol {e_{1}}}}+I_{2}{\\dot {\\omega _{2}}}{\\hat {\\boldsymbol {e_{2}}}}+I_{3}{\\dot {\\omega _{3}}}{\\hat {\\boldsymbol {e_{3}}}}+I_{1}\\omega _{1}{\\frac {d{\\hat {\\boldsymbol {e_{1}}}}}{dt}}+I_{2}\\omega _{2}{\\frac {d{\\hat {\\boldsymbol {e_{2}}}}}{dt}}+I_{3}\\omega _{3}{\\frac {d{\\hat {\\boldsymbol {e_{3}}}}}{dt}}=I{\\boldsymbol {\\dot {\\omega }}}+{\\boldsymbol {\\omega }}\\times (I{\\boldsymbol {\\omega }})}\n  \n\nusing the derivative of a vector is\n  \n    \n      \n        \n          \n            \n              d\n              \n                \n                  \n                    \n                      e\n                      \n                        i\n                      \n                    \n                    ^\n                  \n                \n              \n            \n            \n              d\n              t\n            \n          \n        \n        =\n        \n          \u03c9\n        \n        \u00d7\n        \n          \n            \n              \n                e\n                \n                  i\n                \n              \n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {d{\\boldsymbol {\\hat {e_{i}}}} \\over dt}={\\boldsymbol {\\omega }}\\times {\\boldsymbol {\\hat {e_{i}}}}}\n  \nThis equation is the rotational analogue of Newton's second law for point particles, and is valid for any type of trajectory. In some simple cases like a rotating disc, where only the moment of inertia on rotating axis is, the rotational Newton's second law can be\n  \n    \n      \n        \n          \u03c4\n        \n        =\n        I\n        \n          \u03b1\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\tau }}=I{\\boldsymbol {\\alpha }}}\n  \nwhere \n  \n    \n      \n        \n          \u03b1\n        \n        =\n        \n          \n            \n              \u03c9\n              \u02d9\n            \n          \n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\alpha }}={\\dot {\\boldsymbol {\\omega }}}}\n  \n. \n\n\n==== Proof of the equivalence of definitions ====\nThe definition of angular momentum for a single point particle is:\n\n  \n    \n      \n        \n          L\n        \n        =\n        \n          r\n        \n        \u00d7\n        \n          p\n        \n      \n    \n    {\\displaystyle \\mathbf {L} =\\mathbf {r} \\times \\mathbf {p} }\n  \n\nwhere p is the particle's linear momentum and r is the position vector from the origin. The time-derivative of this is:\n\n  \n    \n      \n        \n          \n            \n              \n                d\n              \n              \n                L\n              \n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        =\n        \n          r\n        \n        \u00d7\n        \n          \n            \n              \n                d\n              \n              \n                p\n              \n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        +\n        \n          \n            \n              \n                d\n              \n              \n                r\n              \n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        \u00d7\n        \n          p\n        \n        .\n      \n    \n    {\\displaystyle {\\frac {\\mathrm {d} \\mathbf {L} }{\\mathrm {d} t}}=\\mathbf {r} \\times {\\frac {\\mathrm {d} \\mathbf {p} }{\\mathrm {d} t}}+{\\frac {\\mathrm {d} \\mathbf {r} }{\\mathrm {d} t}}\\times \\mathbf {p} .}\n  \n\nThis result can easily be proven by splitting the vectors into components and applying the product rule. But because the rate of change of linear momentum is force \n  \n    \n      \n        \n          F\n        \n      \n    \n    {\\textstyle \\mathbf {F} }\n  \n and the rate of change of position is velocity \n  \n    \n      \n        \n          v\n        \n      \n    \n    {\\textstyle \\mathbf {v} }\n  \n,\n\n  \n    \n      \n        \n          \n            \n              \n                d\n              \n              \n                L\n              \n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        =\n        \n          r\n        \n        \u00d7\n        \n          F\n        \n        +\n        \n          v\n        \n        \u00d7\n        \n          p\n        \n      \n    \n    {\\displaystyle {\\frac {\\mathrm {d} \\mathbf {L} }{\\mathrm {d} t}}=\\mathbf {r} \\times \\mathbf {F} +\\mathbf {v} \\times \\mathbf {p} }\n  \n\nThe cross product of momentum \n  \n    \n      \n        \n          p\n        \n      \n    \n    {\\displaystyle \\mathbf {p} }\n  \n with its associated velocity \n  \n    \n      \n        \n          v\n        \n      \n    \n    {\\displaystyle \\mathbf {v} }\n  \n is zero because velocity and momentum are parallel, so the second term vanishes. Therefore, torque on a particle is equal to the first derivative of its angular momentum with respect to time. If multiple forces are applied, according Newton's second law it follows that\n  \n    \n      \n        \n          \n            \n              \n                d\n              \n              \n                L\n              \n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        =\n        \n          r\n        \n        \u00d7\n        \n          \n            F\n          \n          \n            \n              n\n              e\n              t\n            \n          \n        \n        =\n        \n          \n            \u03c4\n          \n          \n            \n              n\n              e\n              t\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle {\\frac {\\mathrm {d} \\mathbf {L} }{\\mathrm {d} t}}=\\mathbf {r} \\times \\mathbf {F} _{\\mathrm {net} }={\\boldsymbol {\\tau }}_{\\mathrm {net} }.}\n  \n\nThis is a general proof for point particles, but it can be generalized to a system of point particles by applying the above proof to each of the point particles and then summing over all the point particles. Similarly, the proof can be generalized to a continuous mass by applying the above proof to each point within the mass, and then integrating over the entire mass.\n\n\n=== Derivatives of torque ===\nIn physics, rotatum is the derivative of torque with respect to time\n  \n    \n      \n        \n          P\n        \n        =\n        \n          \n            \n              \n                d\n              \n              \n                \u03c4\n              \n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle \\mathbf {P} ={\\frac {\\mathrm {d} {\\boldsymbol {\\tau }}}{\\mathrm {d} t}},}\n  \nwhere \u03c4 is torque.\nThis word is derived from the Latin word rot\u0101tus meaning 'to rotate', but the term rotatum is not universally recognized but is commonly used. There is not a universally accepted lexicon to indicate the successive derivatives of rotatum, even if sometimes various proposals have been made.\nUsing the cross product definition of torque, an alternative expression for rotatum is:\n\n  \n    \n      \n        \n          P\n        \n        =\n        \n          r\n        \n        \u00d7\n        \n          \n            \n              \n                d\n              \n              \n                F\n              \n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        +\n        \n          \n            \n              \n                d\n              \n              \n                r\n              \n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        \u00d7\n        \n          F\n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {P} =\\mathbf {r} \\times {\\frac {\\mathrm {d} \\mathbf {F} }{\\mathrm {d} t}}+{\\frac {\\mathrm {d} \\mathbf {r} }{\\mathrm {d} t}}\\times \\mathbf {F} .}\n  \n\nBecause the rate of change of force is yank \n  \n    \n      \n        \n          Y\n        \n      \n    \n    {\\textstyle \\mathbf {Y} }\n  \n and the rate of change of position is velocity \n  \n    \n      \n        \n          v\n        \n      \n    \n    {\\textstyle \\mathbf {v} }\n  \n, the expression can be further simplified to:\n\n  \n    \n      \n        \n          P\n        \n        =\n        \n          r\n        \n        \u00d7\n        \n          Y\n        \n        +\n        \n          v\n        \n        \u00d7\n        \n          F\n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {P} =\\mathbf {r} \\times \\mathbf {Y} +\\mathbf {v} \\times \\mathbf {F} .}\n  \n\n\n=== Relationship with power and energy ===\nThe law of conservation of energy can also be used to understand torque. If a force is allowed to act through a distance, it is doing mechanical work. Similarly, if torque is allowed to act through an angular displacement, it is doing work. Mathematically, for rotation about a fixed axis through the center of mass, the work W can be expressed as\n\n  \n    \n      \n        W\n        =\n        \n          \u222b\n          \n            \n              \u03b8\n              \n                1\n              \n            \n          \n          \n            \n              \u03b8\n              \n                2\n              \n            \n          \n        \n        \u03c4\n         \n        \n          d\n        \n        \u03b8\n        ,\n      \n    \n    {\\displaystyle W=\\int _{\\theta _{1}}^{\\theta _{2}}\\tau \\ \\mathrm {d} \\theta ,}\n  \n\nwhere \u03c4 is torque, and \u03b81 and \u03b82 represent (respectively) the initial and final angular positions of the body.\nIt follows from the work\u2013energy principle that W also represents the change in the rotational kinetic energy Er of the body, given by\n\n  \n    \n      \n        \n          E\n          \n            \n              r\n            \n          \n        \n        =\n        \n          \n            \n              1\n              2\n            \n          \n        \n        I\n        \n          \u03c9\n          \n            2\n          \n        \n        ,\n      \n    \n    {\\displaystyle E_{\\mathrm {r} }={\\tfrac {1}{2}}I\\omega ^{2},}\n  \n\nwhere I is the moment of inertia of the body and \u03c9 is its angular speed.\nPower is the work per unit time, given by\n\n  \n    \n      \n        P\n        =\n        \n          \u03c4\n        \n        \u22c5\n        \n          \u03c9\n        \n        ,\n      \n    \n    {\\displaystyle P={\\boldsymbol {\\tau }}\\cdot {\\boldsymbol {\\omega }},}\n  \n\nwhere P is power, \u03c4 is torque, \u03c9 is the angular velocity, and \n  \n    \n      \n        \u22c5\n      \n    \n    {\\displaystyle \\cdot }\n  \n represents the scalar product.\nAlgebraically, the equation may be rearranged to compute torque for a given angular speed and power output. The power injected by the torque depends only on the instantaneous angular speed \u2013 not on whether the angular speed increases, decreases, or remains constant while the torque is being applied (this is equivalent to the linear case where the power injected by a force depends only on the instantaneous speed \u2013 not on the resulting acceleration, if any).\n\n\n==== Proof ====\nThe work done by a variable force acting over a finite linear displacement \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n is given by integrating the force with respect to an elemental linear displacement \n  \n    \n      \n        \n          d\n        \n        \n          s\n        \n      \n    \n    {\\displaystyle \\mathrm {d} \\mathbf {s} }\n  \n\n  \n    \n      \n        W\n        =\n        \n          \u222b\n          \n            \n              s\n              \n                1\n              \n            \n          \n          \n            \n              s\n              \n                2\n              \n            \n          \n        \n        \n          F\n        \n        \u22c5\n        \n          d\n        \n        \n          s\n        \n      \n    \n    {\\displaystyle W=\\int _{s_{1}}^{s_{2}}\\mathbf {F} \\cdot \\mathrm {d} \\mathbf {s} }\n  \n\nHowever, the infinitesimal linear displacement \n  \n    \n      \n        \n          d\n        \n        \n          s\n        \n      \n    \n    {\\displaystyle \\mathrm {d} \\mathbf {s} }\n  \n is related to a corresponding angular displacement \n  \n    \n      \n        \n          d\n        \n        \n          \u03b8\n        \n      \n    \n    {\\displaystyle \\mathrm {d} {\\boldsymbol {\\theta }}}\n  \n and the radius vector \n  \n    \n      \n        \n          r\n        \n      \n    \n    {\\displaystyle \\mathbf {r} }\n  \n as \n\n  \n    \n      \n        \n          d\n        \n        \n          s\n        \n        =\n        \n          d\n        \n        \n          \u03b8\n        \n        \u00d7\n        \n          r\n        \n      \n    \n    {\\displaystyle \\mathrm {d} \\mathbf {s} =\\mathrm {d} {\\boldsymbol {\\theta }}\\times \\mathbf {r} }\n  \n\nSubstitution in the above expression for work, , gives\n\n  \n    \n      \n        W\n        =\n        \n          \u222b\n          \n            \n              s\n              \n                1\n              \n            \n          \n          \n            \n              s\n              \n                2\n              \n            \n          \n        \n        \n          F\n        \n        \u22c5\n        \n          d\n        \n        \n          \u03b8\n        \n        \u00d7\n        \n          r\n        \n      \n    \n    {\\displaystyle W=\\int _{s_{1}}^{s_{2}}\\mathbf {F} \\cdot \\mathrm {d} {\\boldsymbol {\\theta }}\\times \\mathbf {r} }\n  \n\nThe expression inside the integral is a scalar triple product \n  \n    \n      \n        \n          F\n        \n        \u22c5\n        \n          d\n        \n        \n          \u03b8\n        \n        \u00d7\n        \n          r\n        \n        =\n        \n          r\n        \n        \u00d7\n        \n          F\n        \n        \u22c5\n        \n          d\n        \n        \n          \u03b8\n        \n      \n    \n    {\\displaystyle \\mathbf {F} \\cdot \\mathrm {d} {\\boldsymbol {\\theta }}\\times \\mathbf {r} =\\mathbf {r} \\times \\mathbf {F} \\cdot \\mathrm {d} {\\boldsymbol {\\theta }}}\n  \n, but as per the definition of torque, and since the parameter of integration has been changed from linear displacement to angular displacement, the equation becomes\n\n  \n    \n      \n        W\n        =\n        \n          \u222b\n          \n            \n              \u03b8\n              \n                1\n              \n            \n          \n          \n            \n              \u03b8\n              \n                2\n              \n            \n          \n        \n        \n          \u03c4\n        \n        \u22c5\n        \n          d\n        \n        \n          \u03b8\n        \n      \n    \n    {\\displaystyle W=\\int _{\\theta _{1}}^{\\theta _{2}}{\\boldsymbol {\\tau }}\\cdot \\mathrm {d} {\\boldsymbol {\\theta }}}\n  \n\nIf the torque and the angular displacement are in the same direction, then the scalar product reduces to a product of magnitudes; i.e., \n  \n    \n      \n        \n          \u03c4\n        \n        \u22c5\n        \n          d\n        \n        \n          \u03b8\n        \n        =\n        \n          |\n          \n            \u03c4\n          \n          |\n        \n        \n          |\n          \n            \n              d\n            \n            \n              \u03b8\n            \n          \n          |\n        \n        cos\n        \u2061\n        0\n        =\n        \u03c4\n        \n        \n          d\n        \n        \u03b8\n      \n    \n    {\\displaystyle {\\boldsymbol {\\tau }}\\cdot \\mathrm {d} {\\boldsymbol {\\theta }}=\\left|{\\boldsymbol {\\tau }}\\right|\\left|\\mathrm {d} {\\boldsymbol {\\theta }}\\right|\\cos 0=\\tau \\,\\mathrm {d} \\theta }\n  \n giving\n\n  \n    \n      \n        W\n        =\n        \n          \u222b\n          \n            \n              \u03b8\n              \n                1\n              \n            \n          \n          \n            \n              \u03b8\n              \n                2\n              \n            \n          \n        \n        \u03c4\n        \n        \n          d\n        \n        \u03b8\n      \n    \n    {\\displaystyle W=\\int _{\\theta _{1}}^{\\theta _{2}}\\tau \\,\\mathrm {d} \\theta }\n  \n\n\n== Principle of moments ==\nThe principle of moments, also known as Varignon's theorem (not to be confused with the geometrical theorem of the same name) states that the resultant torques due to several forces applied to about a point is equal to the sum of the contributing torques:\n\n  \n    \n      \n        \u03c4\n        =\n        \n          \n            r\n          \n          \n            1\n          \n        \n        \u00d7\n        \n          \n            F\n          \n          \n            1\n          \n        \n        +\n        \n          \n            r\n          \n          \n            2\n          \n        \n        \u00d7\n        \n          \n            F\n          \n          \n            2\n          \n        \n        +\n        \u2026\n        +\n        \n          \n            r\n          \n          \n            N\n          \n        \n        \u00d7\n        \n          \n            F\n          \n          \n            N\n          \n        \n        .\n      \n    \n    {\\displaystyle \\tau =\\mathbf {r} _{1}\\times \\mathbf {F} _{1}+\\mathbf {r} _{2}\\times \\mathbf {F} _{2}+\\ldots +\\mathbf {r} _{N}\\times \\mathbf {F} _{N}.}\n  \n\nFrom this it follows that the torques resulting from N number of forces acting around a pivot on an object are balanced when\n\n  \n    \n      \n        \n          \n            r\n          \n          \n            1\n          \n        \n        \u00d7\n        \n          \n            F\n          \n          \n            1\n          \n        \n        +\n        \n          \n            r\n          \n          \n            2\n          \n        \n        \u00d7\n        \n          \n            F\n          \n          \n            2\n          \n        \n        +\n        \u2026\n        +\n        \n          \n            r\n          \n          \n            N\n          \n        \n        \u00d7\n        \n          \n            F\n          \n          \n            N\n          \n        \n        =\n        \n          0\n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {r} _{1}\\times \\mathbf {F} _{1}+\\mathbf {r} _{2}\\times \\mathbf {F} _{2}+\\ldots +\\mathbf {r} _{N}\\times \\mathbf {F} _{N}=\\mathbf {0} .}\n  \n\n\n== Units ==\nTorque has the dimension of force times distance, symbolically T\u22122L2M and those fundamental dimensions are the same as that for energy or work. Official SI literature indicates newton-metre, is properly denoted N\u22c5m, as the unit for torque; although this is dimensionally equivalent to the joule, which is the unit of energy, the latter can never used for torque. In the case of torque, the unit is assigned to a vector, whereas for energy, it is assigned to a scalar. This means that the dimensional equivalence of the newton-metre and the joule may be applied in the former but not in the latter case. This problem is addressed in orientational analysis, which treats the radian as a base unit rather than as a dimensionless unit.\nThe traditional imperial units for torque are the pound foot (lbf-ft), or, for small values, the pound inch (lbf-in). In the US, torque is most commonly referred to as the foot-pound (denoted as either lb-ft or ft-lb) and the inch-pound (denoted as in-lb). Practitioners depend on context and the hyphen in the abbreviation to know that these refer to torque and not to energy or moment of mass (as the symbolism ft-lb would properly imply).\n\n\n=== Conversion to other units ===\nA conversion factor may be necessary when using different units of power or torque. For example, if rotational speed (unit: revolution per minute or second) is used in place of angular speed (unit: radian per second), we must multiply by 2\u03c0 radians per revolution. In the following formulas, P is power, \u03c4 is torque, and \u03bd (Greek letter nu) is rotational speed.\n\n  \n    \n      \n        P\n        =\n        \u03c4\n        \u22c5\n        2\n        \u03c0\n        \u22c5\n        \u03bd\n      \n    \n    {\\displaystyle P=\\tau \\cdot 2\\pi \\cdot \\nu }\n  \n\nShowing units:\n\n  \n    \n      \n        \n          P\n          \n            \n              W\n            \n          \n        \n        =\n        \n          \u03c4\n          \n            \n              N\n              \n                \u22c5\n              \n              m\n            \n          \n        \n        \u22c5\n        2\n        \n          \u03c0\n          \n            \n              r\n              a\n              d\n              \n                /\n              \n              r\n              e\n              v\n            \n          \n        \n        \u22c5\n        \n          \u03bd\n          \n            \n              r\n              e\n              v\n              \n                /\n              \n              s\n            \n          \n        \n      \n    \n    {\\displaystyle P_{\\rm {W}}=\\tau _{\\rm {N{\\cdot }m}}\\cdot 2\\pi _{\\rm {rad/rev}}\\cdot \\nu _{\\rm {rev/s}}}\n  \n\nDividing by 60 seconds per minute gives us the following.\n\n  \n    \n      \n        \n          P\n          \n            \n              W\n            \n          \n        \n        =\n        \n          \n            \n              \n                \u03c4\n                \n                  \n                    N\n                    \n                      \u22c5\n                    \n                    m\n                  \n                \n              \n              \u22c5\n              2\n              \n                \u03c0\n                \n                  \n                    r\n                    a\n                    d\n                    \n                      /\n                    \n                    r\n                    e\n                    v\n                  \n                \n              \n              \u22c5\n              \n                \u03bd\n                \n                  \n                    r\n                    e\n                    v\n                    \n                      /\n                    \n                    m\n                    i\n                    n\n                  \n                \n              \n            \n            \n              60\n               \n              s\n              \n                /\n              \n              m\n              i\n              n\n            \n          \n        \n      \n    \n    {\\displaystyle P_{\\rm {W}}={\\frac {\\tau _{\\rm {N{\\cdot }m}}\\cdot 2\\pi _{\\rm {rad/rev}}\\cdot \\nu _{\\rm {rev/min}}}{\\rm {60~s/min}}}}\n  \n\nwhere rotational speed is in revolutions per minute (rpm, rev/min).\nSome people (e.g., American automotive engineers) use horsepower (mechanical) for power, foot-pounds (lbf\u22c5ft) for torque and rpm for rotational speed. This results in the formula changing to:\n\n  \n    \n      \n        \n          P\n          \n            \n              h\n              p\n            \n          \n        \n        =\n        \n          \n            \n              \n                \u03c4\n                \n                  \n                    l\n                    b\n                    f\n                    \n                      \u22c5\n                    \n                    f\n                    t\n                  \n                \n              \n              \u22c5\n              2\n              \n                \u03c0\n                \n                  \n                    r\n                    a\n                    d\n                    \n                      /\n                    \n                    r\n                    e\n                    v\n                  \n                \n              \n              \u22c5\n              \n                \u03bd\n                \n                  \n                    r\n                    e\n                    v\n                    \n                      /\n                    \n                    m\n                    i\n                    n\n                  \n                \n              \n            \n            \n              33\n              ,\n              000\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle P_{\\rm {hp}}={\\frac {\\tau _{\\rm {lbf{\\cdot }ft}}\\cdot 2\\pi _{\\rm {rad/rev}}\\cdot \\nu _{\\rm {rev/min}}}{33,000}}.}\n  \n\nThe constant below (in foot-pounds per minute) changes with the definition of the horsepower; for example, using metric horsepower, it becomes approximately 32,550.\nThe use of other units (e.g., BTU per hour for power) would require a different custom conversion factor.\n\n\n==== Derivation ====\nFor a rotating object, the linear distance covered at the circumference of rotation is the product of the radius with the angle covered.  That is:  linear distance = radius \u00d7 angular distance. And by definition, linear distance = linear speed \u00d7 time = radius \u00d7 angular speed \u00d7 time.\nBy the definition of torque: torque = radius \u00d7 force. We can rearrange this to determine force = torque \u00f7 radius. These two values can be substituted into the definition of power:\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  power\n                \n              \n              \n                \n                =\n                \n                  \n                    \n                      \n                        force\n                      \n                      \u22c5\n                      \n                        linear distance\n                      \n                    \n                    time\n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  \n                    \n                      \n                        (\n                        \n                          \n                            \n                              torque\n                              r\n                            \n                          \n                        \n                        )\n                      \n                      \u22c5\n                      (\n                      r\n                      \u22c5\n                      \n                        angular speed\n                      \n                      \u22c5\n                      t\n                      )\n                    \n                    t\n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  torque\n                \n                \u22c5\n                \n                  angular speed\n                \n                .\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}{\\text{power}}&={\\frac {{\\text{force}}\\cdot {\\text{linear distance}}}{\\text{time}}}\\\\[6pt]&={\\frac {\\left({\\dfrac {\\text{torque}}{r}}\\right)\\cdot (r\\cdot {\\text{angular speed}}\\cdot t)}{t}}\\\\[6pt]&={\\text{torque}}\\cdot {\\text{angular speed}}.\\end{aligned}}}\n  \n\nThe radius r and time t have dropped out of the equation.  However, angular speed must be in radians per unit of time, by the assumed direct relationship between linear speed and angular speed at the beginning of the derivation.  If the rotational speed is measured in revolutions per unit of time, the linear speed and distance are increased proportionately by 2\u03c0 in the above derivation to give:\n\n  \n    \n      \n        \n          power\n        \n        =\n        \n          torque\n        \n        \u22c5\n        2\n        \u03c0\n        \u22c5\n        \n          rotational speed\n        \n        .\n        \n      \n    \n    {\\displaystyle {\\text{power}}={\\text{torque}}\\cdot 2\\pi \\cdot {\\text{rotational speed}}.\\,}\n  \n\nIf torque is in newton-metres and rotational speed in revolutions per second, the above equation gives power in newton-metres per second or watts.  If Imperial units are used, and if torque is in pounds-force feet and rotational speed in revolutions per minute, the above equation gives power in foot pounds-force per minute.  The horsepower form of the equation is then derived by applying the conversion factor 33,000 ft\u22c5lbf/min per horsepower:\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  power\n                \n              \n              \n                \n                =\n                \n                  torque\n                \n                \u22c5\n                2\n                \u03c0\n                \u22c5\n                \n                  rotational speed\n                \n                \u22c5\n                \n                  \n                    \n                      \n                        ft\n                      \n                      \n                        \u22c5\n                      \n                      \n                        lbf\n                      \n                    \n                    min\n                  \n                \n                \u22c5\n                \n                  \n                    horsepower\n                    \n                      33\n                      ,\n                      000\n                      \u22c5\n                      \n                        \n                          \n                            \n                              ft\n                            \n                            \u22c5\n                            \n                              lbf\n                            \n                          \n                          min\n                        \n                      \n                    \n                  \n                \n              \n            \n            \n              \n              \n                \n                \u2248\n                \n                  \n                    \n                      \n                        torque\n                      \n                      \u22c5\n                      \n                        RPM\n                      \n                    \n                    \n                      5\n                      ,\n                      252\n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}{\\text{power}}&={\\text{torque}}\\cdot 2\\pi \\cdot {\\text{rotational speed}}\\cdot {\\frac {{\\text{ft}}{\\cdot }{\\text{lbf}}}{\\text{min}}}\\cdot {\\frac {\\text{horsepower}}{33,000\\cdot {\\frac {{\\text{ft}}\\cdot {\\text{lbf}}}{\\text{min}}}}}\\\\[6pt]&\\approx {\\frac {{\\text{torque}}\\cdot {\\text{RPM}}}{5,252}}\\end{aligned}}}\n  \n\nbecause \n  \n    \n      \n        5252.113122\n        \u2248\n        \n          \n            \n              33\n              ,\n              000\n            \n            \n              2\n              \u03c0\n            \n          \n        \n        .\n        \n      \n    \n    {\\displaystyle 5252.113122\\approx {\\frac {33,000}{2\\pi }}.\\,}\n  \n\n\n== Special cases and other facts ==\n\n\n=== Moment arm formula ===\n\nA very useful special case, often given as the definition of torque in fields other than physics, is as follows:\n\n  \n    \n      \n        \u03c4\n        =\n        (\n        \n          moment arm\n        \n        )\n        (\n        \n          force\n        \n        )\n        .\n      \n    \n    {\\displaystyle \\tau =({\\text{moment arm}})({\\text{force}}).}\n  \n\nThe construction of the \"moment arm\" is shown in the figure to the right, along with the vectors r and F mentioned above. The problem with this definition is that it does not give the direction of the torque but only the magnitude, and hence it is difficult to use in three-dimensional cases. If the force is perpendicular to the displacement vector r, the moment arm will be equal to the distance to the centre, and torque will be a maximum for the given force. The equation for the magnitude of a torque, arising from a perpendicular force:\n\n  \n    \n      \n        \u03c4\n        =\n        (\n        \n          distance to centre\n        \n        )\n        (\n        \n          force\n        \n        )\n        .\n      \n    \n    {\\displaystyle \\tau =({\\text{distance to centre}})({\\text{force}}).}\n  \n\nFor example, if a person places a force of 10 N at the terminal end of a wrench that is 0.5 m long (or a force of 10 N acting 0.5 m from the twist point of a wrench of any length), the torque will be 5 N\u22c5m \u2013 assuming that the person moves the wrench by applying force in the plane of movement and perpendicular to the wrench.\n\n\n=== Static equilibrium ===\nFor an object to be in static equilibrium, not only must the sum of the forces be zero, but also the sum of the torques (moments) about any point. For a two-dimensional situation with horizontal and vertical forces, the sum of the forces requirement is two equations: \u03a3H = 0 and \u03a3V = 0, and the torque a third equation: \u03a3\u03c4 = 0. That is, to solve statically determinate equilibrium problems in two-dimensions, three equations are used.\n\n\n=== Net force versus torque ===\nWhen the net force on the system is zero, the torque measured from any point in space is the same. For example, the torque on a current-carrying loop in a uniform magnetic field is the same regardless of the point of reference. If the net force \n  \n    \n      \n        \n          F\n        \n      \n    \n    {\\displaystyle \\mathbf {F} }\n  \n is not zero, and \n  \n    \n      \n        \n          \n            \u03c4\n          \n          \n            1\n          \n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\tau }}_{1}}\n  \n is the torque measured from \n  \n    \n      \n        \n          \n            r\n          \n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {r} _{1}}\n  \n, then the torque measured from \n  \n    \n      \n        \n          \n            r\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {r} _{2}}\n  \n is\n\n  \n    \n      \n        \n          \n            \u03c4\n          \n          \n            2\n          \n        \n        =\n        \n          \n            \u03c4\n          \n          \n            1\n          \n        \n        +\n        (\n        \n          \n            r\n          \n          \n            2\n          \n        \n        \u2212\n        \n          \n            r\n          \n          \n            1\n          \n        \n        )\n        \u00d7\n        \n          F\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\tau }}_{2}={\\boldsymbol {\\tau }}_{1}+(\\mathbf {r} _{2}-\\mathbf {r} _{1})\\times \\mathbf {F} }\n  \n\n\n=== Machine torque ===\n\nTorque forms part of the basic specification of an engine: the power output of an engine is expressed as its torque multiplied by the angular speed of the drive shaft. Internal-combustion engines produce useful torque only over a limited range of rotational speeds (typically from around 1,000\u20136,000 rpm for a small car). One can measure the varying torque output over that range with a dynamometer, and show it as a torque curve. Steam engines and electric motors tend to produce maximum torque close to zero rpm, with the torque diminishing as rotational speed rises (due to increasing friction and other constraints). Reciprocating steam-engines and electric motors can start heavy loads from zero rpm without a clutch.\nIn practice, the relationship between power and torque can be observed in bicycles: Bicycles are typically composed of two road wheels, front and rear gears (referred to as sprockets) meshing with a chain, and a derailleur mechanism if the bicycle's transmission system allows multiple gear ratios to be used (i.e. multi-speed bicycle), all of which attached to the frame. A cyclist, the person who rides the bicycle, provides the input power by turning pedals, thereby cranking the front sprocket (commonly referred to as chainring). The input power provided by the cyclist is equal to the product of angular speed (i.e. the number of pedal revolutions per minute times 2\u03c0) and the torque at the spindle of the bicycle's crankset. The bicycle's drivetrain transmits the input power to the road wheel, which in turn conveys the received power to the road as the output power of the bicycle. Depending on the gear ratio of the bicycle, a (torque, angular speed)input pair is converted to a (torque, angular speed)output pair. By using a larger rear gear, or by switching to a lower gear in multi-speed bicycles, angular speed of the road wheels is decreased while the torque is increased, product of which (i.e. power) does not change.\n\n\n=== Torque multiplier ===\n\nTorque can be multiplied via three methods: by locating the fulcrum such that the length of a lever is increased; by using a longer lever; or by the use of a speed-reducing gearset or gear box.  Such a mechanism multiplies torque, as rotation rate is reduced.\n\n\n== See also ==\n\n\n== References ==\n\n\n== External links ==\n\n\"Horsepower and Torque\" Archived 2007-03-28 at the Wayback Machine An article showing how power, torque, and gearing affect a vehicle's performance.\nTorque and Angular Momentum in Circular Motion  on Project PHYSNET.\nAn interactive simulation of torque\nTorque Unit Converter\nA feel for torque Archived 2021-05-08 at the Wayback Machine An order-of-magnitude interactive.",
        "unit": "torque",
        "url": "https://en.wikipedia.org/wiki/Torque"
    },
    {
        "_id": "Base_pair",
        "clean": "Base pair",
        "text": "A base pair (bp) is a fundamental unit of double-stranded nucleic acids consisting of two nucleobases bound to each other by hydrogen bonds.  They form the building blocks of the DNA double helix and contribute to the folded structure of both DNA and RNA. Dictated by specific hydrogen bonding patterns, \"Watson\u2013Crick\" (or \"Watson\u2013Crick\u2013Franklin\") base pairs (guanine\u2013cytosine and adenine\u2013thymine) allow the DNA helix to maintain a regular helical structure that is subtly dependent on its nucleotide sequence. The complementary nature of this based-paired structure provides a redundant copy of the genetic information encoded within each strand of DNA. The regular structure and data redundancy provided by the DNA double helix make DNA well suited to the storage of genetic information, while base-pairing between DNA and incoming nucleotides provides the mechanism through which DNA polymerase replicates DNA and RNA polymerase transcribes DNA into RNA. Many DNA-binding proteins can recognize specific base-pairing patterns that identify particular regulatory regions of genes.\nIntramolecular base pairs can occur within single-stranded nucleic acids. This is particularly important in RNA molecules (e.g., transfer RNA), where Watson\u2013Crick base pairs (guanine\u2013cytosine and adenine\u2013uracil) permit the formation of short double-stranded helices, and a wide variety of non\u2013Watson\u2013Crick interactions (e.g., G\u2013U or A\u2013A) allow RNAs to fold into a vast range of specific three-dimensional structures. In addition, base-pairing between transfer RNA (tRNA) and messenger RNA (mRNA) forms the basis for the molecular recognition events that result in the nucleotide sequence of mRNA becoming translated into the amino acid sequence of proteins via the genetic code.\nThe size of an individual gene or an organism's entire genome is often measured in base pairs because DNA is usually double-stranded. Hence, the number of total base pairs is equal to the number of nucleotides in one of the strands (with the exception of non-coding single-stranded regions of telomeres). The haploid human genome (23 chromosomes) is estimated to be about 3.2 billion base pairs long and to contain 20,000\u201325,000 distinct protein-coding genes. A kilobase (kb) is a unit of measurement in molecular biology equal to 1000 base pairs of DNA or RNA. The total number of DNA base pairs on Earth is estimated at 5.0\u00d71037 with a weight of 50 billion tonnes. In comparison, the total mass of the biosphere has been estimated to be as much as 4 TtC (trillion tons of carbon).\n\n\n== Hydrogen bonding and stability ==\n\nHydrogen bonding is the chemical interaction that underlies the base-pairing rules described above. Appropriate geometrical correspondence of hydrogen bond donors and acceptors allows only the \"right\" pairs to form stably. DNA with high GC-content is more stable than DNA with low GC-content. Crucially, however, stacking interactions are primarily responsible for stabilising the double-helical structure; Watson-Crick base pairing's contribution to global structural stability is minimal, but its role in the specificity underlying complementarity is, by contrast, of maximal importance as this underlies the template-dependent processes of the central dogma (e.g. DNA replication).\nThe bigger nucleobases, adenine and guanine, are members of a class of double-ringed chemical structures called purines; the smaller nucleobases, cytosine and thymine (and uracil), are members of a class of single-ringed chemical structures called pyrimidines. Purines are complementary only with pyrimidines: pyrimidine\u2013pyrimidine pairings are energetically unfavorable because the molecules are too far apart for hydrogen bonding to be established; purine\u2013purine pairings are energetically unfavorable because the molecules are too close, leading to overlap repulsion. Purine\u2013pyrimidine base-pairing of AT or GC or UA (in RNA) results in proper duplex structure. The only other purine\u2013pyrimidine pairings would be AC and GT and UG (in RNA); these pairings are mismatches because the patterns of hydrogen donors and acceptors do not correspond. The GU pairing, with two hydrogen bonds, does occur fairly often in RNA (see wobble base pair).\nPaired DNA and RNA molecules are comparatively stable at room temperature, but the two nucleotide strands will separate above a melting point that is determined by the length of the molecules, the extent of mispairing (if any), and the GC content. Higher GC content results in higher melting temperatures; it is, therefore, unsurprising that the genomes of extremophile organisms such as Thermus thermophilus are particularly GC-rich. On the converse, regions of a genome that need to separate frequently \u2014 for example, the promoter regions for often-transcribed genes \u2014 are comparatively GC-poor (for example, see TATA box). GC content and melting temperature must also be taken into account when designing  primers for PCR reactions.\n\n\n=== Examples ===\nThe following DNA sequences illustrate pair double-stranded patterns. By convention, the top strand is written from the 5\u2032-end to the 3\u2032-end; thus, the bottom strand is written 3\u2032 to 5\u2032.\n\nA base-paired DNA sequence:\nATCGATTGAGCTCTAGCG\nTAGCTAACTCGAGATCGC\nThe corresponding RNA sequence, in which uracil is substituted for thymine in the RNA strand:\nAUCGAUUGAGCUCUAGCG\nUAGCUAACUCGAGAUCGC\n\n\n== Base analogs and intercalators ==\n\nChemical analogs of nucleotides can take the place of proper nucleotides and establish non-canonical base-pairing, leading to errors (mostly point mutations) in DNA replication and DNA transcription. This is due to their isosteric chemistry. One common mutagenic base analog is 5-bromouracil, which resembles thymine but can base-pair to guanine in its enol form.\nOther chemicals, known as DNA intercalators, fit into the gap between adjacent bases on a single strand and induce frameshift mutations by \"masquerading\" as a base, causing the DNA replication machinery to skip or insert additional nucleotides at the intercalated site. Most intercalators are large polyaromatic compounds and are known or suspected carcinogens. Examples include ethidium bromide and acridine.\n\n\n== Mismatch repair ==\nMismatched base pairs can be generated by errors of DNA replication and as intermediates during homologous recombination. The process of mismatch repair ordinarily must recognize and correctly repair a small number of base mispairs within a long sequence of normal DNA base pairs.  To repair mismatches formed during DNA replication, several distinctive repair processes have evolved to distinguish between the template strand and the newly formed strand so that only the newly inserted incorrect nucleotide is removed (in order to avoid generating a mutation).  The proteins employed in mismatch repair during DNA replication, and the clinical significance of defects in this process are described in the article DNA mismatch repair.  The process of mispair correction during recombination is described in the article gene conversion.\n\n\n== Length measurements ==\n\nThe following abbreviations are commonly used to describe the length of a D/RNA molecule:\n\nbp  = base pair\u2014one bp corresponds to approximately 3.4 \u00c5 (340 pm)  of length along the strand, and to roughly 618 or 643 daltons for DNA and RNA respectively.\nkb (= kbp) = kilo\u2013base-pair = 1,000 bp\nMb (= Mbp) = mega\u2013base-pair = 1,000,000 bp\nGb (= Gbp) = giga\u2013base-pair = 1,000,000,000 bp\nFor single-stranded DNA/RNA, units of nucleotides are used\u2014abbreviated nt (or knt, Mnt, Gnt)\u2014as they are not paired.\nTo distinguish between units of computer storage and bases, kbp, Mbp, Gbp, etc. may be used for base pairs.\nThe centimorgan is also often used to imply distance along a chromosome, but the number of base pairs it corresponds to varies widely. In the human genome, the centimorgan is about 1 million base pairs.\n\n\n== Unnatural base pair (UBP) ==\n\nAn unnatural base pair (UBP) is a designed subunit (or nucleobase) of DNA which is created in a laboratory and does not occur in nature.  DNA sequences have been described which use newly created nucleobases to form a third base pair, in addition to the two base pairs found in nature, A-T (adenine \u2013 thymine) and G-C (guanine \u2013 cytosine).  A few research groups have been searching for a third base pair for DNA, including teams led by Steven A. Benner, Philippe Marliere, Floyd E. Romesberg and Ichiro Hirao. Some new base pairs based on alternative hydrogen bonding, hydrophobic interactions and metal coordination have been reported.\nIn 1989 Steven Benner (then working at the Swiss Federal Institute of Technology in Zurich) and his team led with modified forms of cytosine and guanine into DNA molecules in vitro. The nucleotides, which encoded RNA and proteins, were successfully replicated in vitro. Since then, Benner's team has been trying to engineer cells that can make foreign bases from scratch, obviating the need for a feedstock.\nIn 2002, Ichiro Hirao's group in Japan developed an unnatural base pair between 2-amino-8-(2-thienyl)purine (s) and pyridine-2-one (y) that functions in transcription and translation, for the site-specific incorporation of non-standard amino acids into proteins. In 2006, they created 7-(2-thienyl)imidazo[4,5-b]pyridine (Ds) and pyrrole-2-carbaldehyde (Pa) as a third base pair for replication and transcription. Afterward, Ds and 4-[3-(6-aminohexanamido)-1-propynyl]-2-nitropyrrole (Px) was discovered as a high fidelity pair in PCR amplification. In 2013, they applied the Ds-Px pair to DNA aptamer generation by in vitro selection (SELEX) and demonstrated the genetic alphabet expansion significantly augment DNA aptamer affinities to target proteins.\nIn 2012, a group of American scientists led by Floyd Romesberg, a chemical biologist at the Scripps Research Institute in San Diego, California, published that his team designed an unnatural base pair (UBP).  The two new artificial nucleotides or Unnatural Base Pair (UBP) were named d5SICS and dNaM. More technically, these artificial nucleotides bearing hydrophobic nucleobases, feature two fused aromatic rings that form a (d5SICS\u2013dNaM) complex or base pair in DNA. His team designed a variety of in vitro or \"test tube\" templates containing the unnatural base pair and they confirmed that it was efficiently replicated with high fidelity in virtually all sequence contexts using the modern standard in vitro techniques, namely PCR amplification of DNA and PCR-based applications. Their results show that for PCR and PCR-based applications, the d5SICS\u2013dNaM unnatural base pair is functionally equivalent to a natural base pair, and when combined with the other two natural base pairs used by all organisms, A\u2013T and G\u2013C, they provide a fully functional and expanded six-letter \"genetic alphabet\".\nIn 2014 the same team from the Scripps Research Institute reported that they synthesized a stretch of circular DNA known as a plasmid containing natural T-A and C-G base pairs along with the best-performing UBP Romesberg's laboratory had designed and inserted it into cells of the common bacterium E. coli that successfully replicated the unnatural base pairs through multiple generations. The transfection did not hamper the growth of the E. coli cells and showed no sign of losing its unnatural base pairs to its natural DNA repair mechanisms. This is the first known example of a living organism passing along an expanded genetic code to subsequent generations. Romesberg said he and his colleagues created 300 variants to refine the design of nucleotides that would be stable enough and would be replicated as easily as the natural ones when the cells divide.  This was in part achieved by the addition of a supportive algal gene that expresses a nucleotide triphosphate transporter which efficiently imports the triphosphates of both d5SICSTP and dNaMTP into E. coli bacteria. Then, the natural bacterial replication pathways use them to accurately replicate a plasmid containing d5SICS\u2013dNaM. Other researchers were surprised that the bacteria replicated these human-made DNA subunits.\nThe successful incorporation of a third base pair is a significant breakthrough toward the goal of greatly expanding the number of amino acids which can be encoded by DNA, from the existing 20 amino acids to a theoretically possible 172, thereby expanding the potential for living organisms to produce novel proteins. The artificial strings of DNA do not encode for anything yet, but scientists speculate they could be designed to manufacture new proteins which could have industrial or pharmaceutical uses. Experts said the synthetic DNA incorporating the unnatural base pair raises the possibility of life forms based on a different DNA code.\n\n\n== Non-canonical base pairing ==\n\nIn addition to the canonical pairing, some conditions can also favour base-pairing with alternative base orientation, and number and geometry of hydrogen bonds. These pairings are accompanied by alterations to the local backbone shape.\nThe most common of these is the wobble base pairing that occurs between tRNAs and mRNAs at the third base position of many codons during transcription and during the charging of tRNAs by some tRNA synthetases. They have also been observed in the secondary structures of some RNA sequences.\nAdditionally, Hoogsteen base pairing (typically written as A\u2022U/T and G\u2022C) can exist in some DNA sequences (e.g. CA and TA dinucleotides) in dynamic equilibrium with standard Watson\u2013Crick pairing. They have also been observed in some protein\u2013DNA complexes.\nIn addition to these alternative base pairings, a wide range of base-base hydrogen bonding is observed in RNA secondary and tertiary structure. These bonds are often necessary for the precise, complex shape of an RNA, as well as its binding to interaction partners.\n\n\n== See also ==\nList of Y-DNA single-nucleotide polymorphisms\nNon-canonical base pairing\nChargaff's rules\n\n\n== References ==\n\n\n== Further reading ==\n\n\n== External links ==\n\nDAN\u2014webserver version of the EMBOSS tool for calculating melting temperatures",
        "unit": "giga base pair",
        "url": "https://en.wikipedia.org/wiki/Base_pair"
    },
    {
        "_id": "Fuel_efficiency",
        "clean": "Fuel efficiency",
        "text": "Fuel efficiency (or fuel economy) is a form of thermal efficiency, meaning the ratio of effort to result of a process that converts chemical potential energy contained in a carrier (fuel) into kinetic energy or work. Overall fuel efficiency may vary per device, which in turn may vary per application, and this spectrum of variance is often illustrated as a continuous energy profile. Non-transportation applications, such as industry, benefit from increased fuel efficiency, especially fossil fuel power plants or industries dealing with combustion, such as ammonia production during the Haber process.\nIn the context of transport, fuel economy is the energy efficiency of a particular vehicle,  given as a ratio of distance traveled per unit of fuel consumed. It is dependent on several factors  including engine efficiency, transmission design, and tire design. In most countries, using the metric system, fuel economy is stated as \"fuel consumption\" in liters per 100 kilometers (L/100 km) or kilometers per liter (km/L or kmpl). In a number of countries still using other systems, fuel economy is expressed in miles per gallon (mpg), for example in the US and usually also in the UK (imperial gallon); there is sometimes confusion as the imperial gallon is 20% larger than the US gallon so that mpg values are not directly comparable.  Traditionally, litres per mil were used in Norway and Sweden, but both have aligned to the EU standard of L/100 km. \nFuel consumption is a more accurate measure of a vehicle's performance because it is a linear relationship while fuel economy leads to distortions in efficiency improvements. Weight-specific efficiency (efficiency per unit weight) may be stated for freight, and passenger-specific efficiency (vehicle efficiency per passenger) for passenger vehicles.\n\n\n== Vehicle design ==\nFuel efficiency is dependent on many parameters of a vehicle, including its engine parameters, aerodynamic drag, weight, AC usage, fuel and rolling resistance. There have been advances in all areas of vehicle design in recent decades. Fuel efficiency of vehicles can also be improved by careful maintenance and driving habits.\nHybrid vehicles use two or more power sources for propulsion. In many designs, a small combustion engine is combined with electric motors.  Kinetic energy which would otherwise be lost to heat during braking is recaptured as electrical power to improve fuel efficiency. The larger batteries in these vehicles power the car's electronics, allowing the engine to shut off and avoid prolonged idling.\n\n\n== Fleet efficiency ==\n\nFleet efficiency describes the average efficiency of a population of vehicles.  Technological advances in efficiency may be offset by a change in buying habits with a propensity to heavier vehicles that are less fuel-efficient.\n\n\n== Energy efficiency terminology ==\nEnergy efficiency is similar to fuel efficiency but the input is usually in units of energy such as megajoules (MJ), kilowatt-hours (kW\u00b7h), kilocalories (kcal) or British thermal units (BTU). The inverse of \"energy efficiency\" is \"energy intensity\", or the amount of input energy required for a unit of output such as MJ/passenger-km (of passenger transport), BTU/ton-mile or kJ/t-km (of freight transport), GJ/t (for production of steel and other materials), BTU/(kW\u00b7h) (for electricity generation), or litres/100 km (of vehicle travel). Litres per 100 km is also a measure of \"energy intensity\" where the input is measured by the amount of fuel and the output is measured by the distance travelled.  For example: Fuel economy in automobiles.\nGiven a heat value of a fuel, it would be trivial to convert from fuel units (such as litres of gasoline) to energy units (such as MJ) and conversely. But there are two problems with comparisons made using energy units:\n\nThere are two different heat values for any hydrogen-containing fuel which can differ by several percent (see below).\nWhen comparing transportation energy costs, a kilowatt hour of electric energy may require an amount of fuel with heating value of 2 or 3 kilowatt hours to produce it.\n\n\n== Energy content of fuel ==\nThe specific energy content of a fuel is the heat energy obtained when a certain quantity is burned (such as a gallon, litre, kilogram).  It is sometimes called the heat of combustion.  There exists two different values of specific heat energy for the same batch of fuel.  One is the high (or gross) heat of combustion and the other is the low (or net) heat of combustion.  The high value is obtained when, after the combustion, the water in the exhaust is in liquid form.  For the low value, the exhaust has all the water in vapor form (steam).  Since water vapor gives up heat energy when it changes from vapor to liquid, the liquid water value is larger since it includes the latent heat of vaporization of water.  The difference between the high and low values is significant, about 8 or 9%.  This accounts for most of the apparent discrepancy in the heat value of gasoline. In the U.S. (and the table) the high heat values have traditionally been used, but in many other countries, the low heat values are commonly used.\n\nNeither the gross heat of combustion nor the net heat of combustion gives the theoretical amount of mechanical energy (work) that can be obtained from the reaction. (This is given by the change in Gibbs free energy, and is around 45.7 MJ/kg for gasoline.) The actual amount of mechanical work obtained from fuel (the inverse of the specific fuel consumption) depends on the engine. A figure of 17.6 MJ/kg is possible with a gasoline engine, and 19.1 MJ/kg for a diesel engine. See Brake-specific fuel consumption for more information.\n\n\n== Transportation ==\n\n\n=== Fuel efficiency of motor vehicles ===\n\n\n=== Driving technique ===\n\n\n== Advanced technology ==\nThe most efficient machines for converting energy to rotary motion are electric motors, as used in electric vehicles. However, electricity is not a primary energy source so the efficiency of the electricity production has also to be taken into account. Railway trains can be powered using electricity, delivered through an additional running rail, overhead catenary system or by on-board generators used in diesel-electric locomotives as common on the US and UK rail networks. Pollution produced from centralised generation of electricity is emitted at a distant power station, rather than \"on site\". Pollution can be reduced by using more railway electrification and low carbon power for electricity. Some railways, such as the French SNCF and Swiss federal railways derive most, if not 100% of their power, from hydroelectric or nuclear power stations, therefore atmospheric pollution from their rail networks is very low. This was reflected in a study by AEA Technology between a Eurostar train and airline journeys between London and Paris, which showed the trains on average emitting 10 times less CO2, per passenger, than planes, helped in part by French nuclear generation.\n\n\n=== Hydrogen fuel cells ===\nIn the future, hydrogen cars may be commercially available. Toyota is test-marketing vehicles powered by hydrogen fuel cells in southern California, where a series of hydrogen fueling stations has been established. Powered either through chemical reactions in a fuel cell that create electricity to drive very efficient electrical motors or by directly burning hydrogen in a combustion engine (near identically to a natural gas vehicle, and similarly compatible with both natural gas and gasoline); these vehicles promise to have near-zero pollution from the tailpipe (exhaust pipe). Potentially the atmospheric pollution could be minimal, provided the hydrogen is made by electrolysis using electricity from non-polluting sources such as solar, wind or hydroelectricity or nuclear. Commercial hydrogen production uses fossil fuels and produces more carbon dioxide than hydrogen.\nBecause there are pollutants involved in the manufacture and destruction of a car and the production, transmission and storage of electricity and hydrogen, the label \"zero pollution\" applies only to the car's conversion of stored energy into movement.\nIn 2004, a consortium of major auto-makers \u2014 BMW, General Motors, Honda, Toyota and Volkswagen/Audi \u2014 came up with \"Top Tier Detergent Gasoline Standard\" to gasoline brands in the US and Canada that meet their minimum standards for detergent content and do not contain metallic additives. Top Tier gasoline contains higher levels of detergent additives in order to prevent the build-up of deposits (typically, on fuel injector and intake valve) known to reduce fuel economy and engine performance.\n\n\n=== In microgravity ===\nHow fuel combusts affects how much energy is produced. The National Aeronautics and Space Administration (NASA) has investigated fuel consumption in microgravity.\nThe common distribution of a flame under normal gravity conditions depends on convection, because soot tends to rise to the top of a flame, such as in a candle, making the flame yellow. In microgravity or zero gravity, such as an environment in outer space, convection no longer occurs, and the flame becomes spherical, with a tendency to become more blue and more efficient. There are several possible explanations for this difference, of which the most likely one given is the hypothesis that the temperature is evenly distributed enough that soot is not formed and complete combustion occurs., National Aeronautics and Space Administration, April 2005. Experiments by NASA in microgravity reveal that diffusion flames in microgravity allow more soot to be completely oxidised after they are produced than diffusion flames on Earth, because of a series of mechanisms that behaved differently in microgravity when compared to normal gravity conditions.LSP-1 experiment results, National Aeronautics and Space Administration, April 2005. Premixed flames in microgravity burn at a much slower rate and more efficiently than even a candle on Earth, and last much longer.\n\n\n== See also ==\n\n\n== References ==\n\n\n== External links ==\n\nUS Government website on fuel economy\nUK DfT comparisons on road and rail\nNASA Offers a $1.5 Million Prize for a Fast and Fuel-Efficient Aircraft Archived 2016-03-03 at the Wayback Machine\nCar Fuel Consumption Official Figures\nSpritmonitor.de \"the most fuel efficient cars\" - Database of thousands of (mostly German) car owners' actual fuel consumption figures (cf. Spritmonitor)\nSearchable fuel economy data from the EPA - United States Environmental Protection Agency\npenghemat bbm - Alat penghemat bbm\nNy Times: A Road Test of Alternative Fuel Visions",
        "unit": "fuel consumption",
        "url": "https://en.wikipedia.org/wiki/Fuel_efficiency"
    },
    {
        "_id": "Minute",
        "clean": "Minute",
        "text": "Minute is a unit of time defined as equal to 60 seconds.\nOne hour contains 60 minutes.\nAlthough not a unit in the International System of Units (SI), the minute is accepted for use in the SI. The SI symbol for minutes is min (without a dot). The prime symbol \u2032 is also sometimes used informally to denote minutes.\nIn the UTC time standard, a minute on rare occasions has 61 seconds, a consequence of leap seconds; there is also a provision to insert a negative leap second, which would result in a 59-second minute, but this has never happened in more than 40 years under this system.\n\n\n== History ==\nAl-Biruni first subdivided the hour sexagesimally into minutes, seconds, thirds and fourths in 1000 CE while discussing Jewish months.\nHistorically, the word \"minute\" comes from the Latin pars minuta prima, meaning \"first small part\".  This division of the hour can be further refined with a \"second small part\" (Latin: pars minuta secunda), and this is where the word \"second\" comes from.  For even further refinement, the term \"third\" (1\u204460 of a second) remains in some languages, for example Polish (tercja) and Turkish (salise), although most modern usage subdivides seconds by using decimals.  The symbol notation of the prime for minutes and double prime for seconds can be seen as indicating the first and second cut of the hour (similar to how the foot is the first cut of the yard or perhaps chain, with inches as the second cut).  In 1267, the medieval scientist Roger Bacon, writing in Latin, defined the division of time between full moons as a number of hours, minutes, seconds, thirds, and fourths (horae, minuta, secunda, tertia, and quarta) after noon on specified calendar dates. The introduction of the minute hand into watches was possible only after the invention of the hairspring by Thomas Tompion, an English watchmaker, in 1675.\n\n\n== See also ==\nClock face\nInternational System of Units\nLatitude and longitude\nOrders of magnitude (time)\n\n\n== Notes and references ==\n\n\n== Bibliography ==\nHenry Campbell Black, Black's Law Dictionary, 6th Edition, entry on Minute. West Publishing Company, St. Paul, Minnesota, 1991.\nEric W. Weisstein. \"Arc Minute.\" From MathWorld \u2013 A Wolfram",
        "unit": "minute",
        "url": "https://en.wikipedia.org/wiki/Minute"
    },
    {
        "_id": "Pint",
        "clean": "Pint",
        "text": "The pint (, ; symbol pt, sometimes abbreviated as p) is a unit of volume or capacity in both the imperial and United States customary measurement systems. In both of those systems it is traditionally one eighth of a gallon. The British imperial pint is about 20% larger than the American pint because the two systems are defined differently. Almost all other countries have standardized on the metric system, so although some of them still also have traditional units called pints (such as for beverages), the volume varies by regional custom.\nThe imperial pint (\u2248\u2009568 mL) is used in the United Kingdom and Ireland and to a limited extent in Commonwealth nations. In the United States, two kinds of pint are used: a liquid pint (\u2248\u2009473 mL) and a less common dry pint (\u2248\u2009551 mL). Other former British colonies, such as Australia, South Africa and New Zealand, converted to the metric system in the 1960s and 1970s; so while the term pint may still be in common use in these countries, it may no longer refer to the British imperial pint once used throughout the British Empire.\n\n\n== Name ==\n\nPint comes from the Old French word pinte and perhaps ultimately from Vulgar Latin pincta meaning \"painted\", for marks painted on the side of a container to show capacity. It is linguistically related, though greatly diverging in meaning, to Pinto \u2013 an Italian, Spanish, and Portuguese name for a person with a speckled or dark complexion, often used as a surname in these languages.\nIn France, the French word pinte is now used to describe a half-litre, slightly smaller than an Imperial pint, but in Quebec it is used to describe an Imperial quart and the French word chopine is used for an Imperial pint.\n\n\n== Definitions ==\n\n\n=== Imperial pint ===\nThe imperial pint is equal to one eighth of an imperial gallon. \n\n\n=== US liquid pint ===\nIn the United States, the liquid pint is legally defined as one eighth of a liquid gallon of precisely 231 cubic inches.\n\n\n=== US dry pint ===\nIn the United States, the dry pint is one sixty-fourth of a bushel.\n\n\n=== Other pints ===\n\nThe United States dry pint is equal to one eighth of a United States dry gallon. It is used in the United States, but is not as common as the liquid pint.\nA now-obsolete unit of measurement in Scotland, known as the Scottish pint, or joug, is equal to 1696 mL (2 pints 19.69 imp fl oz). It remained in use until the 19th century, surviving significantly longer than most of the old Scottish measurements.\nThe word pint is one of numerous false friends between English and French. They are not the same unit although they have the same linguistic origin. The French word pinte is etymologically related, but historically described a larger unit. The Royal pint (pinte du roi) was 48 French cubic inches (952.1 mL), but regional pints varied in size depending on locality and on commodity (usually wine or olive oil) varying from 0.95 L to over 2 L.\nIn Canada, the Weights and Measures Act (R.S. 1985) defines a pint in English as one eighth of a gallon, but defines a pinte in French as one quarter of a gallon. Thus, if \"a pint of beer\" is ordered in English, servers are legally required to serve an imperial pint (568 mL) of beer, but under the federal Act, \"une pinte de bi\u00e8re\" legally refers to the larger imperial quart (1136 mL), while an imperial pint is designated as une chopine. However, in practice and according to Quebec\u2019s Board of the French Language, une pinte commonly refers to the same 568 mL imperial pint as in English.\nIn Flanders, the word pintje, meaning 'little pint', refers only to a 250 mL glass of lager. Some West- and East-Flemish dialects use it as a word for beaker. The equivalent word in German, Pintchen, refers to a glass of a third of a litre in Cologne and the Rhineland.\nIn South Australia, ordering \"a pint of beer\" results in 425 mL (15 fl oz) being served. Customers must specifically request \"an Imperial pint of beer\" to get 570 mL (20 fl oz). Australians from other states often contest the size of their beers in Adelaide.\n\n\n== Equivalence ==\nOne US liquid pint of water weighs 1.04318 pounds (16.6909 oz), which gives rise to a popular saying: \"A pint's a pound the world around\".\nHowever, the statement does not hold around the world because the British imperial pint, which was also the standard measure in Australia, India, Malaya, New Zealand, South Africa and other former British colonies, weighs 1.2528 pounds (20.0448 oz). This prompted the Society for the Diffusion of Useful Knowledge to coin a saying for use in Commonwealth countries: \"a pint of pure water weighs a pound and a quarter\".\n\n\n== History ==\nThe pint is traditionally one eighth of a gallon. In the Latin of the apothecaries' system, the symbol O (octarius or octavius; plural octarii or octavii \u2013 reflecting the \"eighth\" concept in its octa- syllable) was used for the pint. Because of the variety of definitions of a gallon, there have been equally many versions of the pint.\nBritain's North American colonies adopted the British wine gallon, defined in 1707 as 231 cubic inches exactly (3 in \u00d7 7 in \u00d7 11 in) as their basic liquid measure, from which the US wet pint is derived; and the British corn gallon (1\u20448 of a standard \"Winchester\" bushel of corn, or 268.8 cubic inches) as its dry measure, from which the US dry pint is derived.\nIn 1824, the British parliament replaced all the various gallons with a new imperial gallon based on ten pounds of distilled water at 62 \u00b0F (16.667 \u00b0C) (277.42 cubic inches), from which the current UK pint is derived.\nThe various Canadian provinces continued to use the Queen Anne Winchester wine gallon as a basis for their pint until 1873, well after Britain adopted the imperial system in 1824. This made the Canadian pint compatible with the American pint, but after 1824 it was incompatible with the British pint. The traditional French pinte used in Lower Canada (Quebec) was twice the size of the traditional English \"pint\" used in Upper Canada (Ontario). After four of the British provinces united in the Canadian Confederation in 1867, Canada legally adopted the British imperial system of measure in 1873, making Canadian liquid units incompatible with American ones from that year forward. In 1873, the French Canadian pinte was defined as being one imperial quart or two imperial pints, while the imperial pint was legally called a chopine in French Canada. Canadian imperial units of liquid measure remain incompatible with American traditional units to this day, and although the Canadian pint, quart, and gallon are still legal units of measure in Canada, they are still 20% larger than the American ones.\nHistorically, units called a pint (or the equivalent in the local language) were used across much of Europe, with values varying between countries from less than half a litre to over one litre. Within continental Europe, these pints were replaced with liquid measures based on the metric system during the 19th century. The term is still in limited use in parts of France and Central Europe, notably some areas of Germany and Switzerland, where ein Schoppen is colloquially used for roughly half a litre. In Spanish holiday resorts frequented by British tourists, 'pint' is often taken to mean a beer glass (especially a dimple mug). Half-pint 285 mL, and pint mugs, 570 mL,  may therefore be referred to as media jarra ('half jar/jug') and jarra (grande) ('large jar/jug').\n\n\n=== Effects of metrication ===\n\nIn the British and Irish metrication processes, the pint was replaced by metric units as the legally defined primary unit of measure for trading by volume or capacity, except for the sale of draught beer and cider, and milk in returnable containers. As a supplementary unit, the pint can still be used in those countries in all circumstances. UK legislation mandates that draught beer and cider must be sold in a third of a pint, two thirds of a pint or multiples of half a pint, which must be served in stamped, measured glasses or from government-stamped meters. Milk, in returnable containers may come in pints without the metric equivalent stated. However all other goods apart from the aforementioned exceptions must be sold or labelled in metric units. Milk in plastic containers mostly comes multiples of 1 pint sizes, but are required to display the metric equivalent on packaging. Filtered milk, and UHT Milk sold in the UK is commonly sold in multiples of 1 litre bottles or containers. \n Recipes published in the UK and Ireland would have given ingredient quantities in imperial, where the pint is used as a unit for larger liquid quantities, as well as the metric measure - though recipes written now are more likely to use metric units.\nIn Australia and New Zealand, a subtle change was made to 1 pint milk bottles during the conversion from imperial to metric in the 1970s. The height and diameter of the milk bottle remained unchanged, so that existing equipment for handling and storing the bottles was unaffected, but the shape was adjusted to increase the capacity from 568 mL to 600 mL\u2014a conveniently rounded metric measure. Such milk bottles are no longer officially referred to as pints. However, the \"pint glass\" in pubs in Australia remains closer to the standard imperial pint, at 570 mL. It holds about 500 mL of beer and about 70 mL of froth, except in South Australia, where a pint is served in a 425 mL glass and a 570 mL glass is called an \"imperial pint\". In New Zealand, there is no longer any legal requirement for beer to be served in standard measures: in pubs, the largest size of glass, which is referred to as a pint, varies, but usually contains 425 mL.\nAfter metrication in Canada, milk and other liquids in pre-packaged containers came in metric sizes so conversion issues could no longer arise. Draft beer in Canada, when advertised as a \"pint\", is legally required to be an imperial pint (568 mL). With the allowed margin of error of 0.5 fluid ounces, a \"pint\" that is less than 554 mL of beer is an offence, though this regulation is often violated and rarely enforced. To avoid legal issues, many drinking establishments are moving away from using the term \"pint\" and are selling \"glasses\" or \"sleeves\" of beer, neither of which have a legal definition.\nA 375 mL bottle of liquor in the US and the Canadian maritime provinces is sometimes referred to as a \"pint\" and a 200 mL bottle is called a \"half-pint\", harking back to the days when liquor came in US pints, fifths, quarts, and half-gallons. Liquor in the US has been sold in metric-sized bottles since 1980 although beer is still sold in US traditional units.\nIn France, a standard 250 mL measure of beer is known as un demi (\"a half\"), originally meaning a half-pint.\n\n\n== Notes ==\n\n\n== References ==\n\n\n== External links ==\n\nEuropean Commission press release (IP/07/1297, 11 September 2007): Pints and miles will not disappear due to European Commission proposal",
        "unit": "pint",
        "url": "https://en.wikipedia.org/wiki/Pint"
    },
    {
        "_id": "Radioactive_decay",
        "clean": "Radioactive decay",
        "text": "Radioactive decay (also known as nuclear decay, radioactivity, radioactive disintegration, or nuclear disintegration) is the process by which an unstable atomic nucleus loses energy by radiation. A material containing unstable nuclei is considered radioactive. Three of the most common types of decay are alpha, beta, and gamma decay. The weak force is the mechanism that is responsible for beta decay, while the other two are governed by the electromagnetic and nuclear forces.\nRadioactive decay is a random process at the level of single atoms. According to quantum theory, it is impossible to predict when a particular atom will decay, regardless of how long the atom has existed. However, for a significant number of identical atoms, the overall decay rate can be expressed as a decay constant or as a half-life. The half-lives of radioactive atoms have a huge range: from nearly instantaneous to far longer than the age of the universe.\nThe decaying nucleus is called the parent radionuclide (or parent radioisotope), and the process produces at least one daughter nuclide. Except for gamma decay or internal conversion from a nuclear excited state, the decay is a nuclear transmutation resulting in a daughter containing a different number of protons or neutrons (or both). When the number of protons changes, an atom of a different chemical element is created.\nThere are 28 naturally occurring chemical elements on Earth that are radioactive, consisting of 35 radionuclides (seven elements have two different radionuclides each) that date before the time of formation of the Solar System. These 35 are known as primordial radionuclides. Well-known examples are uranium and thorium, but also included are naturally occurring long-lived radioisotopes, such as potassium-40. Each of the heavy primordial radionuclides participates in one of the four decay chains.\n\n\n== History of discovery ==\n\nRadioactivity was discovered in 1896 by scientists Henri Becquerel and Marie Curie, while working with phosphorescent materials. These materials glow in the dark after exposure to light, and Becquerel suspected that the glow produced in cathode-ray tubes by X-rays might be associated with phosphorescence. He wrapped a photographic plate in black paper and placed various phosphorescent salts on it. All results were negative until he used uranium salts. The uranium salts caused a blackening of the plate in spite of the plate being wrapped in black paper. These radiations were given the name \"Becquerel Rays\".\nIt soon became clear that the blackening of the plate had nothing to do with phosphorescence, as the blackening was also produced by non-phosphorescent salts of uranium and by metallic uranium. It became clear from these experiments that there was a form of invisible radiation that could pass through paper and was causing the plate to react as if exposed to light.\nAt first, it seemed as though the new radiation was similar to the then recently discovered X-rays. Further research by Becquerel, Ernest Rutherford, Paul Villard, Pierre Curie, Marie Curie, and others showed that this form of radioactivity was significantly more complicated. Rutherford was the first to realize that all such elements decay in accordance with the same mathematical exponential formula. Rutherford and his student Frederick Soddy were the first to realize that many decay processes resulted in the transmutation of one element to another. Subsequently, the radioactive displacement law of Fajans and Soddy was formulated to describe the products of alpha and beta decay.\nThe early researchers also discovered that many other chemical elements, besides uranium, have radioactive isotopes. A systematic search for the total radioactivity in uranium ores also guided Pierre and Marie Curie to isolate two new elements: polonium and radium. Except for the radioactivity of radium, the chemical similarity of radium to barium made these two elements difficult to distinguish.\nMarie and Pierre Curie's study of radioactivity is an important factor in science and medicine. After their research on Becquerel's rays led them to the discovery of both radium and polonium, they coined the term \"radioactivity\" to define the emission of ionizing radiation by some heavy elements. (Later the term was generalized to all elements.) Their research on the penetrating rays in uranium and the discovery of radium launched an era of using radium for the treatment of cancer. Their exploration of radium could be seen as the first peaceful use of nuclear energy and the start of modern nuclear medicine.\n\n\n== Early health dangers ==\n\nThe dangers of ionizing radiation due to radioactivity and X-rays were not immediately recognized.\n\n\n=== X-rays ===\nThe discovery of X\u2011rays by Wilhelm R\u00f6ntgen in 1895 led to widespread experimentation by scientists, physicians, and inventors. Many people began recounting stories of burns, hair loss and worse in technical journals as early as 1896. In February of that year, Professor Daniel and Dr. Dudley of Vanderbilt University performed an experiment involving X-raying Dudley's head that resulted in his hair loss. A report by Dr. H.D. Hawks, of his suffering severe hand and chest burns in an X-ray demonstration, was the first of many other reports in Electrical Review.\nOther experimenters, including Elihu Thomson and Nikola Tesla, also reported burns. Thomson deliberately exposed a finger to an X-ray tube over a period of time and suffered pain, swelling, and blistering. Other effects, including ultraviolet rays and ozone, were sometimes blamed for the damage, and many physicians still claimed that there were no effects from X-ray exposure at all.\nDespite this, there were some early systematic hazard investigations, and as early as 1902 William Herbert Rollins wrote almost despairingly that his warnings about the dangers involved in the careless use of X-rays were not being heeded, either by industry or by his colleagues. By this time, Rollins had proved that X-rays could kill experimental animals, could cause a pregnant guinea pig to abort, and that they could kill a foetus. He also stressed that \"animals vary in susceptibility to the external action of X-light\" and warned that these differences be considered when patients were treated by means of X-rays.\n\n\n=== Radioactive substances ===\n\nHowever, the biological effects of radiation due to radioactive substances were less easy to gauge. This gave the opportunity for many physicians and corporations to market radioactive substances as patent medicines. Examples were radium enema treatments, and radium-containing waters to be drunk as tonics. Marie Curie protested against this sort of treatment, warning that \"radium is dangerous in untrained hands\". Curie later died from aplastic anaemia, likely caused by exposure to ionizing radiation. By the 1930s, after a number of cases of bone necrosis and death of radium treatment enthusiasts, radium-containing medicinal products had been largely removed from the market (radioactive quackery).\n\n\n=== Radiation protection ===\n\nOnly a year after R\u00f6ntgen's discovery of X-rays, the American engineer Wolfram Fuchs (1896) gave what is probably the first protection advice, but it was not until 1925 that the first International Congress of Radiology (ICR) was held and considered establishing international protection standards. The effects of radiation on genes, including the effect of cancer risk, were recognized much later. In 1927, Hermann Joseph Muller published research showing genetic effects and, in 1946, was awarded the Nobel Prize in Physiology or Medicine for his findings.\nThe second ICR was held in Stockholm in 1928 and proposed the adoption of the r\u00f6ntgen unit, and the International X-ray and Radium Protection Committee (IXRPC) was formed. Rolf Sievert was named chairman, but a driving force was George Kaye of the British National Physical Laboratory. The committee met in 1931, 1934, and 1937.\nAfter World War II, the increased range and quantity of radioactive substances being handled as a result of military and civil nuclear programs led to large groups of occupational workers and the public being potentially exposed to harmful levels of ionising radiation. This was considered at the first post-war ICR convened in London in 1950, when the present International Commission on Radiological Protection (ICRP) was born.\nSince then the ICRP has developed the present international system of radiation protection, covering all aspects of radiation hazards.\nIn 2020, Hauptmann and another 15 international researchers from eight nations (among them: Institutes of Biostatistics, Registry Research, Centers of Cancer Epidemiology, Radiation Epidemiology, and also the U.S. National Cancer Institute (NCI), International Agency for Research on Cancer (IARC) and the Radiation Effects Research Foundation of Hiroshima) studied definitively through meta-analysis the damage resulting from the \"low doses\" that have afflicted survivors of the atomic bombings of Hiroshima and Nagasaki and also in numerous accidents at nuclear plants that have occurred. These scientists reported, in JNCI Monographs: Epidemiological Studies of Low Dose Ionizing Radiation and Cancer Risk, that the new epidemiological studies directly support excess cancer risks from low-dose ionizing radiation. In 2021, Italian researcher Sebastiano Venturi reported the first correlations between radio-caesium and pancreatic cancer with the role of caesium in biology, in pancreatitis and in diabetes of pancreatic origin.\n\n\n== Units ==\n\nThe International System of Units (SI) unit of radioactive activity is the becquerel (Bq), named in honor of the scientist Henri Becquerel. One Bq is defined as one transformation (or decay or disintegration) per second.\nAn older unit of radioactivity is the curie, Ci, which was originally defined as \"the quantity or mass of radium emanation in equilibrium with one gram of radium (element)\". Today, the curie is defined as 3.7\u00d71010 disintegrations per second, so that 1 curie (Ci) = 3.7\u00d71010 Bq.\nFor radiological protection purposes, although the United States Nuclear Regulatory Commission permits the use of the unit curie alongside SI units, the European Union European units of measurement directives required that its use for \"public health ... purposes\" be phased out by 31 December 1985.\nThe effects of ionizing radiation are often measured in units of gray for mechanical or sievert for damage to tissue.\n\n\n== Types ==\n\nRadioactive decay results in a reduction of summed rest mass, once the released energy (the disintegration energy) has escaped in some way. Although decay energy is sometimes defined as associated with the difference between the mass of the parent nuclide products and the mass of the decay products, this is true only of rest mass measurements, where some energy has been removed from the product system. This is true because the decay energy must always carry mass with it, wherever it appears (see mass in special relativity) according to the formula E = mc2. The decay energy is initially released as the energy of emitted photons plus the kinetic energy of massive emitted particles (that is, particles that have rest mass). If these particles come to thermal equilibrium with their surroundings and photons are absorbed, then the decay energy is transformed to thermal energy, which retains its mass.\nDecay energy, therefore, remains associated with a certain measure of the mass of the decay system, called invariant mass, which does not change during the decay, even though the energy of decay is distributed among decay particles. The energy of photons, the kinetic energy of emitted particles, and, later, the thermal energy of the surrounding matter, all contribute to the invariant mass of the system. Thus, while the sum of the rest masses of the particles is not conserved in radioactive decay, the system mass and system invariant mass (and also the system total energy) is conserved throughout any decay process. This is a restatement of the equivalent laws of conservation of energy and conservation of mass.\n\n\n=== Alpha, beta and gamma decay ===\n\nEarly researchers found that an electric or magnetic field could split radioactive emissions into three types of beams. The rays were given the names alpha, beta, and gamma, in increasing order of their ability to penetrate matter. Alpha decay is observed only in heavier elements of atomic number 52 (tellurium) and greater, with the exception of beryllium-8 (which decays to two alpha particles). The other two types of decay are observed in all the elements. Lead, atomic number 82, is the heaviest element to have any isotopes stable (to the limit of measurement) to radioactive decay. Radioactive decay is seen in all isotopes of all elements of atomic number 83 (bismuth) or greater. Bismuth-209, however, is only very slightly radioactive, with a half-life greater than the age of the universe; radioisotopes with extremely long half-lives are considered effectively stable for practical purposes.\nIn analyzing the nature of the decay products, it was obvious from the direction of the electromagnetic forces applied to the radiations by external magnetic and electric fields that alpha particles carried a positive charge, beta particles carried a negative charge, and gamma rays were neutral. From the magnitude of deflection, it was clear that alpha particles were much more massive than beta particles. Passing alpha particles through a very thin glass window and trapping them in a discharge tube allowed researchers to study the emission spectrum of the captured particles, and ultimately proved that alpha particles are helium nuclei. Other experiments showed beta radiation, resulting from decay and cathode rays, were high-speed electrons. Likewise, gamma radiation and X-rays were found to be high-energy electromagnetic radiation.\nThe relationship between the types of decays also began to be examined: For example, gamma decay was almost always found to be associated with other types of decay, and occurred at about the same time, or afterwards. Gamma decay as a separate phenomenon, with its own half-life (now termed isomeric transition), was found in natural radioactivity to be a result of the gamma decay of excited metastable nuclear isomers, which were in turn created from other types of decay. Although alpha, beta, and gamma radiations were most commonly found, other types of emission were eventually discovered. Shortly after the discovery of the positron in cosmic ray products, it was realized that the same process that operates in classical beta decay can also produce positrons (positron emission), along with neutrinos (classical beta decay produces antineutrinos).\n\n\n=== Electron capture ===\n\nIn electron capture, some proton-rich nuclides were found to capture their own atomic electrons instead of emitting positrons, and subsequently, these nuclides emit only a neutrino and a gamma ray from the excited nucleus (and often also Auger electrons and characteristic X-rays, as a result of the re-ordering of electrons to fill the place of the missing captured electron). These types of decay involve the nuclear capture of electrons or emission of electrons or positrons, and thus acts to move a nucleus toward the ratio of neutrons to protons that has the least energy for a given total number of nucleons. This consequently produces a more stable (lower energy) nucleus.\nA hypothetical process of positron capture, analogous to electron capture, is theoretically possible in antimatter atoms, but has not been observed, as complex antimatter atoms beyond antihelium are not experimentally available. Such a decay would require antimatter atoms at least as complex as beryllium-7, which is the lightest known isotope of normal matter to undergo decay by electron capture.\n\n\n=== Nucleon emission ===\n\nShortly after the discovery of the neutron in 1932, Enrico Fermi realized that certain rare beta-decay reactions immediately yield neutrons as an additional decay particle, so called beta-delayed neutron emission. Neutron emission usually happens from nuclei that are in an excited state, such as the excited 17O* produced from the beta decay of 17N. The neutron emission process itself is controlled by the nuclear force and therefore is extremely fast, sometimes referred to as \"nearly instantaneous\". Isolated proton emission was eventually observed in some elements. It was also found that some heavy elements may undergo spontaneous fission into products that vary in composition. In a phenomenon called cluster decay, specific combinations of neutrons and protons other than alpha particles (helium nuclei) were found to be spontaneously emitted from atoms. \n\n\n=== More exotic types of decay ===\nOther types of radioactive decay were found to emit previously seen particles but via different mechanisms. An example is internal conversion, which results in an initial electron emission, and then often further characteristic X-rays and Auger electrons emissions, although the internal conversion process involves neither beta nor gamma decay. A neutrino is not emitted, and none of the electron(s) and photon(s) emitted originate in the nucleus, even though the energy to emit all of them does originate there. Internal conversion decay, like isomeric transition gamma decay and neutron emission, involves the release of energy by an excited nuclide, without the transmutation of one element into another.\nRare events that involve a combination of two beta-decay-type events happening simultaneously are known (see below). Any decay process that does not violate the conservation of energy or momentum laws (and perhaps other particle conservation laws) is permitted to happen, although not all have been detected. An interesting example discussed in a final section, is bound state beta decay of rhenium-187. In this process, the beta electron-decay of the parent nuclide is not accompanied by beta electron emission, because the beta particle has been captured into the K-shell of the emitting atom. An antineutrino is emitted, as in all negative beta decays.\nIf energy circumstances are favorable, a given radionuclide may undergo many competing types of decay, with some atoms decaying by one route, and others decaying by another. An example is copper-64, which has 29 protons, and 35 neutrons, which decays with a half-life of 12.7004(13) hours. This isotope has one unpaired proton and one unpaired neutron, so either the proton or the neutron can decay to the other particle, which has opposite isospin. This particular nuclide (though not all nuclides in this situation) is more likely to decay through beta plus decay (61.52(26)%) than through electron capture (38.48(26)%). The excited energy states resulting from these decays which fail to end in a ground energy state, also produce later internal conversion and gamma decay in almost 0.5% of the time.\n\n\n=== List of decay modes ===\n\n\n=== Decay chains and multiple modes ===\n\nThe daughter nuclide of a decay event may also be unstable (radioactive). In this case, it too will decay, producing radiation. The resulting second daughter nuclide may also be radioactive. This can lead to a sequence of several decay events called a decay chain (see this article for specific details of important natural decay chains). Eventually, a stable nuclide is produced. Any decay daughters that are the result of an alpha decay will also result in helium atoms being created.\nSome radionuclides may have several different paths of decay. For example, 35.94(6)% of bismuth-212 decays, through alpha-emission, to thallium-208 while 64.06(6)% of bismuth-212 decays, through beta-emission, to polonium-212. Both thallium-208 and polonium-212 are radioactive daughter products of bismuth-212, and both decay directly to stable lead-208.\n\n\n== Occurrence and applications ==\n\nAccording to the Big Bang theory, stable isotopes of the lightest three elements (H, He, and traces of Li) were produced very shortly after the emergence of the universe, in a process called Big Bang nucleosynthesis. These lightest stable nuclides (including deuterium) survive to today, but any radioactive isotopes of the light elements produced in the Big Bang (such as tritium) have long since decayed. Isotopes of elements heavier than boron were not produced at all in the Big Bang, and these first five elements do not have any long-lived radioisotopes. Thus, all radioactive nuclei are, therefore, relatively young with respect to the birth of the universe, having formed later in various other types of nucleosynthesis in stars (in particular, supernovae), and also during ongoing interactions between stable isotopes and energetic particles. For example, carbon-14, a radioactive nuclide with a half-life of only 5700(30) years, is constantly produced in Earth's upper atmosphere due to interactions between cosmic rays and nitrogen.\nNuclides that are produced by radioactive decay are called radiogenic nuclides, whether they themselves are stable or not. There exist stable radiogenic nuclides that were formed from short-lived extinct radionuclides in the early Solar System. The extra presence of these stable radiogenic nuclides (such as xenon-129 from extinct iodine-129) against the background of primordial stable nuclides can be inferred by various means.\nRadioactive decay has been put to use in the technique of radioisotopic labeling, which is used to track the passage of a chemical substance through a complex system (such as a living organism). A sample of the substance is synthesized with a high concentration of unstable atoms. The presence of the substance in one or another part of the system is determined by detecting the locations of decay events.\nOn the premise that radioactive decay is truly random (rather than merely chaotic), it has been used in hardware random-number generators. Because the process is not thought to vary significantly in mechanism over time, it is also a valuable tool in estimating the absolute ages of certain materials. For geological materials, the radioisotopes and some of their decay products become trapped when a rock solidifies, and can then later be used (subject to many well-known qualifications) to estimate the date of the solidification. These include checking the results of several simultaneous processes and their products against each other, within the same sample. In a similar fashion, and also subject to qualification, the rate of formation of carbon-14 in various eras, the date of formation of organic matter within a certain period related to the isotope's half-life may be estimated, because the carbon-14 becomes trapped when the organic matter grows and incorporates the new carbon-14 from the air. Thereafter, the amount of carbon-14 in organic matter decreases according to decay processes that may also be independently cross-checked by other means (such as checking the carbon-14 in individual tree rings, for example).\n\n\n=== Szilard\u2013Chalmers effect ===\nThe Szilard\u2013Chalmers effect is the breaking of a chemical bond as a result of a kinetic energy imparted from radioactive decay.  It operates by the absorption of neutrons by an atom and subsequent emission of gamma rays, often with significant amounts of kinetic energy. This kinetic energy, by Newton's third law, pushes back on the decaying atom, which causes it to move with enough speed to break a chemical bond. This effect can be used to separate isotopes by chemical means.\nThe Szilard\u2013Chalmers effect was discovered in 1934 by Le\u00f3 Szil\u00e1rd and Thomas A. Chalmers. They observed that after bombardment by neutrons,  the breaking of a bond in liquid ethyl iodide allowed radioactive iodine to be removed.\n\n\n=== Origins of radioactive nuclides ===\n\nRadioactive primordial nuclides found in the Earth are residues from ancient supernova explosions that occurred before the formation of the Solar System. They are the fraction of radionuclides that survived from that time, through the formation of the primordial solar nebula, through planet accretion, and up to the present time. The naturally occurring short-lived radiogenic radionuclides found in today's rocks, are the daughters of those radioactive primordial nuclides. Another minor source of naturally occurring radioactive nuclides are cosmogenic nuclides, that are formed by cosmic ray bombardment of material in the Earth's atmosphere or crust. The decay of the radionuclides in rocks of the Earth's mantle and crust contribute significantly to Earth's internal heat budget.\n\n\n== Aggregate processes ==\nWhile the underlying process of radioactive decay is subatomic, historically and in most practical cases it is encountered in bulk materials with very large numbers of atoms. This section discusses models that connect events at the atomic level to observations in aggregate.\n\n\n=== Terminology ===\nThe decay rate, or activity, of a radioactive substance is characterized by the following time-independent parameters:\n\nThe half-life, t1/2, is the time taken for the activity of a given amount of a radioactive substance to decay to half of its initial value.\nThe decay constant, \u03bb \"lambda\", the reciprocal of the mean lifetime (in s\u22121), sometimes referred to as simply decay rate.\nThe mean lifetime, \u03c4 \"tau\", the average lifetime (1/e life) of a radioactive particle before decay.\nAlthough these are constants, they are associated with the statistical behavior of populations of atoms. In consequence, predictions using these constants are less accurate for minuscule samples of atoms.\nIn principle a half-life, a third-life, or even a (1/\u221a2)-life, could be used in exactly the same way as half-life; but the mean life and half-life t1/2 have been adopted as standard times associated with exponential decay.\nThose parameters can be related to the following time-dependent parameters:\n\nTotal activity (or just activity), A, is the number of decays per unit time of a radioactive sample.\nNumber of particles, N, in the sample.\nSpecific activity, a, is the number of decays per unit time per amount of substance of the sample at time set to zero (t = 0). \"Amount of substance\" can be the mass, volume or moles of the initial sample.\nThese are related as follows:\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  t\n                  \n                    1\n                    \n                      /\n                    \n                    2\n                  \n                \n              \n              \n                \n                =\n                \n                  \n                    \n                      ln\n                      \u2061\n                      (\n                      2\n                      )\n                    \n                    \u03bb\n                  \n                \n                =\n                \u03c4\n                ln\n                \u2061\n                (\n                2\n                )\n              \n            \n            \n              \n                A\n              \n              \n                \n                =\n                \u2212\n                \n                  \n                    \n                      \n                        d\n                      \n                      N\n                    \n                    \n                      \n                        d\n                      \n                      t\n                    \n                  \n                \n                =\n                \u03bb\n                N\n                =\n                \n                  \n                    \n                      ln\n                      \u2061\n                      (\n                      2\n                      )\n                    \n                    \n                      t\n                      \n                        1\n                        \n                          /\n                        \n                        2\n                      \n                    \n                  \n                \n                N\n              \n            \n            \n              \n                \n                  S\n                  \n                    A\n                  \n                \n                \n                  a\n                  \n                    0\n                  \n                \n              \n              \n                \n                =\n                \u2212\n                \n                  \n                    \n                      \n                        d\n                      \n                      N\n                    \n                    \n                      \n                        d\n                      \n                      t\n                    \n                  \n                \n                \n                  \n                    \n                      |\n                    \n                  \n                  \n                    t\n                    =\n                    0\n                  \n                \n                =\n                \u03bb\n                \n                  N\n                  \n                    0\n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}t_{1/2}&={\\frac {\\ln(2)}{\\lambda }}=\\tau \\ln(2)\\\\[2pt]A&=-{\\frac {\\mathrm {d} N}{\\mathrm {d} t}}=\\lambda N={\\frac {\\ln(2)}{t_{1/2}}}N\\\\[2pt]S_{A}a_{0}&=-{\\frac {\\mathrm {d} N}{\\mathrm {d} t}}{\\bigg |}_{t=0}=\\lambda N_{0}\\end{aligned}}}\n  \n\nwhere N0 is the initial amount of active substance \u2014 substance that has the same percentage of unstable particles as when the substance was formed.\n\n\n=== Assumptions ===\nThe mathematics of radioactive decay depend on a key assumption that a nucleus of a radionuclide has no \"memory\" or way of translating its history into its present behavior. A nucleus does not \"age\" with the passage of time. Thus, the probability of its breaking down does not increase with time but stays constant, no matter how long the nucleus has existed. This constant probability may differ greatly between one type of nucleus and another, leading to the many different observed decay rates. However, whatever the probability is, it does not change over time. This is in marked contrast to complex objects that do show aging, such as automobiles and humans. These aging systems do have a chance of breakdown per unit of time that increases from the moment they begin their existence.\nAggregate processes, like the radioactive decay of a lump of atoms, for which the single-event probability of realization is very small but in which the number of time-slices is so large that there is nevertheless a reasonable rate of events, are modelled by the Poisson distribution, which is discrete. Radioactive decay and nuclear particle reactions are two examples of such aggregate processes. The mathematics of Poisson processes reduce to the law of exponential decay, which describes the statistical behaviour of a large number of nuclei, rather than one individual nucleus. In the following formalism, the number of nuclei or the nuclei population N, is of course a discrete variable (a natural number)\u2014but for any physical sample N is so large that it can be treated as a continuous variable. Differential calculus is used to model the behaviour of nuclear decay.\n\n\n==== One-decay process ====\n\nConsider the case of a nuclide A that decays into another B by some process A \u2192 B (emission of other particles, like electron neutrinos \u03bde and electrons e\u2212 as in beta decay, are irrelevant in what follows). The decay of an unstable nucleus is entirely random in time so it is impossible to predict when a particular atom will decay. However, it is equally likely to decay at any instant in time. Therefore, given a sample of a particular radioisotope, the number of decay events \u2212dN expected to occur in a small interval of time dt is proportional to the number of atoms present N, that is\n\n  \n    \n      \n        \u2212\n        \n          \n            \n              \n                d\n              \n              N\n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        \u221d\n        N\n      \n    \n    {\\displaystyle -{\\frac {\\mathrm {d} N}{\\mathrm {d} t}}\\propto N}\n  \n\nParticular radionuclides decay at different rates, so each has its own decay constant \u03bb. The expected decay \u2212dN/N is proportional to an increment of time, dt:\n\nThe negative sign indicates that N decreases as time increases, as the decay events follow one after another. The solution to this first-order differential equation is the function:\n\n  \n    \n      \n        N\n        (\n        t\n        )\n        =\n        \n          N\n          \n            0\n          \n        \n        \n        \n          e\n          \n            \u2212\n            \n              \u03bb\n            \n            t\n          \n        \n      \n    \n    {\\displaystyle N(t)=N_{0}\\,e^{-{\\lambda }t}}\n  \n\nwhere N0 is the value of N at time t = 0, with the decay constant expressed as \u03bb\nWe have for all time t:\n\n  \n    \n      \n        \n          N\n          \n            A\n          \n        \n        +\n        \n          N\n          \n            B\n          \n        \n        =\n        \n          N\n          \n            total\n          \n        \n        =\n        \n          N\n          \n            A\n            0\n          \n        \n        ,\n      \n    \n    {\\displaystyle N_{A}+N_{B}=N_{\\text{total}}=N_{A0},}\n  \n\nwhere Ntotal is the constant number of particles throughout the decay process, which is equal to the initial number of A nuclides since this is the initial substance.\nIf the number of non-decayed A nuclei is:\n\n  \n    \n      \n        \n          N\n          \n            A\n          \n        \n        =\n        \n          N\n          \n            A\n            0\n          \n        \n        \n          e\n          \n            \u2212\n            \u03bb\n            t\n          \n        \n      \n    \n    {\\displaystyle N_{A}=N_{A0}e^{-\\lambda t}}\n  \n\nthen the number of nuclei of B (i.e. the number of decayed A nuclei) is\n\n  \n    \n      \n        \n          N\n          \n            B\n          \n        \n        =\n        \n          N\n          \n            A\n            0\n          \n        \n        \u2212\n        \n          N\n          \n            A\n          \n        \n        =\n        \n          N\n          \n            A\n            0\n          \n        \n        \u2212\n        \n          N\n          \n            A\n            0\n          \n        \n        \n          e\n          \n            \u2212\n            \u03bb\n            t\n          \n        \n        =\n        \n          N\n          \n            A\n            0\n          \n        \n        \n          (\n          \n            1\n            \u2212\n            \n              e\n              \n                \u2212\n                \u03bb\n                t\n              \n            \n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle N_{B}=N_{A0}-N_{A}=N_{A0}-N_{A0}e^{-\\lambda t}=N_{A0}\\left(1-e^{-\\lambda t}\\right).}\n  \n\nThe number of decays observed over a given interval obeys Poisson statistics. If the average number of decays is \u27e8N\u27e9, the probability of a given number of decays N is\n\n  \n    \n      \n        P\n        (\n        N\n        )\n        =\n        \n          \n            \n              \u27e8\n              N\n              \n                \u27e9\n                \n                  N\n                \n              \n              exp\n              \u2061\n              (\n              \u2212\n              \u27e8\n              N\n              \u27e9\n              )\n            \n            \n              N\n              !\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle P(N)={\\frac {\\langle N\\rangle ^{N}\\exp(-\\langle N\\rangle )}{N!}}.}\n  \n\n\n==== Chain-decay processes ====\n\n\n===== Chain of two decays =====\nNow consider the case of a chain of two decays: one nuclide A decaying into another B by one process, then B decaying into another C by a second process, i.e. A \u2192 B \u2192 C. The previous equation cannot be applied to the decay chain, but can be generalized as follows. Since A decays into B, then B decays into C, the activity of A adds to the total number of B nuclides in the present sample, before those B nuclides decay and reduce the number of nuclides leading to the later sample. In other words, the number of second generation nuclei B increases as a result of the first generation nuclei decay of A, and decreases as a result of its own decay into the third generation nuclei C. The sum of these two terms gives the law for a decay chain for two nuclides:\n\n  \n    \n      \n        \n          \n            \n              \n                d\n              \n              \n                N\n                \n                  B\n                \n              \n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        =\n        \u2212\n        \n          \u03bb\n          \n            B\n          \n        \n        \n          N\n          \n            B\n          \n        \n        +\n        \n          \u03bb\n          \n            A\n          \n        \n        \n          N\n          \n            A\n          \n        \n        .\n      \n    \n    {\\displaystyle {\\frac {\\mathrm {d} N_{B}}{\\mathrm {d} t}}=-\\lambda _{B}N_{B}+\\lambda _{A}N_{A}.}\n  \n\nThe rate of change of NB, that is dNB/dt, is related to the changes in the amounts of A and B, NB can increase as B is produced from A and decrease as B produces C.\nRe-writing using the previous results:\n\nThe subscripts simply refer to the respective nuclides, i.e. NA is the number of nuclides of type A; NA0 is the initial number of nuclides of type A; \u03bbA is the decay constant for A \u2013 and similarly for nuclide B. Solving this equation for NB gives:\n\n  \n    \n      \n        \n          N\n          \n            B\n          \n        \n        =\n        \n          \n            \n              \n                N\n                \n                  A\n                  0\n                \n              \n              \n                \u03bb\n                \n                  A\n                \n              \n            \n            \n              \n                \u03bb\n                \n                  B\n                \n              \n              \u2212\n              \n                \u03bb\n                \n                  A\n                \n              \n            \n          \n        \n        \n          (\n          \n            \n              e\n              \n                \u2212\n                \n                  \u03bb\n                  \n                    A\n                  \n                \n                t\n              \n            \n            \u2212\n            \n              e\n              \n                \u2212\n                \n                  \u03bb\n                  \n                    B\n                  \n                \n                t\n              \n            \n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle N_{B}={\\frac {N_{A0}\\lambda _{A}}{\\lambda _{B}-\\lambda _{A}}}\\left(e^{-\\lambda _{A}t}-e^{-\\lambda _{B}t}\\right).}\n  \n\nIn the case where B is a stable nuclide (\u03bbB = 0), this equation reduces to the previous solution:\n\n  \n    \n      \n        \n          lim\n          \n            \n              \u03bb\n              \n                B\n              \n            \n            \u2192\n            0\n          \n        \n        \n          [\n          \n            \n              \n                \n                  \n                    N\n                    \n                      A\n                      0\n                    \n                  \n                  \n                    \u03bb\n                    \n                      A\n                    \n                  \n                \n                \n                  \n                    \u03bb\n                    \n                      B\n                    \n                  \n                  \u2212\n                  \n                    \u03bb\n                    \n                      A\n                    \n                  \n                \n              \n            \n            \n              (\n              \n                \n                  e\n                  \n                    \u2212\n                    \n                      \u03bb\n                      \n                        A\n                      \n                    \n                    t\n                  \n                \n                \u2212\n                \n                  e\n                  \n                    \u2212\n                    \n                      \u03bb\n                      \n                        B\n                      \n                    \n                    t\n                  \n                \n              \n              )\n            \n          \n          ]\n        \n        =\n        \n          \n            \n              \n                N\n                \n                  A\n                  0\n                \n              \n              \n                \u03bb\n                \n                  A\n                \n              \n            \n            \n              0\n              \u2212\n              \n                \u03bb\n                \n                  A\n                \n              \n            \n          \n        \n        \n          (\n          \n            \n              e\n              \n                \u2212\n                \n                  \u03bb\n                  \n                    A\n                  \n                \n                t\n              \n            \n            \u2212\n            1\n          \n          )\n        \n        =\n        \n          N\n          \n            A\n            0\n          \n        \n        \n          (\n          \n            1\n            \u2212\n            \n              e\n              \n                \u2212\n                \n                  \u03bb\n                  \n                    A\n                  \n                \n                t\n              \n            \n          \n          )\n        \n        ,\n      \n    \n    {\\displaystyle \\lim _{\\lambda _{B}\\rightarrow 0}\\left[{\\frac {N_{A0}\\lambda _{A}}{\\lambda _{B}-\\lambda _{A}}}\\left(e^{-\\lambda _{A}t}-e^{-\\lambda _{B}t}\\right)\\right]={\\frac {N_{A0}\\lambda _{A}}{0-\\lambda _{A}}}\\left(e^{-\\lambda _{A}t}-1\\right)=N_{A0}\\left(1-e^{-\\lambda _{A}t}\\right),}\n  \n\nas shown above for one decay. The solution can be found by the integration factor method, where the integrating factor is e\u03bbBt. This case is perhaps the most useful since it can derive both the one-decay equation (above) and the equation for multi-decay chains (below) more directly.\n\n\n===== Chain of any number of decays =====\nFor the general case of any number of consecutive decays in a decay chain, i.e. A1 \u2192 A2 \u00b7\u00b7\u00b7 \u2192 Ai \u00b7\u00b7\u00b7 \u2192 AD, where D is the number of decays and i is a dummy index (i = 1, 2, 3, ..., D), each nuclide population can be found in terms of the previous population. In this case N2 = 0, N3 = 0, ..., ND = 0. Using the above result in a recursive form:\n\n  \n    \n      \n        \n          \n            \n              \n                d\n              \n              \n                N\n                \n                  j\n                \n              \n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        =\n        \u2212\n        \n          \u03bb\n          \n            j\n          \n        \n        \n          N\n          \n            j\n          \n        \n        +\n        \n          \u03bb\n          \n            j\n            \u2212\n            1\n          \n        \n        \n          N\n          \n            (\n            j\n            \u2212\n            1\n            )\n            0\n          \n        \n        \n          e\n          \n            \u2212\n            \n              \u03bb\n              \n                j\n                \u2212\n                1\n              \n            \n            t\n          \n        \n        .\n      \n    \n    {\\displaystyle {\\frac {\\mathrm {d} N_{j}}{\\mathrm {d} t}}=-\\lambda _{j}N_{j}+\\lambda _{j-1}N_{(j-1)0}e^{-\\lambda _{j-1}t}.}\n  \n\nThe general solution to the recursive problem is given by Bateman's equations:\n\n\n==== Multiple products ====\nIn all of the above examples, the initial nuclide decays into just one product. Consider the case of one initial nuclide that can decay into either of two products, that is A \u2192 B and A \u2192 C in parallel. For example, in a sample of potassium-40, 89.3% of the nuclei decay to calcium-40 and 10.7% to argon-40. We have for all time t:\n\n  \n    \n      \n        N\n        =\n        \n          N\n          \n            A\n          \n        \n        +\n        \n          N\n          \n            B\n          \n        \n        +\n        \n          N\n          \n            C\n          \n        \n      \n    \n    {\\displaystyle N=N_{A}+N_{B}+N_{C}}\n  \n\nwhich is constant, since the total number of nuclides remains constant. Differentiating with respect to time:\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  \n                    \n                      \n                        d\n                      \n                      \n                        N\n                        \n                          A\n                        \n                      \n                    \n                    \n                      \n                        d\n                      \n                      t\n                    \n                  \n                \n              \n              \n                \n                =\n                \u2212\n                \n                  (\n                  \n                    \n                      \n                        \n                          \n                            d\n                          \n                          \n                            N\n                            \n                              B\n                            \n                          \n                        \n                        \n                          \n                            d\n                          \n                          t\n                        \n                      \n                    \n                    +\n                    \n                      \n                        \n                          \n                            d\n                          \n                          \n                            N\n                            \n                              C\n                            \n                          \n                        \n                        \n                          \n                            d\n                          \n                          t\n                        \n                      \n                    \n                  \n                  )\n                \n              \n            \n            \n              \n                \u2212\n                \u03bb\n                \n                  N\n                  \n                    A\n                  \n                \n              \n              \n                \n                =\n                \u2212\n                \n                  N\n                  \n                    A\n                  \n                \n                \n                  (\n                  \n                    \n                      \u03bb\n                      \n                        B\n                      \n                    \n                    +\n                    \n                      \u03bb\n                      \n                        C\n                      \n                    \n                  \n                  )\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}{\\frac {\\mathrm {d} N_{A}}{\\mathrm {d} t}}&=-\\left({\\frac {\\mathrm {d} N_{B}}{\\mathrm {d} t}}+{\\frac {\\mathrm {d} N_{C}}{\\mathrm {d} t}}\\right)\\\\-\\lambda N_{A}&=-N_{A}\\left(\\lambda _{B}+\\lambda _{C}\\right)\\\\\\end{aligned}}}\n  \n\ndefining the total decay constant \u03bb in terms of the sum of partial decay constants \u03bbB and \u03bbC:\n\n  \n    \n      \n        \u03bb\n        =\n        \n          \u03bb\n          \n            B\n          \n        \n        +\n        \n          \u03bb\n          \n            C\n          \n        \n        .\n      \n    \n    {\\displaystyle \\lambda =\\lambda _{B}+\\lambda _{C}.}\n  \n\nSolving this equation for NA:\n\n  \n    \n      \n        \n          N\n          \n            A\n          \n        \n        =\n        \n          N\n          \n            A\n            0\n          \n        \n        \n          e\n          \n            \u2212\n            \u03bb\n            t\n          \n        \n        .\n      \n    \n    {\\displaystyle N_{A}=N_{A0}e^{-\\lambda t}.}\n  \n\nwhere NA0 is the initial number of nuclide A. When measuring the production of one nuclide, one can only observe the total decay constant \u03bb. The decay constants \u03bbB and \u03bbC determine the probability for the decay to result in products B or C as follows:\n\n  \n    \n      \n        \n          N\n          \n            B\n          \n        \n        =\n        \n          \n            \n              \u03bb\n              \n                B\n              \n            \n            \u03bb\n          \n        \n        \n          N\n          \n            A\n            0\n          \n        \n        \n          (\n          \n            1\n            \u2212\n            \n              e\n              \n                \u2212\n                \u03bb\n                t\n              \n            \n          \n          )\n        \n        ,\n      \n    \n    {\\displaystyle N_{B}={\\frac {\\lambda _{B}}{\\lambda }}N_{A0}\\left(1-e^{-\\lambda t}\\right),}\n  \n\n  \n    \n      \n        \n          N\n          \n            C\n          \n        \n        =\n        \n          \n            \n              \u03bb\n              \n                C\n              \n            \n            \u03bb\n          \n        \n        \n          N\n          \n            A\n            0\n          \n        \n        \n          (\n          \n            1\n            \u2212\n            \n              e\n              \n                \u2212\n                \u03bb\n                t\n              \n            \n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle N_{C}={\\frac {\\lambda _{C}}{\\lambda }}N_{A0}\\left(1-e^{-\\lambda t}\\right).}\n  \n\nbecause the fraction \u03bbB/\u03bb of nuclei decay into B while the fraction \u03bbC/\u03bb of nuclei decay into C.\n\n\n=== Corollaries of laws ===\nThe above equations can also be written using quantities related to the number of nuclide particles N in a sample;\n\nThe activity: A = \u03bbN.\nThe amount of substance: n = N/NA.\nThe mass: m = Mn = MN/NA.\nwhere NA = 6.02214076\u00d71023 mol\u22121\u200d is the Avogadro constant, M is the molar mass of the substance in kg/mol, and the amount of the substance n is in moles.\n\n\n=== Decay timing: definitions and relations ===\n\n\n==== Time constant and mean-life ====\nFor the one-decay solution A \u2192 B:\n\n  \n    \n      \n        N\n        =\n        \n          N\n          \n            0\n          \n        \n        \n        \n          e\n          \n            \u2212\n            \n              \u03bb\n            \n            t\n          \n        \n        =\n        \n          N\n          \n            0\n          \n        \n        \n        \n          e\n          \n            \u2212\n            t\n            \n              /\n            \n            \u03c4\n          \n        \n        ,\n        \n        \n      \n    \n    {\\displaystyle N=N_{0}\\,e^{-{\\lambda }t}=N_{0}\\,e^{-t/\\tau },\\,\\!}\n  \n\nthe equation indicates that the decay constant \u03bb has units of t\u22121, and can thus also be represented as 1/\u03c4, where \u03c4 is a characteristic time of the process called the time constant.\nIn a radioactive decay process, this time constant is also the mean lifetime for decaying atoms. Each atom \"lives\" for a finite amount of time before it decays, and it may be shown that this mean lifetime is the arithmetic mean of all the atoms' lifetimes, and that it is \u03c4, which again is related to the decay constant as follows:\n\n  \n    \n      \n        \u03c4\n        =\n        \n          \n            1\n            \u03bb\n          \n        \n        .\n      \n    \n    {\\displaystyle \\tau ={\\frac {1}{\\lambda }}.}\n  \n\nThis form is also true for two-decay processes simultaneously A \u2192 B + C, inserting the equivalent values of decay constants (as given above)\n\n  \n    \n      \n        \u03bb\n        =\n        \n          \u03bb\n          \n            B\n          \n        \n        +\n        \n          \u03bb\n          \n            C\n          \n        \n        \n      \n    \n    {\\displaystyle \\lambda =\\lambda _{B}+\\lambda _{C}\\,}\n  \n\ninto the decay solution leads to:\n\n  \n    \n      \n        \n          \n            1\n            \u03c4\n          \n        \n        =\n        \u03bb\n        =\n        \n          \u03bb\n          \n            B\n          \n        \n        +\n        \n          \u03bb\n          \n            C\n          \n        \n        =\n        \n          \n            1\n            \n              \u03c4\n              \n                B\n              \n            \n          \n        \n        +\n        \n          \n            1\n            \n              \u03c4\n              \n                C\n              \n            \n          \n        \n        \n      \n    \n    {\\displaystyle {\\frac {1}{\\tau }}=\\lambda =\\lambda _{B}+\\lambda _{C}={\\frac {1}{\\tau _{B}}}+{\\frac {1}{\\tau _{C}}}\\,}\n  \n\n\n==== Half-life ====\n\nA more commonly used parameter is the half-life T1/2. Given a sample of a particular radionuclide, the half-life is the time taken for half the radionuclide's atoms to decay. For the case of one-decay nuclear reactions:\n\n  \n    \n      \n        N\n        =\n        \n          N\n          \n            0\n          \n        \n        \n        \n          e\n          \n            \u2212\n            \n              \u03bb\n            \n            t\n          \n        \n        =\n        \n          N\n          \n            0\n          \n        \n        \n        \n          e\n          \n            \u2212\n            t\n            \n              /\n            \n            \u03c4\n          \n        \n        ,\n        \n        \n      \n    \n    {\\displaystyle N=N_{0}\\,e^{-{\\lambda }t}=N_{0}\\,e^{-t/\\tau },\\,\\!}\n  \n\nthe half-life is related to the decay constant as follows: set N = N0/2 and t = T1/2 to obtain\n\n  \n    \n      \n        \n          t\n          \n            1\n            \n              /\n            \n            2\n          \n        \n        =\n        \n          \n            \n              ln\n              \u2061\n              2\n            \n            \u03bb\n          \n        \n        =\n        \u03c4\n        ln\n        \u2061\n        2.\n      \n    \n    {\\displaystyle t_{1/2}={\\frac {\\ln 2}{\\lambda }}=\\tau \\ln 2.}\n  \n\nThis relationship between the half-life and the decay constant shows that highly radioactive substances are quickly spent, while those that radiate weakly endure longer. Half-lives of known radionuclides vary by almost 54 orders of magnitude, from more than 2.25(9)\u00d71024 years (6.9\u00d71031 sec) for the very nearly stable nuclide 128Te, to 8.6(6)\u00d710\u221223 seconds for the highly unstable nuclide 5H.\nThe factor of ln(2) in the above relations results from the fact that the concept of \"half-life\" is merely a way of selecting a different base other than the natural base e for the lifetime expression. The time constant \u03c4 is the e \u22121 -life, the time until only 1/e remains, about 36.8%, rather than the 50% in the half-life of a radionuclide. Thus, \u03c4 is longer than t1/2. The following equation can be shown to be valid:\n\n  \n    \n      \n        N\n        (\n        t\n        )\n        =\n        \n          N\n          \n            0\n          \n        \n        \n        \n          e\n          \n            \u2212\n            t\n            \n              /\n            \n            \u03c4\n          \n        \n        =\n        \n          N\n          \n            0\n          \n        \n        \n        \n          2\n          \n            \u2212\n            t\n            \n              /\n            \n            \n              t\n              \n                1\n                \n                  /\n                \n                2\n              \n            \n          \n        \n        .\n        \n        \n      \n    \n    {\\displaystyle N(t)=N_{0}\\,e^{-t/\\tau }=N_{0}\\,2^{-t/t_{1/2}}.\\,\\!}\n  \n\nSince radioactive decay is exponential with a constant probability, each process could as easily be described with a different constant time period that (for example) gave its \"(1/3)-life\" (how long until only 1/3 is left) or \"(1/10)-life\" (a time period until only 10% is left), and so on. Thus, the choice of \u03c4 and t1/2 for marker-times, are only for convenience, and from convention. They reflect a fundamental principle only in so much as they show that the same proportion of a given radioactive substance will decay, during any time-period that one chooses.\nMathematically, the nth life for the above situation would be found in the same way as above\u2014by setting N = N0/n, t = T1/n and substituting into the decay solution to obtain\n\n  \n    \n      \n        \n          t\n          \n            1\n            \n              /\n            \n            n\n          \n        \n        =\n        \n          \n            \n              ln\n              \u2061\n              n\n            \n            \u03bb\n          \n        \n        =\n        \u03c4\n        ln\n        \u2061\n        n\n        .\n      \n    \n    {\\displaystyle t_{1/n}={\\frac {\\ln n}{\\lambda }}=\\tau \\ln n.}\n  \n\n\n=== Example for carbon-14 ===\nCarbon-14 has a half-life of 5700(30) years and a decay rate of 14 disintegrations per minute (dpm) per gram of natural carbon.\nIf an artifact is found to have radioactivity of 4 dpm per gram of its present C, we can find the approximate age of the object using the above equation:\n\n  \n    \n      \n        N\n        =\n        \n          N\n          \n            0\n          \n        \n        \n        \n          e\n          \n            \u2212\n            t\n            \n              /\n            \n            \u03c4\n          \n        \n        ,\n      \n    \n    {\\displaystyle N=N_{0}\\,e^{-t/\\tau },}\n  \n\nwhere: \n\n  \n    \n      \n        \n          \n            \n              \n                \n                  \n                    N\n                    \n                      N\n                      \n                        0\n                      \n                    \n                  \n                \n              \n              \n                \n                =\n                4\n                \n                  /\n                \n                14\n                \u2248\n                0.286\n                ,\n              \n            \n            \n              \n                \u03c4\n              \n              \n                \n                =\n                \n                  \n                    \n                      T\n                      \n                        1\n                        \n                          /\n                        \n                        2\n                      \n                    \n                    \n                      ln\n                      \u2061\n                      2\n                    \n                  \n                \n                \u2248\n                8267\n                \n                   years\n                \n                ,\n              \n            \n            \n              \n                t\n              \n              \n                \n                =\n                \u2212\n                \u03c4\n                \n                ln\n                \u2061\n                \n                  \n                    N\n                    \n                      N\n                      \n                        0\n                      \n                    \n                  \n                \n                \u2248\n                10356\n                \n                   years\n                \n                .\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}{\\frac {N}{N_{0}}}&=4/14\\approx 0.286,\\\\\\tau &={\\frac {T_{1/2}}{\\ln 2}}\\approx 8267{\\text{ years}},\\\\t&=-\\tau \\,\\ln {\\frac {N}{N_{0}}}\\approx 10356{\\text{ years}}.\\end{aligned}}}\n  \n\n\n=== Changing rates ===\nThe radioactive decay modes of electron capture and internal conversion are known to be slightly sensitive to chemical and environmental effects that change the electronic structure of the atom, which in turn affects the presence of 1s and 2s electrons that participate in the decay process. A small number of nuclides are affected. For example, chemical bonds can affect the rate of electron capture to a small degree (in general, less than 1%) depending on the proximity of electrons to the nucleus. In 7Be, a difference of 0.9% has been observed between half-lives in metallic and insulating environments. This relatively large effect is because beryllium is a small atom whose valence electrons are in 2s atomic orbitals, which are subject to electron capture in 7Be because (like all s atomic orbitals in all atoms) they naturally penetrate into the nucleus.\nIn 1992, Jung et al. of the Darmstadt Heavy-Ion Research group observed an accelerated \u03b2\u2212 decay of 163Dy66+. Although neutral 163Dy is a stable isotope, the fully ionized 163Dy66+ undergoes \u03b2\u2212 decay into the K and L shells to 163Ho66+ with a half-life of 47 days.\nRhenium-187 is another spectacular example. 187Re normally undergoes beta decay to 187Os with a half-life of 41.6 \u00d7 109 years, but studies using fully ionised 187Re atoms (bare nuclei) have found that this can decrease to only 32.9 years. This is attributed to \"bound-state \u03b2\u2212 decay\" of the fully ionised atom \u2013 the electron is emitted into the \"K-shell\" (1s atomic orbital), which cannot occur for neutral atoms in which all low-lying bound states are occupied.\n\nA number of experiments have found that decay rates of other modes of artificial and naturally occurring radioisotopes are, to a high degree of precision, unaffected by external conditions such as temperature, pressure, the chemical environment, and electric, magnetic, or gravitational fields. Comparison of laboratory experiments over the last century, studies of the Oklo natural nuclear reactor (which exemplified the effects of thermal neutrons on nuclear decay), and astrophysical observations of the luminosity decays of distant supernovae (which occurred far away so the light has taken a great deal of time to reach us), for example, strongly indicate that unperturbed decay rates have been constant (at least to within the limitations of small experimental errors) as a function of time as well.\nRecent results suggest the possibility that decay rates might have a weak dependence on environmental factors. It has been suggested that measurements of decay rates of silicon-32, manganese-54, and radium-226 exhibit small seasonal variations (of the order of 0.1%).  However, such measurements are highly susceptible to systematic errors, and a subsequent paper has found no evidence for such correlations in seven other isotopes (22Na, 44Ti, 108Ag, 121Sn, 133Ba, 241Am, 238Pu), and sets upper limits on the size of any such effects. The decay of radon-222 was once reported to exhibit large 4% peak-to-peak seasonal variations (see plot),  which were proposed to be related to either solar flare activity or the distance from the Sun, but detailed analysis of the experiment's design flaws, along with comparisons to other, much more stringent and systematically controlled, experiments refute this claim.\n\n\n==== GSI anomaly ====\n\nAn unexpected series of experimental results for the rate of decay of heavy highly charged radioactive ions circulating in a storage ring has provoked theoretical activity in an effort to find a convincing explanation. The rates of weak decay of two radioactive species with half lives of about 40 s and 200 s are found to have a significant oscillatory modulation, with a period of about 7 s.\nThe observed phenomenon is known as the GSI anomaly, as the storage ring is a facility at the GSI Helmholtz Centre for Heavy Ion Research in Darmstadt, Germany.  As the decay process produces an electron neutrino, some of the proposed explanations for the observed rate oscillation invoke neutrino properties. Initial ideas related to flavour oscillation met with skepticism.  A more recent proposal involves mass differences between neutrino mass eigenstates.\n\n\n== Nuclear processes ==\nA nuclide is considered to \"exist\" if it has a half-lifes greater than 2x10-14s. This is an arbitrary boundary; shorter half-lives are considered resonances, such as a system undergoing a nuclear reaction. This time scale is characteristic of the strong interaction which creates the nuclear force.  Only nuclides are considered to decay and produce radioactivity.:\u200a568\u200a\nNuclides can be stable or unstable. Unstable nuclides decay, possibly in several steps, until they become stable. There are 256 known stable nuclides. The number of unstable nuclides discovered has grown, with about 3000 known in 2006.\nThe most common and consequently historically the most important forms of natural radioactive decay involve the emission of alpha-particles, beta-particles, and gamma rays. Each of these correspond to a fundamental interaction predominantly responsible for the radioactivity::\u200a142\u200a \n\nalpha-decay -> strong interaction,\nbeta-decay -> weak interaction,\ngamma-decay -> electromagnetism.\nIn alpha decay, a particle containing two protons and two neutrons, equivalent to a He nucleus, breaks out of the parent nucleus. The process represents a competition between the electromagnetic repulsion between the protons in the nucleus and attractive nuclear force, a residual of the strong interaction. The alpha particle is an especially strongly bound nucleus, helping it win the competition more often.:\u200a872\u200a  However some nuclei break up or fission into larger particles and artificial nuclei decay with the emission of \nsingle protons, double protons, and other combinations.\nBeta decay transforms a neutron into proton or vice versa.  When a neutron inside a parent nuclide decays to a proton, an electron, a anti-neutrino, and nuclide with high atomic number results. When a proton in a parent nuclide transforms to a neutron, a positron, a neutrino, and  nuclide with a lower atomic number results. These changes are a direct manifestation of the weak interaction.:\u200a874\u200a\nGamma decay resembles other kinds of electromagnetic emission: it corresponds to transitions between an excited quantum state and lower energy state. Any of the particle decay mechanisms often leave the daughter in an excited state, which then decays via gamma emission.:\u200a876\u200a\nOther forms of decay include neutron emission, electron capture, internal conversion, cluster decay.\n\n\n== Hazard warning signs ==\n\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\n\n== See also ==\n\n Nuclear technology portal\n Physics portal\n\n\n== Notes ==\n\n\n== References ==\n\n\n== External links ==\n\nThe Lund/LBNL Nuclear Data Search \u2013 Contains tabulated information on radioactive decay types and energies.\nNomenclature of nuclear chemistry\nSpecific activity and related topics.\nThe Live Chart of Nuclides \u2013 IAEA\nInteractive Chart of Nuclides Archived 10 October 2018 at the Wayback Machine\nHealth Physics Society Public Education Website\nBeach, Chandler B., ed. (1914). \"Becquerel Rays\" . The New Student's Reference Work . Chicago: F. E. Compton and Co.\nAnnotated bibliography for radioactivity from the Alsos Digital Library for Nuclear Issues Archived 7 October 2010 at the Wayback Machine\nStochastic Java applet on the decay of radioactive atoms Archived 13 August 2011 at the Wayback Machine by Wolfgang Bauer\nStochastic Flash simulation on the decay of radioactive atoms by David M. Harrison\n\"Henri Becquerel: The Discovery of Radioactivity\", Becquerel's 1896 articles online and analyzed on BibNum [click '\u00e0 t\u00e9l\u00e9charger' for English version].\n\"Radioactive change\", Rutherford & Soddy article (1903), online and analyzed on Bibnum [click '\u00e0 t\u00e9l\u00e9charger' for English version]",
        "unit": "radioactivity",
        "url": "https://en.wikipedia.org/wiki/Radioactive_decay"
    },
    {
        "_id": "Swiss_franc",
        "clean": "Swiss franc",
        "text": "The Swiss franc, or simply the franc, is the currency and legal tender of Switzerland and Liechtenstein. It is also legal tender in the Italian exclave of Campione d'Italia which is surrounded by Swiss territory. The Swiss National Bank (SNB) issues banknotes and the federal mint Swissmint issues coins.\nIt is also designated through currency signs Fr. (in German language), fr. (in French, Italian, Romansh languages), as well as in any other language, or internationally as CHF which stands for Confoederatio Helvetica Franc. This acronym also serves as the ISO 4217 currency code, used by banks and financial institutions.\nThe smaller denomination, a hundredth of a franc, is a Rappen (Rp.) in German, centime (c.) in French, centesimo (ct.) in Italian, and rap (rp.) in Romansh.\nThe official symbols Fr. (German symbol) and fr. (Latin languages) are widely used by businesses and advertisers, also for the English language. According to Art. 1 SR/RS 941.101 of the federal law collection the internationally official abbreviation \u2013 besides the national languages \u2013 however is CHF, also in English; respective guides also request to use the ISO 4217 code. The use of SFr. for Swiss Franc and fr.sv. are outdated. As previously indicated Latinate \"CH\" stands for Confoederatio Helvetica; given the different languages used in Switzerland, Latin is used for language-neutral inscriptions on its coins.\n\n\n== History ==\n\n\n=== Before the Helvetic Republic ===\n\nBefore 1798, about 75 entities were making coins in Switzerland, including the 25 cantons and half-cantons, 16 cities, and abbeys, resulting in about 860 different coins in circulation, with different values, denominations and monetary systems. However, the origins of a majority of these currencies can be traced to either the French livre tournois (the predecessor of the French franc) or the South German gulden of the 17th century. The new Swiss currencies emerged in the 18th century after Swiss cantons did not follow the pace of depreciations which occurred in France and Germany. However, they mostly existed only in small change as they were little more than community currency, current in one canton but not in the other, and foreign coins like French francs and kronenthalers were more recognized as currency all over Switzerland.\nA high-level summary of existing currencies at the end of the 18th century is shown below, including their equivalents in terms of the French \u00e9cu of 26.67 g fine silver, the South German kronenthaler of 25.71 g fine silver, and Swiss francs of 4.5 g fine silver.\n\nThe livre of Bern and most western Swiss cantons like Basel, Aargau, Fribourg, Vaud, Valais, Lausanne, Neuch\u00e2tel and Solothurn originated from the French livre tournois.\n\nThe livre was divided into 20 sols, 10 batzen or 40 kreuzer.\nAfter 1690, 30 Bern batzen equated to either\na German Reichsthaler (25.984 g fine silver) worth 2 gulden or 120 kreuzer, or\na French Louis d'Argent, equivalent to the Spanish dollar (24.93 g fine silver), worth 3 livres tournois or 60 sols.\nAfter 1726, the French \u00e9cu (laubthaler) of 26+2\u20443 g fine silver was valued at 4 livres or 40 batzen (vs 6 livres tournois in France).\nAfter 1815, the German kronenthaler (Brabant thaler) of 25+5\u20447 g fine silver was valued at 3.9 livres or 39 batzen (in Neuch\u00e2tel, 4.1 livres).\nThis livre or frank of 1\u20444 \u00e9cu was the model for the frank of the Helvetic Republic of 1798\u20131847.\nCurrencies identical to this standard include the Berne thaler, Basel thaler, Fribourg gulden, Neuch\u00e2tel gulden, Solothurn thaler and Valais thaler.\nGeneva had its own currency, the florin petite monnaie, with 3+1\u20442 florins equal to the livre courant. After 1641, the Spanish dollar was worth 10+1\u20442 florins or 3 livres. Afterwards, the \u00e9cu was valued at 12+3\u20444 florins or 3+9\u204414 livres, while the kronenthaler was valued at 12+3\u20448 florins or 3+15\u204428 livres. See also Geneva thaler and Geneva genevoise.\nMany currencies of central and eastern Switzerland originated from the South German gulden. It was divided into 40 schilling or 60 kreuzer, and the thaler was worth 2 gulden. After 1690, this gulden was worth 1\u20442 a Reichsthaler specie, or 12.992 g fine silver. After 1730, the different guilders of Southern Germany and Switzerland fragmented under varying rates of depreciation. The South German gulden, worth 1\u204424 a Cologne mark (233.856 g) of fine silver, also applied to the Swiss cantons of St. Gallen, Appenzell, Schaffhausen and Thurgau. The French \u00e9cu was valued at 2.8 gulden, while the kronenthaler was valued at 2.7 gulden. See St. Gallen thaler.\nThe cantons of Zurich, Schwyz and Glarus, however, maintained a stronger gulden worth 1\u204422 a Cologne mark of fine silver. The French \u00e9cu was valued at 2+1\u20442 gulden, while the kronenthaler was valued at 2+18\u204440 gulden; see Z\u00fcrich thaler and Schwyz gulden. On the other hand, the central Swiss cantons of Luzern, Uri, Zug and Unterwalden maintained a weaker gulden vs the South German gulden. The French \u00e9cu was valued at 3 gulden, while the kronenthaler was valued at 2+37\u204440 gulden (see Luzern gulden).\n\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\n\n=== Helvetic Republic to Regeneration, 1798\u20131847 ===\nIn 1798, the Helvetic Republic introduced the franc or frank, modelled on the Bern livre worth 1\u20444 the \u00e9cu, subdivided into 10 batzen or 100 rappen (centimes). It contained 6+2\u20443 grams of fine silver and was initially worth 1+1\u20442 livres tournois or 1.48 French francs.\n\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\nThis franc was issued until the end of the Helvetic Republic in 1803, but served as the model for the currencies of several cantons in the Mediation period (1803\u20131814). These 19 cantonal currencies were the Appenzell frank, Argovia frank, Basel frank, Berne frank, Fribourg frank, Geneva franc, Glarus frank, Graub\u00fcnden frank, Luzern frank, St. Gallen frank, Schaffhausen frank, Schwyz frank, Solothurn frank, Thurgau frank, Ticino franco, Unterwalden frank, Uri frank, Vaud franc, and Z\u00fcrich frank.\n\nAfter 1815, the restored Swiss Confederacy attempted to simplify the system of currencies once again. As of 1820, a total of 8,000 distinct coins were current in Switzerland: those issued by cantons, cities, abbeys, and principalities or lordships, mixed with surviving coins of the Helvetic Republic and the pre-1798 Helvetic Republic. In 1825, the cantons of Bern, Basel, Fribourg, Solothurn, Aargau, and Vaud formed a monetary concordate, issuing standardised coins, the so-called Konkordanzbatzen, still carrying the coat of arms of the issuing canton, but interchangeable and identical in value. The reverse side of the coin displayed a Swiss cross with the letter C in the center.\n\n\n=== Franc of the Swiss Confederation, 1850\u2013present ===\nThe Konkordanzbatzen among the Swiss cantons agreeing on an exclusive issue of currency in francs and batzen failed to replace the over 8,000 different coins and notes in circulation. Despite introduction of the first Swiss franc, the South German kronenthaler became the more desirable coin to use in the 19th century, and it was still quoted in pre-1798 currency equivalents. Furthermore, less than 15% of Swiss money in circulation was in local currency, since French and German gold and silver trade coins proved to be more desirable means of exchange. A final problem was that the first Swiss franc was based on the French \u00e9cu which was being phased out by France in the 19th century.\nTo solve this problem, the new Swiss Federal Constitution of 1848 specified that the federal government would be the only entity allowed to issue money in Switzerland. This was followed two years later by the first Federal Coinage Act, passed by the Federal Assembly on 7 May 1850, which introduced the franc as the monetary unit of Switzerland.\nThe Swiss franc was introduced at par with the French franc, at 4.5 g fine silver or 9\u204431 g = 0.29032 g fine gold (ratio 15.5). The currencies of the Swiss cantons were converted to Swiss francs by first restating their equivalents in German kronenthaler (\u00e9cu brabant) of 25+5\u20447 grams fine silver, and then to Swiss francs at the rate of 7 \u00e9cu brabant = 40 Swiss francs. The first franc worth 1\u20444th the French \u00e9cu was converted at 1.4597 Swiss francs.\n\nIn 1865, France, Belgium, Italy, and Switzerland formed the Latin Monetary Union, in which they agreed to value their national currencies to a standard of 4.5 grams of fine silver or 0.290322 grams fine gold, equivalent to US$1 = CHF 5.1826 until 1934. Even after the monetary union faded away in the 1920s and officially ended in 1927, the Swiss franc remained on that standard until 27 September 1936, when it suffered its sole devaluation during the  Great Depression. Following the devaluations of the British pound, U.S. dollar and French franc, the Swiss franc was devalued 30% to 0.20322 grams fine gold, equivalent to US$1 = CHF 4.37295. In 1945, Switzerland joined the Bretton Woods system with its exchange rate to the dollar fixed until 1970.\nThe Swiss franc has historically been considered a safe-haven currency, with a legal requirement that a minimum of 40% be backed by gold reserves. However, this link to gold, which dated from the 1920s, was terminated on 1 May 2000 following a referendum, making the franc fiat money. By March 2005, following a gold-selling program, the Swiss National Bank held 1,290 tonnes of gold in reserves, which equated to 20% of its assets.\nIn November 2014, the referendum on the \"Swiss Gold Initiative\", which proposed a restoration of 20% gold backing for the Swiss franc, was voted down.\n\n\n==== 2011\u20132014: Big movements and capping ====\n\nThe onset of the Greek sovereign debt crisis resulted in a strong appreciation in the value of the Swiss franc, past US$1.10 (CHF 0.91 per USD) in March 2011, to US$1.20 (CHF 0.833 per USD) in June 2011, and to US$1.30 (CHF 0.769 per USD) in August 2011. This prompted the Swiss National Bank to boost the franc's liquidity to try to counter its \"massive overvaluation\". The Economist argued that its Big Mac Index in July 2011 indicated an overvaluation of 98% over the dollar, and cited Swiss companies releasing profit warnings and threatening to move operations out of the country due to the strength of the franc. Demand for francs and franc-denominated assets was so strong that nominal short-term Swiss interest rates became negative.\nOn 6 September 2011, the day after the franc traded at 1.11 CHF/\u20ac and appeared headed to parity with the euro, the SNB set a minimum exchange rate of 1.20 CHF to the euro ('capping' the franc's appreciation), saying \"the value of the franc is a threat to the economy\", and that it was \"prepared to buy foreign currency in unlimited quantities\". In response to this announcement the franc fell against the euro from 1.11 to 1.20 CHF, against the U.S. dollar from 0.787 to 0.856 CHF, and against all 16 of the most active currencies on the same day. It was the largest plunge of the franc ever against the euro.\nThe intervention stunned currency traders, since the franc had long been regarded as a safe haven. The SNB had previously set an exchange rate target in 1978 against the Deutsche mark and maintained it, although at the cost of high inflation. Until mid-January 2015, the franc continued to trade below the target level set by the SNB, though the ceiling was broken at least once on 5 April 2012, albeit briefly.\n\n\n==== End of capping ====\nOn 18 December 2014, the Swiss central bank introduced a negative interest rate on bank deposits to support its CHF ceiling. However, with the euro declining in value over the following weeks, in a move dubbed Francogeddon for its effect on markets, the Swiss National Bank abandoned the ceiling on 15 January 2015, and the franc promptly increased in value compared with the euro by 30%, although this only lasted a few minutes before part of the increase was reversed. The move was not announced in advance and resulted in \"turmoil\" in stock and currency markets. By the close of trading that day, the franc was up 23% against the euro and 21% against the US dollar. The full daily appreciation of the franc was equivalent to $31,000 per single futures contract: more than the market had moved collectively in the previous thousand days. The key CHF interest rate was also lowered from \u22120.25% to \u22120.75%, meaning depositors would be paying an increased fee to keep their funds in a Swiss bank account. This devaluation of the euro against the franc was expected to hurt Switzerland's large export industry. The Swatch Group, for example, saw its shares drop 15% (in Swiss franc terms) with the announcements so that the share price may have increased on that day in terms of other major currencies.\nThe large and unexpected jump caused major losses for some currency traders. Alpari, a Russian-owned spread betting firm established in the UK, temporarily declared insolvency before announcing its desire to be acquired (and later denied rumours of an acquisition) by FXCM. FXCM was bailed out by its parent company. Saxo Bank of Denmark reported losses on 19 January 2015. New Zealand foreign exchange broker Global Brokers NZ announced it \"could no longer meet New Zealand regulators' minimum capital requirements\" and terminated its business.\n\n\n== Coins ==\n\n\n=== Coins before the Helvetic Republic ===\nCoins before 1700 were based on either the French livre tournois system (in Louis d'Argent, Louis d'Or and fractions) or the South German gulden system (in Reichsthalers, florins and fractions). After 1700 Swiss cantonal currencies diverged from the value of the French and German units. However, they mostly existed only in small change as they were a mere community currency, current in one canton but not in the other, and foreign coins like French francs and Brabant dollars were more recognized as currency all over Switzerland.\n\n\n=== Coins of the Helvetic Republic ===\n\nBetween 1798 and 1803, billon coins were issued in denominations of 1 centime, 1\u20442 batzen, and 1 batzen. Silver coins were issued for 10, 20 and 40 batzen (also denominated 4 francs), matching with French coins worth 1\u20444, 1\u20442 and 1 \u00e9cu. Gold 16- and 32-franc coins were issued in 1800, also matching with French coins worth 24 and 48 livres tournois.\n\n\n=== Coins of the Swiss Confederation ===\nIn 1850, coins were introduced in denominations of 1 centime, 2 centimes, 5 centimes, 10 centimes 20 centimes, 1\u20442 franc, 1 franc, 2 francs, and 5 francs. The 1 centime and 2 centime coins were struck in bronze; the 5 centimes, 10 centime and 20 centime in billon (with 5% to 15% silver content); and the 1\u20442 franc, 1 franc, 2 franc and 5 franc in .900 fine silver. Between 1860 and 1863, .800 fine silver was used, before the standard used in France of .835 fineness was adopted for all silver coins except the 5 francs (which remained .900 fineness) in 1875. In 1879, billon was replaced by cupronickel in the 5 centime and 10 centime coins and by nickel in the 20 centime piece. Gold coins in denominations of 10, 20, and 100 francs, known as Vreneli, circulated until 1936.\nBoth world wars only had a small effect on the Swiss coinage, with brass and zinc coins temporarily being issued. In 1931, the mass of the 5 franc coin was reduced from 25 grams to 15, with the silver content reduced to .835 fineness. The next year, nickel replaced cupronickel in the 5 centime and 10 centime coins.\nIn the late 1960s, the prices of internationally traded commodities rose significantly. A silver coin's metal value exceeded its monetary value, and many were being sent abroad for melting, which prompted the federal government to make this practice illegal. The statute was of little effect, and the melting of francs only subsided when the collectible value of the remaining francs again exceeded their material value.\nThe 1 centime coin was still produced until 2006, albeit in ever decreasing quantities, but its importance declined. Those who could justify the use of 1 centime coins for monetary purposes could obtain them at face value; any other user (such as collectors) had to pay an additional four centimes per coin to cover the production costs, which had exceeded the actual face value of the coin for many years. The coin fell into disuse in the late 1970s and early 1980s, but was only officially fully withdrawn from circulation and declared to be no longer legal tender on 1 January 2007. The long-forgotten 2 centime coin, not minted since 1974, was demonetized on 1 January 1978.\n\nThe designs of the coins have changed very little since 1879. Among the notable changes were new designs for the 5 francs coins in 1888, 1922, 1924 (minor), and 1931 (mostly just a size reduction). A new design for the bronze coins was used from 1948. Coins depicting a ring of stars (such as the 1 franc coin seen beside this paragraph) were altered from 22 stars to 23 stars in 1983; since the stars represent the Swiss cantons, the design was updated when in 1979 Jura seceded from the Canton of Bern and became the 23rd canton of the Swiss Confederation.\n\nThe 10 centime coins from 1879 onwards (except the years 1918\u201319 and 1932\u20131939) have had the same composition, size, and design to present and are still legal tender and found in circulation. For this, the coin entered the Guinness Book of Records as the oldest original currency in circulation.\n\nAll Swiss coins are language-neutral with respect to Switzerland's four national languages, featuring only numerals, the abbreviation \"Fr.\" for franc, and the Latin phrases Helvetia or Conf\u0153deratio Helvetica (depending on the denomination) or the inscription Libertas (Roman goddess of liberty) on the small coins. The name of the artist is present on the coins with the standing Helvetia and the herder.\nIn addition to these general-circulation coins, numerous series of commemorative coins have been issued, as well as silver and gold coins. These coins are no longer legal tender, but can in theory be exchanged at face value at post offices, and at national and cantonal banks, although their metal or collectors' value equals or exceeds their face value.\n\n\n== Banknotes ==\n\nIn 1907, the Swiss National Bank took over the issuance of banknotes from the cantons and various banks. It introduced denominations of 50, 100, 500 and 1000 francs. Twenty-franc notes were introduced in 1911, followed by 5-franc notes in 1913. In 1914, the Federal Treasury issued paper money in denominations of 5, 10 and 20 francs. These notes were issued in three different versions: French, German and Italian. The State Loan Bank also issued 25-franc notes that year. In 1952, the national bank ceased issuing 5-franc notes but introduced 10-franc notes in 1955. In 1996, 200-franc notes were introduced whilst the 500-franc note was discontinued.\nNine series of banknotes have been printed by the Swiss National Bank, seven of which have been released for use by the general public, the fourth and seventh being reserved and never issued. The sixth series from 1976, designed by Ernst and Ursula Hiestand, depicted persons from the world of science.\nThis series was recalled on 1 May 2000 and is no longer legal tender, but notes can still be exchanged for valid ones of the same face value at any National Bank branch or authorized agent, or mailed in by post to the National Bank in exchange for a bank account deposit. The exchange program originally was due to end on 30 April 2020, after which sixth-series notes would lose all value. As of 2016, 1.1 billion francs' worth of sixth-series notes had not yet been exchanged, even though they had not been legal tender for 16 years and only 4 more years remained to exchange them. To avoid having to expire such large amounts of money in 2020, the Federal Council (cabinet) and National Bank proposed in April 2017 to remove the time limit on exchanges for the sixth and future recalled series. As of 2020, this proposal was enacted, so old banknote series will not expire.\nThe seventh series was printed in 1984, but kept as a \"reserve series\", ready to be used if, for example, wide counterfeiting of the current series suddenly happened. When the Swiss National Bank decided to develop new security features and to abandon the concept of a reserve series, the details of the seventh series were released and the printed notes were destroyed. \nThe eighth series of banknotes was designed by J\u00f6rg Zintzmeyer around the theme of the arts and released starting in 1995. In addition to its new vertical design, this series was different from the previous one on several counts. Probably the most important difference from a practical point of view was that the seldom-used 500-franc note was replaced by a new 200-franc note; this new note has indeed proved more successful than the old 500-franc note. The base colours of the new notes were kept similar to the old ones, except that the 20-franc note was changed from blue to red to prevent a frequent confusion with the 100-franc note, and that the 10-franc note was changed from red to yellow. The size of the notes was changed as well, with all notes from the eighth series having the same height (74 mm), while the widths were changed as well, still increasing with the value of the notes. The new series contain many more security features than the previous ones; many of them are now visibly displayed and have been widely advertised, in contrast with the previous series for which most of the features were kept secret.\n\nAll banknotes are quadrilingual, displaying all information in the four national languages. With the eighth series, the banknotes depicting a Germanophone person have German and Romansch on the same side as the picture, whereas banknotes depicting a Francophone or an Italophone person have French and Italian on the same side as the picture. The reverse has the other two languages.\nWhen the fifth series lost its validity at the end of April 2000, the banknotes that had not been exchanged represented a total value of 244.3 million Swiss francs; in accordance with Swiss law, this amount was transferred to the Swiss Fund for Emergency Losses in the Case of Non-insurable Natural Disasters.\nIn February 2005, a competition was announced for the design of the ninth series, then planned to be released around 2010 on the theme \"Switzerland open to the world\". The results were announced in November 2005. The National Bank selected the designs of Swiss graphic designer Manuela Pfrunder as the basis of the new series. The first denomination to be issued was the 50-franc note on 12 April 2016. It was followed by the 20-franc note (17 May 2017), the 10-franc note (18 October 2017), the 200-franc note (15 August 2018), the 1000-franc note (5 March 2019), and the 100-franc note (12 September 2019).\nAll banknotes from the eighth series were withdrawn on 30 April 2021, but, like banknotes of the sixth series withdrawn in 2000, remain indefinitely redeemable at the Swiss National Bank.\n\n\n== Circulation ==\n\nThe Swiss franc is the currency and legal tender of Switzerland and Liechtenstein and also legal tender in the Italian exclave of Campione d'Italia. Although not formally legal tender in the German exclave of B\u00fcsingen am Hochrhein (the sole legal currency is the euro), it is in wide daily use there; with many prices quoted in Swiss francs. The Swiss franc is the only version of the franc still issued in Europe.\nAs of March 2010, the total value of released Swiss coins and banknotes was 49.664 billion Swiss francs.\n\nCombinations of up to 100 circulating Swiss coins (not including special or commemorative coins) are legal tender; banknotes are legal tender for any amount.\n\n\n== Current exchange rates ==\n\n\n== See also ==\nBanking in Switzerland\nEconomy of Switzerland\nGold standard\nHard currency\nIraqi Swiss dinar, a common name for the old Iraqi currency but not related to Swiss currency.\nLiechtenstein franc\nList of currencies in Europe\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Further reading ==\n\n\n== External links ==\n Media related to Money of Switzerland at Wikimedia Commons\n(in German) CashFollow.ch, Swiss Franc Tracker\n(in German) Schweizer-Franken.ch Archived 18 October 2018 at the Wayback Machine, Information about the Swiss Franc\n(in English) Switzerland Banknotes, Swiss Franc: Banknote Catalog from 1907\n(in English and German) The Banknotes of Switzerland\nFranc \u2013 currency at Merriam-Webster",
        "unit": "swiss franc",
        "url": "https://en.wikipedia.org/wiki/Swiss_franc"
    },
    {
        "_id": "Chinese_yuan",
        "clean": "Chinese yuan",
        "text": "The renminbi (Chinese: \u4eba\u6c11\u5e01; pinyin: R\u00e9nm\u00ednb\u00ec; lit. 'People's Currency'; symbol: \u00a5; ISO code: CNY; abbreviation: RMB), also known as the Chinese yuan, is the official currency of the People's Republic of China. The renminbi is issued by the People's Bank of China, the monetary authority of China. It is the world's fifth-most-traded currency as of April 2022.\nThe yuan (\u5143) is the basic unit of the renminbi. One yuan is divided into 10 jiao (\u89d2), and the jiao is further subdivided into 10 fen (\u5206). The word yuan is widely used to refer to the Chinese currency generally, especially in international contexts.\n\n\n== Valuation ==\nUntil 2005, the value of the renminbi was pegged to the US dollar. As China pursued its transition from central planning to a market economy and increased its participation in foreign trade, the renminbi was devalued to increase the competitiveness of Chinese industry. It has previously been claimed that the renminbi's official exchange rate was undervalued by as much as 37.5% against its purchasing power parity. However, more recently, appreciation actions by the Chinese government, as well as quantitative easing measures taken by the American Federal Reserve and other major central banks, have caused the renminbi to be within as little as 8% of its equilibrium value by the second half of 2012. Since 2006, the renminbi exchange rate has been allowed to float in a narrow margin around a fixed base rate determined with reference to a basket of world currencies. The Chinese government has announced that it will gradually increase the flexibility of the exchange rate. As a result of the rapid internationalization of the renminbi, it became the world's 8th most traded currency in 2013, 5th by 2015, but 6th in 2019.\nOn 1 October 2016, the renminbi became the first emerging market currency to be included in the IMF's special drawing rights basket, the basket of currencies used by the IMF as a reserve currency. Its initial weighting in the basket was 10.9%.:\u200a259\u200a\n\n\n== Terminology ==\n\nThe ISO code for the renminbi is CNY, the PRC's country code (CN) plus \"Y\" from \"yuan\". Hong Kong markets that trade renminbi at free-floating rates use the unofficial code CNH. This is to distinguish the rates from those fixed by Chinese central banks on the mainland. The abbreviation RMB is not an ISO code but is sometimes used like one by banks and financial institutions.\nThe currency symbol for the yuan unit is \u00a5, but when distinction from the Japanese yen is required RMB (e.g. RMB 10,000) or \u00a5 RMB (e.g. \u00a510,000 RMB) is used. However, in written Chinese contexts, the Chinese character for yuan (Chinese: \u5143; lit. 'constituent', 'part') or, in formal contexts Chinese: \u5706; lit. 'round', usually follows the number in lieu of a currency symbol.\nRenminbi is the name of the currency while yuan is the name of the primary unit of the renminbi. This is analogous to the distinction between \"sterling\" and \"pound\" when discussing the official currency of the United Kingdom. Jiao and fen are also units of renminbi.\nIn everyday Mandarin, kuai (Chinese: \u5757; pinyin: ku\u00e0i; lit. 'piece') is usually used when discussing money and \"renminbi\" or \"yuan\" are rarely heard. Similarly, Mandarin speakers typically use mao (Chinese: \u6bdb; pinyin: m\u00e1o) instead of jiao. For example, \u00a58.74 might be read as \u516b\u5757\u4e03\u6bdb\u56db (pinyin: b\u0101 ku\u00e0i q\u012b m\u00e1o s\u00ec) in everyday conversation, but read \u516b\u5143\u4e03\u89d2\u56db\u5206 (pinyin: b\u0101 yu\u00e1n q\u012b ji\u01ceo s\u00ec f\u0113n) formally.\nRenminbi is sometimes referred to as the \"redback\", a play on \"greenback\", a slang term for the US dollar.\n\n\n== History ==\n\nThe various currencies called yuan or dollar issued in mainland China as well as Taiwan, Hong Kong, Macau and Singapore were all derived from the Spanish dollar, which China imported in large quantities from Spanish America from the 16th to 20th centuries. The first locally minted silver dollar or yuan accepted all over Qing dynasty China (1644\u20131912) was the silver dragon dollar introduced in 1889. Various banknotes denominated in dollars or yuan were also introduced, which were convertible to silver dollars until 1935 when the silver standard was discontinued and the Chinese yuan was made fabi (\u6cd5\u5e01; legal tender fiat currency).\nThe renminbi was introduced by the People's Bank of China in December 1948, about a year before the establishment of the People's Republic of China. It was issued only in paper form at first, and replaced the various currencies circulating in the areas controlled by the Communists. One of the first tasks of the new government was to end the hyperinflation that had plagued China in the final years of the Kuomintang (KMT) era. That achieved, a revaluation occurred in 1955 at the rate of 1 new yuan = 10,000 old yuan.\nAs the Chinese Communist Party took control of ever larger territories in the latter part of the Chinese Civil War, its People's Bank of China began to issue a unified currency in 1948 for use in Communist-controlled territories. Also denominated in yuan, this currency was identified by different names, including \"People's Bank of China banknotes\" (simplified Chinese: \u4e2d\u56fd\u4eba\u6c11\u94f6\u884c\u949e\u7968; traditional Chinese: \u4e2d\u570b\u4eba\u6c11\u9280\u884c\u9214\u7968; from November 1948), \"New Currency\" (simplified Chinese: \u65b0\u5e01; traditional Chinese: \u65b0\u5e63; from December 1948), \"People's Bank of China notes\" (simplified Chinese: \u4e2d\u56fd\u4eba\u6c11\u94f6\u884c\u5238; traditional Chinese: \u4e2d\u570b\u4eba\u6c11\u9280\u884c\u5238; from January 1949), \"People's Notes\" (\u4eba\u6c11\u5238, as an abbreviation of the last name), and finally \"People's Currency\", or \"renminbi\", from June 1949.\n\n\n=== Era of the command economy ===\n\nFrom 1949 until the late 1970s, the state fixed China's exchange rate at a highly overvalued level as part of the country's import-substitution strategy. During this time frame, the focus of the state's central planning was to accelerate industrial development and reduce China's dependence on imported manufactured goods. The overvaluation allowed the government to provide imported machinery and equipment to priority industries at a relatively lower domestic currency cost than otherwise would have been possible.\n\n\n=== Transition to an equilibrium exchange rate ===\nChina's transition by the mid-1990s to a system in which the value of its currency was determined by supply and demand in a foreign exchange market was a gradual process spanning 15 years that involved changes in the official exchange rate, the use of a dual exchange rate system, and the introduction and gradual expansion of markets for foreign exchange.\nThe most important move to a market-oriented exchange rate was an easing of controls on trade and other current account transactions, as occurred in several very early steps. In 1979, the State Council approved a system allowing exporters and their provincial and local government owners to retain a share of their foreign exchange earnings, referred to as foreign exchange quotas. At the same time, the government introduced measures to allow retention of part of the foreign exchange earnings from non-trade sources, such as overseas remittances, port fees paid by foreign vessels, and tourism.\nAs early as October 1980, exporting firms that retained foreign exchange above their own import needs were allowed to sell the excess through the state agency responsible for the management of China's exchange controls and its foreign exchange reserves, the State Administration of Exchange Control. Beginning in the mid-1980s, the government sanctioned foreign exchange markets, known as swap centres, eventually in most large cities.\nThe government also gradually allowed market forces to take the dominant role by introducing an \"internal settlement rate\" of \u00a52.8 to 1 US dollar which was a devaluation of almost 100%.\n\n\n=== Foreign exchange certificates, 1980\u20131994 ===\nIn the process of opening up China to external trade and tourism, transactions with foreign visitors between 1980 and 1994 were done primarily using Foreign exchange certificates (\u5916\u6c47\u5238, waihuiquan) issued by the Bank of China.\nForeign currencies were exchangeable for FECs and vice versa at the renminbi's prevailing official rate which ranged from US$1 = \u00a52.8 FEC to \u00a55.5 FEC. The FEC was issued as banknotes from \u00a50.1 to \u00a5100, and was officially at par with the renminbi. Tourists used FECs to pay for accommodation as well as tourist and luxury goods sold in Friendship Stores. However, given the non-availability of foreign exchange and Friendship Store goods to the general public, as well as the inability of tourists to use FECs at local businesses, an illegal black market developed for FECs where touts approached tourists outside hotels and offered over \u00a51.50 RMB in exchange for \u00a51 FEC. In 1994, as a result of foreign exchange management reforms approved by the 14th CPC Central Committee, the renminbi was officially devalued from US$1 = \u00a55.5 to over \u00a58, and the FEC was retired at \u00a51 FEC = \u00a51 RMB in favour of tourists directly using the renminbi.\n\n\n=== Evolution of exchange policy since 1994 ===\nIn November 1993, the Third Plenum of the Fourteenth CPC Central Committee approved a comprehensive reform strategy in which foreign exchange management reforms were highlighted as a key element for a market-oriented economy. A floating exchange rate regime and convertibility for renminbi were seen as the ultimate goal of the reform. Conditional convertibility under current account was achieved by allowing firms to surrender their foreign exchange earning from current account transactions and purchase foreign exchange as needed. Restrictions on Foreign Direct Investment (FDI) was also loosened and capital inflows to China surged.\n\n\n=== Convertibility ===\nDuring the era of the command economy, the value of the renminbi was set to unrealistic values in exchange with Western currency and severe currency exchange rules were put in place, hence the dual-track currency system from 1980 to 1994 with the renminbi usable only domestically, and with Foreign Exchange Certificates (FECs) used by foreign visitors.\nIn the late 1980s and early 1990s, China worked to make the renminbi more convertible. Through the use of swap centres, the exchange rate was eventually brought to more realistic levels of above \u00a58/US$1 in 1994 and the FEC was discontinued. It stayed above \u00a58/$1 until 2005 when the renminbi's peg to the dollar was loosened and it was allowed to appreciate.\nAs of 2013, the renminbi is convertible on current accounts but not capital accounts. The ultimate goal has been to make the renminbi fully convertible. However, partly in response to the Asian financial crisis in 1998, China has been concerned that the Chinese financial system would not be able to handle the potential rapid cross-border movements of hot money, and as a result, as of 2012, the currency trades within a narrow band specified by the Chinese central government.\nFollowing the internationalization of the renminbi, on 30 November 2015, the IMF voted to designate the renminbi as one of several main world currencies, thus including it in the basket of special drawing rights. The renminbi became the first emerging market currency to be included in the IMF's SDR basket on 1 October 2016. The other main world currencies are the dollar, the euro, sterling, and the yen.\n\n\n=== Digital renminbi ===\n\nIn October 2019, China's central bank, PBOC, announced that a digital renminbi was going to be released after years of preparation. This version of the currency, also called DCEP (Digital Currency Electronic Payment), can be \u201cdecoupled\u201d from the banking system to give visiting tourists a taste of the nation's burgeoning cashless society.  The announcement received a variety of responses: some believe it is more about domestic control and surveillance. Some argue that the real barriers to internationalisation of the renminbi are China's capital controls, which it has no plans to remove. Maximilian K\u00e4rnfelt, an expert at the Mercator Institute for China Studies, said that a digital renminbi \"would not banish many of the problems holding the renminbi back from more use globally\". He went on to say, \"Much of China's financial market is still not open to foreigners and property rights remain fragile.\"\nThe PBOC has filed more than 80 patents surrounding the integration of a digital currency system, choosing to embrace the blockchain technology. The patents reveal the extent of China's digital currency plans. The patents, seen and verified by the Financial Times, include proposals related to the issuance and supply of a central bank digital currency, a system for interbank settlements that uses the currency, and the integration of digital currency wallets into existing retail bank accounts. Several of the 84 patents reviewed by the Financial Times indicate that China may plan to algorithmically adjust the supply of a central bank digital currency based on certain triggers, such as loan interest rates. Other patents are focused on building digital currency chip cards or digital currency wallets that banking consumers could potentially use, which would be linked directly to their bank accounts. The patent filings also point to the proposed \u2018tokenomics\u2019 being considered by the DCEP working group. Some patents show plans towards programmed inflation control mechanisms. While the majority of the patents are attributed to the PBOC's Digital Currency Research Institute, some are attributed to state-owned corporations or subsidiaries of the Chinese central government.\nUncovered by the Chamber of Digital Commerce (an American non-profit advocacy group), their contents shed light on Beijing's mounting efforts to digitise the renminbi, which has sparked alarm in the West and spurred central bankers around the world to begin exploring similar projects. Some commentators have said that the U.S., which has no current plans to issue a government-backed digital currency, risks falling behind China and risking its dominance in the global financial system. Victor Shih, a China expert and professor at the University of California San Diego, said that merely introducing a digital currency \"doesn't solve the problem that some people holding renminbi offshore will want to sell that renminbi and exchange it for the dollar\", as the dollar is considered to be a safer asset. Eswar Prasad, an economics professor at Cornell University, said that the digital renminbi \"will hardly put a dent in the dollar's status as the dominant global reserve currency\" due to the United States' \"economic dominance, deep and liquid capital markets, and still-robust institutional framework\". The U.S. dollar's share as a reserve currency is above 60%, while that of the renminbi is about 2%.\nIn April 2020, The Guardian reported that the digital currency e-RMB had been adopted into multiple cities' monetary systems and \"some government employees and public servants [will] receive their salaries in the digital currency from May. The Guardian quoted a China Daily report which stated \"A sovereign digital currency provides a functional alternative to the dollar settlement system and blunts the impact of any sanctions or threats of exclusion both at a country and company level. It may also facilitate integration into globally traded currency markets with a reduced risk of politically inspired disruption.\" There were talks of testing out the digital renminbi in the Beijing Winter Olympics in 2022, but China's overall timetable for rolling out the digital currency was unclear.\n\n\n=== Interest rate & Green bonds ===\nIn May 2023, RMB interest rate swaps was launched. In June 2023, under the Government Green Bond Programme, the Government of the Hong Kong Special Administrative Region of the People's Republic of China (HKSAR) announced a green bonds offering, of approximately US$6 billion denominated in USD, EUR and RMB.\n\n\n== Issuance ==\n\nAs of 2019, renminbi banknotes are available in denominations of \u00a50.1, \u00a50.5 (1 and 5 jiao), \u00a51, \u00a55, \u00a510, \u00a520, \u00a550 and \u00a5100. These denominations have been available since 1955, except for the \u00a520 notes (added in 1999 with the fifth series) \u00a550 and \u00a5100 notes (added in 1987 with the fourth series). Coins are available in denominations from \u00a50.01 to \u00a51 (\u00a50.01\u20131). Thus some denominations exist in both coins and banknotes. On rare occasions, larger yuan coin denominations such as \u00a55 have been issued to commemorate events but use of these outside of collecting has never been widespread.\nThe denomination of each banknote is printed in simplified written Chinese. The numbers themselves are printed in financial Chinese numeral characters, as well as Arabic numerals. The denomination and the words \"People's Bank of China\" are also printed in Mongolian, Tibetan, Uyghur and Zhuang on the back of each banknote, in addition to the boldface Hanyu Pinyin \"Zhongguo Renmin Yinhang\" (without tones). The right front of the note has a tactile representation of the denomination in Chinese Braille starting from the fourth series. See corresponding section for detailed information.\nThe fen and jiao denominations have become increasingly unnecessary as prices have increased. Coins under \u00a50.1 are used infrequently. Chinese retailers tend to avoid fractional values (such as \u00a59.99), opting instead to round to the nearest yuan (such as \u00a59 or \u00a510).\n\n\n=== Coins ===\nIn 1953, aluminium \u00a50.01, \u00a50.02, and \u00a50.05 coins began being struck for circulation, and were first introduced in 1955. These depict the national emblem on the obverse (front) and the name and denomination framed by wheat stalks on the reverse (back). In 1980, brass \u00a50.1, \u00a50.2, and \u00a50.5 and cupro-nickel \u00a51 coins were added, although the \u00a50.1 and \u00a50.2 were only produced until 1981, with the last \u00a50.5 and \u00a51 issued in 1985. All ji\u01ceo coins depicted similar designs to the f\u0113n coins while the yu\u00e1n depicted the Great Wall of China.\nIn 1991, a new coinage was introduced, consisting of an aluminium \u00a50.1, brass \u00a50.5 and nickel-clad steel \u00a51. These were smaller than the previous ji\u01ceo and yu\u00e1n coins and depicted flowers on the obverse and the national emblem on the reverse. Issuance of the aluminium \u00a50.01 and \u00a50.02 coins ceased in 1991, with that of the \u00a50.05 halting in 1994. The small coins were still struck for annual uncirculated mint sets in limited quantities, and from the beginning of 2005, the \u00a50.01 coin got a new lease on life by being issued again every year since then up to present.\nNew designs of the \u00a50.1, \u00a50.5 (now brass-plated steel), and \u00a51 (nickel-plated steel) were again introduced in between 1999 and 2002. The \u00a50.1 was significantly reduced in size, and in 2005 its composition was changed from aluminium to more durable nickel-plated steel. An updated version of these coins was announced in 2019. While the overall design is unchanged, all coins including the \u00a50.5 are now of nickel-plated steel, and the \u00a51 coin was reduced in size.\nThe frequency of usage of coins varies between different parts of China, with coins typically being more popular in urban areas (with 5-ji\u01ceo and 1-yu\u00e1n coins used in vending machines), and small notes being more popular in rural areas. Older f\u0113n and large ji\u01ceo coins are uncommonly still seen in circulation, but are still valid in exchange.\n\n\n=== Banknotes ===\nAs of 2023, there have been five series of renminbi banknotes issued by the People's Republic of China:\n\nThe first series of renminbi banknotes was issued on 1 December 1948, by the newly founded People's Bank of China. It introduced notes in denominations of \u00a51, \u00a55, \u00a510, \u00a520, \u00a550, \u00a5100 and \u00a51,000 yuan. Notes for \u00a5200, \u00a5500, \u00a55,000 and \u00a510,000 followed in 1949, with \u00a550,000 notes added in 1950. A total of 62 different designs were issued. The notes were officially withdrawn on various dates between 1 April and 10 May 1955. The name \"first series\" was given retroactively in 1950, after work began to design a new series.\nThese first renminbi notes were printed with the words \"People's Bank of China\", \"Republic of China\", and the denomination, written in Chinese characters by Dong Biwu.\nThe second series of renminbi banknotes was introduced on 1 March 1955 (but dated 1953). Each note has the words \"People's Bank of China\" as well as the denomination in the Uyghur, Tibetan, Mongolian and Zhuang languages on the back, which has since appeared in each series of renminbi notes. The denominations available in banknotes were \u00a50.01, \u00a50.02, \u00a50.05, \u00a50.1, \u00a50.2, \u00a50.5, \u00a51, \u00a52, \u00a53, \u00a55 and \u00a510. Except for the three fen denominations and the \u00a53 which were withdrawn, notes in these denominations continued to circulate. Good examples of this series have gained high status with banknote collectors.\nThe third series of renminbi banknotes was introduced on 15 April 1962, though many denominations were dated 1960. New dates would be issued as stocks of older dates were gradually depleted. The sizes and design layout of the notes had changed but not the order of colours for each denomination. For the next two decades, the second and third series banknotes were used concurrently. The denominations were of \u00a50.1, \u00a50.2, \u00a50.5, \u00a51, \u00a52, \u00a55 and \u00a510. The third series was phased out during the 1990s and then was recalled completely on 1 July 2000.\nThe fourth series of renminbi banknotes was introduced between 1987 and 1997, although the banknotes were dated 1980, 1990, or 1996. They were withdrawn from circulation on 1 May 2019. Banknotes are available in denominations of \u00a50.1, \u00a50.2, \u00a50.5, \u00a51, \u00a52, \u00a55, \u00a510, \u00a550 and \u00a5100. Like previous issues, the colour designation for already existing denominations remained in effect. The second to fourth series of renminbi banknotes were designed by professors at the Central Academy of Art including Luo Gongliu and Zhou Lingzhao.\nThe fifth series of renminbi banknotes and coins was progressively introduced from its introduction in 1999. This series also bears the issue years 2005 (all except \u00a51), 2015 (\u00a5100 only) and 2019 (\u00a51, \u00a510, \u00a520 and \u00a550). As of 2019, it includes banknotes for \u00a51, \u00a55, \u00a510, \u00a520, \u00a550 and \u00a5100. Significantly, the fifth series uses the portrait of Chinese Communist Party chairman Mao Zedong on all banknotes, in place of the various leaders, workers and representations of China's ethnic groups which had been featured previously. During this series new security features were added, the \u00a52 denomination was discontinued, the colour pattern for each note was changed and a new denomination of \u00a520 was introduced for this series. A revised series of coins of \u00a50.1, \u00a50.5 and \u00a51 and banknotes of \u00a51, \u00a510, \u00a520 and \u00a550 were issued for general circulation on 30 August 2019. The \u00a55 banknote of the fifth series was issued in November 2020 with new printing technology in a bid to reduce counterfeiting of Chinese currency.\n\n\n=== Commemorative issues of the renminbi banknotes ===\n\nIn 1999, a commemorative red \u00a550 note was issued in honour of the 50th anniversary of the establishment of the People's Republic of China. This note features Chinese Communist Party chairman Mao Zedong on the front and various animals on the back.\nAn orange polymer note, commemorating the new millennium was issued in 2000 with a face value of \u00a5100. This features a dragon on the obverse and the reverse features the China Millennium monument (at the Center for Cultural and Scientific Fairs).\nFor the 2008 Beijing Olympics, a green \u00a510 note was issued featuring the Bird's Nest Stadium on the front with the back showing a classical Olympic discus thrower and various other athletes.\nOn 26 November 2015, the People's Bank of China issued a blue \u00a5100 commemorative note to commemorate aerospace science and technology.\nIn commemoration of the 70th Anniversary of the issuance of the Renminbi, the People's Bank of China issued 120 million \u00a550 banknotes on 28 December 2018.\nIn commemoration of the 2022 Winter Olympics, the People's Bank of China issued \u00a520 commemorative banknotes in both paper and polymer in December 2021.\nIn commemoration of the 2024 Chinese New Year, the People's Bank of China issued \u00a520 commemorative banknotes in polymer in January 2024.\n\n\n=== Use in ethnic minority regions of China ===\n\nThe renminbi yuan has different names when used in ethnic minority regions of China. \n\nWhen used in Inner Mongolia and other Mongol autonomies, a yuan is called a tugreg (Mongolian: \u1832\u1826\u182d\u1826\u1837\u1822\u182d\u180c, \u0442\u04e9\u0433\u0440\u04e9\u0433 t\u00fcg\u00fcrig). However, when used in the republic of Mongolia, it is still named yuani (Mongolian: \u044e\u0430\u043d\u044c) to differentiate it from Mongolian t\u00f6gr\u00f6g (Mongolian: \u0442\u04e9\u0433\u0440\u04e9\u0433). One Chinese t\u00fcg\u00fcrig (tugreg) is divided into 100 m\u00f6ngg\u00fc (Mongolian: \u182e\u1825\u1829\u182d\u1826, \u043c\u04e9\u043d\u0433\u04e9), one Chinese jiao is labeled \"10 m\u00f6ngg\u00fc\". In Mongolian, renminbi is called aradin jogos or arad-un jogos (Mongolian: \u1820\u1837\u1820\u1833\u202f\u1824\u1828 \u1835\u1823\u182d\u1823\u1830, \u0430\u0440\u0434\u044b\u043d \u0437\u043e\u043e\u0441 arad-un \u01f0o\u03b3os).\nWhen used in Tibet and other Tibetan autonomies, a yuan is called a gor (Tibetan: \u0f66\u0f92\u0f7c\u0f62\u0f0b, ZYPY: Gor). One gor is divided into 10 gorsur (Tibetan: \u0f66\u0f92\u0f7c\u0f62\u0f0b\u0f5f\u0f74\u0f62\u0f0b, ZYPY: Gorsur) or 100 gar (Tibetan: \u0f66\u0f90\u0f62\u0f0b, ZYPY: gar). In Tibetan, renminbi is called mimangxogng\u00fc (Tibetan: \u0f58\u0f72\u0f0b\u0f51\u0f58\u0f44\u0f66\u0f0b\u0f64\u0f7c\u0f42\u0f0b\u0f51\u0f44\u0f74\u0f63\u0f0d, ZYPY: Mimang Xogng\u00fc) or mimang shog ngul.\nWhen used in the Uyghur autonomous region of Xinjiang, the renminbi is called Xelq puli (Uyghur: \u062e\u06d5\u0644\u0642 \u067e\u06c7\u0644\u0649)\n\n\n=== Production and minting ===\nRenminbi currency production is carried out by a state owned corporation, China Banknote Printing and Minting Corporation (CBPMC; \u4e2d\u56fd\u5370\u949e\u9020\u5e01\u603b\u516c\u53f8) headquartered in Beijing. CBPMC uses several printing, engraving and minting facilities around the country to produce banknotes and coins for subsequent distribution. Banknote printing facilities are based in Beijing, Shanghai, Chengdu, Xi'an, Shijiazhuang, and Nanchang. Mints are located in Nanjing, Shanghai, and Shenyang. Also, high grade paper for the banknotes is produced at two facilities in Baoding and Kunshan. The Baoding facility is the largest facility in the world dedicated to developing banknote material according to its website. In addition, the People's Bank of China has its own printing technology research division that researches new techniques for creating banknotes and making counterfeiting more difficult.\n\n\n=== Suggested future design ===\nOn 13 March 2006, some delegates to an advisory body at the National People's Congress proposed to include Sun Yat-sen and Deng Xiaoping on the renminbi banknotes. However, the proposal was not adopted.\n\n\n== Economics ==\n\n\n=== Value ===\n\nFor most of its early history, the renminbi was pegged to the U.S. dollar at \u00a52.46 per dollar. During the 1970s, it was revalued until it reached \u00a51.50 per dollar in 1980. When China's economy gradually opened in the 1980s, the renminbi was devalued in order to improve the competitiveness of Chinese exports. Thus, the official exchange rate increased from \u00a51.50 in 1980 to \u00a58.62 by 1994 (the lowest rate on record). Improving current account balance during the latter half of the 1990s enabled the Chinese government to maintain a peg of \u00a58.27 per US$1 from 1997 to 2005.\nThe renminbi reached a record high exchange value of \u00a56.0395 to the US dollar on 14 January 2014. Chinese leadership have been raising the yuan to tame inflation, a step U.S. officials have pushed for years to lower the massive trade deficit with China. Strengthening the value of the renminbi also fits with the Chinese transition to a more consumer-led economic growth model. \nIn 2015 the People's Bank of China again devalued their country's currency. As of 1 September 2015, the exchange rate for US$1 is \u00a56.38.\n\n\n=== Depegged from the US dollar ===\nOn 21 July 2005, the peg was finally lifted, which saw an immediate one-time renminbi revaluation to \u00a58.11 per dollar. The exchange rate against the euro stood at \u00a510.07060 per euro.\nHowever, the peg was reinstituted unofficially when the financial crisis hit: \"Under intense pressure from Washington, China took small steps to allow its currency to strengthen for three years starting in July 2005. But China 're-pegged' its currency to the dollar as the financial crisis intensified in July 2008.\"\nOn 19 June 2010, the People's Bank of China released a statement simultaneously in Chinese and English claiming that they would \"proceed further with reform of the renminbi exchange rate regime and increase the renminbi exchange rate flexibility\". The news was greeted with praise by world leaders including Barack Obama, Nicolas Sarkozy and Stephen Harper. The PBoC maintained there would be no \"large swings\" in the currency. The renminbi rose to its highest level in five years and markets worldwide surged on Monday, 21 June following China's announcement.\nIn August 2015, Joseph Adinolfi, a reporter for MarketWatch, reported that China had re-pegged the renminbi. In his article, he narrated that \"Weak trade data out of China, released over the weekend, weighed on the currencies of Australia and New Zealand on Monday. But the yuan didn't budge. Indeed, the Chinese currency, also known as the renminbi, has been remarkably steady over the past month despite the huge selloff in China's stock market and a spate of disappointing economic data. Market strategists, including Simon Derrick, chief currency strategist at BNY Mellon, and Marc Chandler, head currency strategist at Brown Brothers Harriman, said that is because China's policy makers have effectively re-pegged the yuan. \u201cWhen I look at the dollar-renminbi right now, that looks like a fixed exchange rate again. They\u2019ve re-pegged it,\u201d Chandler said.\"\n\n\n=== Managed float ===\nThe renminbi has now moved to a managed floating exchange rate based on market supply and demand with reference to a basket of foreign currencies. In July 2005, the daily trading price of the US dollar against the renminbi in the inter-bank foreign exchange market was allowed to float within a narrow band of 0.3% around the central parity published by the People's Bank of China; in a later announcement published on 18 May 2007, the band was extended to 0.5%. On 14 April 2012, the band was extended to 1.0%. On 17 March 2014, the band was extended to 2%. China has stated that the basket is dominated by the United States dollar, euro, Japanese yen and South Korean won, with a smaller proportion made up of sterling, Thai baht, roubles, Australian dollars, Canadian dollars and Singaporean dollars.\nOn 10 April 2008, it traded at \u00a56.9920 per US dollar, which was the first time in more than a decade that a dollar had bought less than \u00a57, and at \u00a511.03630 per euro.\nBeginning in January 2010, Chinese and non-Chinese citizens have an annual exchange limit of a maximum of US$50,000. Exchanges within this limit require only a passport or Chinese ID and no additional documentation showing the purpose of the exchange. Currency exchange transactions are centrally registered. The maximum dollar withdrawal is $10,000 per day, the maximum purchase limit of US dollars is $500 per day. This stringent management of the currency leads to a bottled-up demand for exchange in both directions. It is viewed as a major tool to keep the currency peg, preventing inflows of \"hot money\".\nA shift of Chinese reserves into the currencies of their other trading partners has caused these nations to shift more of their reserves into dollars, leading to no great change in the value of the renminbi against the dollar.\n\n\n=== Futures market ===\nRenminbi futures are traded at the Chicago Mercantile Exchange. The futures are cash-settled at the exchange rate published by the People's Bank of China.\n\n\n=== Purchasing power parity ===\nScholarly studies suggest that the yuan is undervalued on the basis of purchasing power parity analysis. One 2011 study suggests a 37.5% undervaluation.\n\nThe World Bank estimated that, by purchasing power parity, one International dollar was equivalent to approximately \u00a51.9 in 2004.\nThe International Monetary Fund estimated that, by purchasing power parity, one International dollar was equivalent to approximately \u00a53.462 in 2006, \u00a53.621 in 2007, \u00a53.798 in 2008, \u00a53.872 in 2009, \u00a53.922 in 2010, \u00a53.946 in 2011, \u00a53.952 in 2012, \u00a53.944 in 2013 and \u00a53.937 in 2014.\nThe People's Bank of China lowered the renminbi's daily fix to the US dollar by 1.9 per cent to \u00a56.2298 on 11 August 2015. The People's Bank of China again lowered the renminbi's daily fix to the US dollar from \u00a56.620 to \u00a56.6375 after Brexit on 27 June 2016. It had not been this low since December 2010.\n\n\n=== Internationalisation ===\n\nBefore 2009, the renminbi had little to no exposure in the international markets because of strict government controls by the central Chinese government that prohibited almost all export of the currency, or use of it in international transactions. Transactions between Chinese companies and a foreign entity were generally denominated in US dollars. With Chinese companies unable to hold US dollars and foreign companies unable to hold Chinese yuan, all transactions would go through the People's Bank of China. Once the sum was paid by the foreign party in dollars, the central bank would pass the settlement in renminbi to the Chinese company at the state-controlled exchange rate.\nIn June 2009 the Chinese officials announced a pilot scheme where business and trade transactions were allowed between limited businesses in Guangdong province and Shanghai, and only counterparties in Hong Kong, Macau, and select ASEAN nations. Proving a success, the program was further extended to 20 Chinese provinces and counterparties internationally in July 2010, and in September 2011 it was announced that the remaining 11 Chinese provinces would be included.\nIn steps intended to establish the renminbi as an international reserve currency, China has agreements with Russia, Vietnam, Sri Lanka, Thailand, and Japan, allowing trade with those countries to be settled directly in renminbi instead of requiring conversion to US dollars, with Australia and South Africa to follow soon. In September 2023, the Renminbi passed the euro as the second most utilized currency in international trade, having tripled in the last three years.\n\n\n=== International reserve currency ===\nCurrency restrictions regarding renminbi-denominated bank deposits and financial products were greatly liberalised in July 2010. In 2010 renminbi-denominated bonds were reported to have been purchased by Malaysia's central bank and that McDonald's had issued renminbi denominated corporate bonds through Standard Chartered Bank of Hong Kong. Such liberalisation allows the yuan to look more attractive as it can be held with higher return on investment yields, whereas previously that yield was virtually none. Nevertheless, some national banks such as Bank of Thailand (BOT) have expressed a serious concern about renminbi since BOT cannot substitute the deprecated US dollars in its US$200 billion foreign exchange reserves for renminbi as much as it wishes because:\n\nThe Chinese government has not taken full responsibilities and commitments on economic affairs at global levels.\nThe renminbi still has not become well-liquidated (fully convertible) yet.\nThe Chinese government still lacks deep and wide vision about how to perform fund-raising to handle international loans at global levels.\nTo meet IMF requirements, China gave up some of its tight control over the currency.\nCountries that are left-leaning in the political spectrum had already begun to use the renminbi as an alternative reserve currency to the United States dollar; the Central Bank of Chile reported in 2011 to have US$91 million worth of renminbi in reserves, and the president of the Central Bank of Venezuela, Nelson Merentes, made statements in favour of the renminbi following the announcement of reserve withdrawals from Europe and the United States.\nIn Africa, the central banks of Ghana, Nigeria, and South Africa either hold renminbi as a reserve currency or have taken steps to purchase bonds denominated in renminbi. The \"Report on the Internationalization of RMB in 2020\", which was released by the People's Bank of China in August 2020, said that renminbi's function as international reserve currency has gradually emerged. In the first quarter 2020, the share of renminbi in global foreign exchange reserves rose to 2.02%, a record high. As of the end of 2019, the People's Bank of China has set up renminbi clearing banks in 25 countries and regions outside of mainland China, which has made the use of renminbi more secure and transaction costs have decreased.\n\n\n=== Use as a currency outside mainland China ===\n\nThe two special administrative regions, Hong Kong and Macau, have their own respective currencies, according to the \"one country, two systems\" principle and the basic laws of the two territories. Therefore, the Hong Kong dollar and the Macanese pataca remain the legal tenders in the two territories, and the renminbi, although sometimes accepted, is not legal tender. Banks in Hong Kong allow people to maintain accounts in RMB. Because of changes in legislation in July 2010, banks around the world offer foreign currency accounts for deposits in Chinese renminbi.\nThe renminbi had a presence in Macau even before the 1999 return to the People's Republic of China from Portugal. Banks in Macau can issue credit cards based on the renminbi, but not loans. Renminbi-based credit cards cannot be used in Macau's casinos.\nThe Republic of China, which governs Taiwan, believes wide usage of the renminbi would create an underground economy and undermine its sovereignty. Tourists are allowed to bring in up to \u00a520,000 when visiting Taiwan. These renminbi must be converted to Taiwanese currency at trial exchange sites in Matsu and Kinmen. The Chen Shui-bian administration insisted that it would not allow full convertibility until the mainland signs a bilateral foreign exchange settlement agreement, though president Ma Ying-jeou, who served from 2008 to 2016, sought to allow full convertibility as soon as possible.\nThe renminbi circulates in some of China's neighbors, such as Pakistan, Mongolia and northern Thailand. Cambodia welcomes the renminbi as an official currency and Laos and Myanmar allow it in border provinces such as Wa and Kokang and economic zones like Mandalay. Though unofficial, Vietnam recognizes the exchange of the renminbi to the \u0111\u1ed3ng. In 2017 \u00a5215 billion was circulating in Indonesia. In 2018 a Bilateral Currency Swap Agreement was made by the Bank of Indonesia and the Bank of China which simplified business transactions, and in 2020 about 10% of Indonesia's global trade was in renminbi.\nSince 2007, renminbi-nominated bonds have been issued outside mainland China; these are colloquially called \"dim sum bonds\". In April 2011, the first initial public offering denominated in renminbi occurred in Hong Kong, when the Chinese property investment trust Hui Xian REIT raised \u00a510.48 billion ($1.6 billion) in its IPO. Beijing has allowed renminbi-denominated financial markets to develop in Hong Kong as part of the effort to internationalise the renminbi. There is limited (under 1%) issuing of renminbi bonds in Indonesia.\n\n\n=== Other markets ===\n\nSince currency flows in and out of mainland China are still restricted, renminbi traded in off-shore markets, such as the Hong Kong market, can have a different value to renminbi traded on the mainland. The offshore RMB market is usually denoted as CNH, but there is another renminbi interbank and spot market in Taiwan for domestic trading known as CNT.\nOther renminbi markets include the dollar-settled non-deliverable forward (NDF), and the trade-settlement exchange rate (CNT).\nNote that the two CNTs mentioned above are different from each other.\n\n\n== See also ==\nChinese lunar coins\nEconomy of China\nList of Chinese cash coins by inscription\nList of renminbi exchange rates\nTibetan coins and currency\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Further reading ==\n\n\n== External links ==\n\nPhotographs of all Chinese currency and sound of pronunciation in Chinese (archived 11 March 2012)\nStephen Mulvey, Why China's currency has two names \u2013 BBC News, 2010-06-26\nHistorical and current banknotes of the People's Republic of China (in English and German)\nForeign exchange certificates (FEC) of the People's Republic of China (in English and German)",
        "unit": "chinese yuan",
        "url": "https://en.wikipedia.org/wiki/Chinese_yuan"
    },
    {
        "_id": "Celsius",
        "clean": "Celsius",
        "text": "The degree Celsius is the unit of temperature on the Celsius temperature scale (originally known as the centigrade scale outside Sweden), one of two temperature scales used in the International System of Units (SI), the other being the closely related Kelvin scale. The degree Celsius (symbol: \u00b0C) can refer to a specific point on the Celsius temperature scale or to a difference or range between two temperatures. It is named after the Swedish astronomer Anders Celsius (1701\u20131744), who proposed the first version of it in 1742. The unit was called centigrade in several languages (from the Latin centum, which means 100, and gradus, which means steps) for many years. In 1948, the International Committee for Weights and Measures renamed it to honor Celsius and also to remove confusion with the term for one hundredth of a gradian in some languages. Most countries use this scale (the Fahrenheit scale is still used in the United States, some island territories, and Liberia).\nThroughout the 19th century, the scale was based on 0 \u00b0C for the freezing point of water and 100 \u00b0C for the boiling point of water at 1 atm pressure. (In Celsius's initial proposal, the values were reversed: the boiling point was 0 degrees and the freezing point was 100 degrees.)\n\nBetween 1954 and 2019, the precise definitions of the unit degree Celsius and the Celsius temperature scale used absolute zero and the triple point of water. Since 2007, the Celsius temperature scale has been defined in terms of the kelvin, the SI base unit of thermodynamic temperature (symbol: K). Absolute zero, the lowest temperature, is now defined as being exactly 0 K and \u2212273.15 \u00b0C.\n\n\n== History ==\n\nIn 1742, Swedish astronomer Anders Celsius (1701\u20131744) created a temperature scale that was the reverse of the scale now known as \"Celsius\": 0 represented the boiling point of water, while 100 represented the freezing point of water. In his paper Observations of two persistent degrees on a thermometer, he recounted his experiments showing that the melting point of ice is essentially unaffected by pressure. He also determined with remarkable precision how the boiling point of water varied as a function of atmospheric pressure. He proposed that the zero point of his temperature scale, being the boiling point, would be calibrated at the mean barometric pressure at mean sea level. This pressure is known as one standard atmosphere. The BIPM's 10th General Conference on Weights and Measures (CGPM) in 1954 defined one standard atmosphere to equal precisely 1,013,250 dynes per square centimeter (101.325 kPa).\nIn 1743, the Lyonnais physicist Jean-Pierre Christin, permanent secretary of the Academy of Lyon, inverted the Celsius temperature scale so that 0 represented the freezing point of water and 100 represented the boiling point of water. Some credit Christin for independently inventing the reverse of Celsius's original scale, while others believe Christin merely reversed Celsius's scale. On 19 May 1743 he published the design of a mercury thermometer, the \"Thermometer of Lyon\" built by the craftsman Pierre Casati that used this scale.\nIn 1744, coincident with the death of Anders Celsius, the Swedish botanist Carl Linnaeus (1707\u20131778) reversed Celsius's scale. His custom-made \"Linnaeus-thermometer\", for use in his greenhouses, was made by Daniel Ekstr\u00f6m, Sweden's leading maker of scientific instruments at the time, whose workshop was located in the basement of the Stockholm observatory. As often happened in this age before modern communications, numerous physicists, scientists, and instrument makers are credited with having independently developed this same scale; among them were Pehr Elvius, the secretary of the Royal Swedish Academy of Sciences (which had an instrument workshop) and with whom Linnaeus had been corresponding; Daniel Ekstr\u00f6m, the instrument maker; and M\u00e5rten Str\u00f6mer (1707\u20131770) who had studied astronomy under Anders Celsius.\nThe first known Swedish document reporting temperatures in this modern \"forward\" Celsius temperature scale is the paper Hortus Upsaliensis dated 16 December 1745 that Linnaeus wrote to a student of his, Samuel Naucl\u00e9r. In it, Linnaeus recounted the temperatures inside the orangery at the University of Uppsala Botanical Garden:\n\n... since the caldarium (the hot part of the greenhouse) by the angle of the windows, merely from the rays of the sun, obtains such heat that the thermometer often reaches 30 degrees, although the keen gardener usually takes care not to let it rise to more than 20 to 25 degrees, and in winter not under 15 degrees ...\n\n\n=== \"Centigrade\" versus \"Celsius\" ===\nSince the 19th century, the scientific and thermometry communities worldwide have used the phrase \"centigrade scale\" and temperatures were often reported simply as \"degrees\" or, when greater specificity was desired, as \"degrees centigrade\", with the symbol \u00b0C.\nIn the French language, the term centigrade also means one hundredth of a gradian, when used for angular measurement. The term centesimal degree was later introduced for temperatures but was also problematic, as it means gradian (one hundredth of a right angle) in the French and Spanish languages. The risk of confusion between temperature and angular measurement was eliminated in 1948 when the 9th meeting of the General Conference on Weights and Measures and the Comit\u00e9 International des Poids et Mesures (CIPM) formally adopted \"degree Celsius\" for temperature.\nWhile \"Celsius\" is commonly used in scientific work, \"centigrade\" is still used in French and English-speaking countries, especially in informal contexts. The frequency of the usage of \"centigrade\" has declined over time.\nDue to metrication in Australia, after 1 September 1972 weather reports in the country were exclusively given in Celsius. In the United Kingdom, it was not until February 1985 that forecasts by BBC Weather switched from \"centigrade\" to \"Celsius\".\n\n\n== Common temperatures ==\n\nAll phase transitions are at standard atmosphere. Figures are either by definition, or approximated from empirical measurements.\n\n\n== Name and symbol typesetting ==\nThe \"degree Celsius\" has been the only SI unit whose full unit name contains an uppercase letter since 1967, when the SI base unit for temperature became the kelvin, replacing the capitalized term degrees Kelvin. The plural form is \"degrees Celsius\".\nThe general rule of the International Bureau of Weights and Measures (BIPM) is that the numerical value always precedes the unit, and a space is always used to separate the unit from the number, e.g. \"30.2 \u00b0C\" (not \"30.2\u00b0C\" or \"30.2\u00b0 C\"). The only exceptions to this rule are for the unit symbols for degree, minute, and second for plane angle (\u00b0, \u2032, and \u2033, respectively), for which no space is left between the numerical value and the unit symbol. Other languages, and various publishing houses, may follow different typographical rules.\n\n\n=== Unicode character ===\nUnicode provides the Celsius symbol at code point U+2103 \u2103 DEGREE CELSIUS. However, this is a compatibility character provided for roundtrip compatibility with legacy encodings. It easily allows correct rendering for vertically written East Asian scripts, such as Chinese. The Unicode standard explicitly discourages the use of this character: \"In normal use, it is better to represent degrees Celsius '\u00b0C' with a sequence of U+00B0 \u00b0 DEGREE SIGN + U+0043 C LATIN CAPITAL LETTER C, rather than U+2103 \u2103 DEGREE CELSIUS. For searching, treat these two sequences as identical.\"\n\n\n== Temperatures and intervals ==\nThe degree Celsius is subject to the same rules as the kelvin with regard to the use of its unit name and symbol. Thus, besides expressing specific temperatures along its scale (e.g. \"Gallium melts at 29.7646 \u00b0C\" and \"The temperature outside is 23 degrees Celsius\"), the degree Celsius is also suitable for expressing temperature intervals: differences between temperatures or their uncertainties (e.g. \"The output of the heat exchanger is hotter by 40 degrees Celsius\", and \"Our standard uncertainty is \u00b13 \u00b0C\"). Because of this dual usage, one must not rely upon the unit name or its symbol to denote that a quantity is a temperature interval; it must be unambiguous through context or explicit statement that the quantity is an interval. This is sometimes solved by using the symbol \u00b0C (pronounced \"degrees Celsius\") for a temperature, and C\u00b0 (pronounced \"Celsius degrees\") for a temperature interval, although this usage is non-standard. Another way to express the same is \"40 \u00b0C \u00b1 3 K\", which can be commonly found in literature.\nCelsius measurement follows an interval system but not a ratio system; and it follows a relative scale not an absolute scale. For example, an object at 20 \u00b0C does not have twice the energy of when it is 10 \u00b0C; and 0 \u00b0C is not the lowest Celsius value. Thus, degrees Celsius is a useful interval measurement but does not possess the characteristics of ratio measures like weight or distance.\n\n\n== Coexistence with Kelvin ==\nIn science and in engineering, the Celsius and Kelvin scales are often used in combination in close contexts, e.g. \"a measured value was 0.01023 \u00b0C with an uncertainty of 70 \u03bcK\". This practice is permissible because the magnitude of the degree Celsius is equal to that of the kelvin. Notwithstanding the official endorsement provided by decision no. 3 of Resolution 3 of the 13th CGPM, which stated \"a temperature interval may also be expressed in degrees Celsius\", the practice of simultaneously using both \u00b0C and K remains widespread throughout the scientific world as the use of SI-prefixed forms of the degree Celsius (such as \"\u03bc\u00b0C\" or \"microdegrees Celsius\") to express a temperature interval has not been widely adopted.\n\n\n== Melting and boiling points of water ==\n\nThe melting and boiling points of water are no longer part of the definition of the Celsius temperature scale. In 1948, the definition was changed to use the triple point of water. In 2005, the definition was further refined to use water with precisely defined isotopic composition (VSMOW) for the triple point. In 2019, the definition was changed to use the Boltzmann constant, completely decoupling the definition of the kelvin from the properties of water. Each of these formal definitions left the numerical values of the Celsius temperature scale identical to the prior definition to within the limits of accuracy of the metrology of the time.\nWhen the melting and boiling points of water ceased being part of the definition, they became measured quantities instead. This is also true of the triple point.\nIn 1948 when the 9th General Conference on Weights and Measures (CGPM) in Resolution 3 first considered using the triple point of water as a defining point, the triple point was so close to being 0.01 \u00b0C greater than water's known melting point, it was simply defined as precisely 0.01 \u00b0C. However, later measurements showed that the difference between the triple and melting points of VSMOW is actually very slightly (< 0.001 \u00b0C) greater than 0.01 \u00b0C. Thus, the actual melting point of ice is very slightly (less than a thousandth of a degree) below 0 \u00b0C. Also, defining water's triple point at 273.16 K precisely defined the magnitude of each 1 \u00b0C increment in terms of the absolute thermodynamic temperature scale (referencing absolute zero). Now decoupled from the actual boiling point of water, the value \"100 \u00b0C\" is hotter than 0 \u00b0C \u2013 in absolute terms \u2013 by a factor of exactly \u2060373.15/273.15\u2060 (approximately 36.61% thermodynamically hotter). When adhering strictly to the two-point definition for calibration, the boiling point of VSMOW under one standard atmosphere of pressure was actually 373.1339 K (99.9839 \u00b0C). When calibrated to ITS-90 (a calibration standard comprising many definition points and commonly used for high-precision instrumentation), the boiling point of VSMOW was slightly less, about 99.974 \u00b0C.\nThis boiling-point difference of 16.1 millikelvins between the Celsius temperature scale's original definition and the previous one (based on absolute zero and the triple point) has little practical meaning in common daily applications because water's boiling point is very sensitive to variations in barometric pressure. For example, an altitude change of only 28 cm (11 in) causes the boiling point to change by one millikelvin.\n\n\n== See also ==\nOutline of metrology and measurement\nComparison of temperature scales\nDegree of frost\nThermodynamic temperature\n\n\n== Notes ==\n\n\n== References ==\n\n\n== External links ==\n The dictionary definition of Celsius at Wiktionary\n\nNIST, Basic unit definitions: Kelvin\nThe Uppsala Astronomical Observatory, History of the Celsius temperature scale\nLondon South Bank University, Water, scientific data\nBIPM, SI brochure, section 2.1.1.5, Unit of thermodynamic temperature",
        "unit": "degree Celsius",
        "url": "https://en.wikipedia.org/wiki/Celsius"
    },
    {
        "_id": "Farad",
        "clean": "Farad",
        "text": "The farad (symbol: F) is the unit of electrical capacitance, the ability of a body to store an electrical charge, in the International System of Units (SI), equivalent to 1 coulomb per volt (C/V). It is named after the English physicist Michael Faraday (1791\u20131867). In SI base units 1 F = 1 kg\u22121\u22c5m\u22122\u22c5s4\u22c5A2.\n\n\n== Definition ==\nThe capacitance of a capacitor is one farad when one coulomb of charge changes the potential between the plates by one volt. Equally, one farad can be described as the capacitance which stores a one-coulomb charge across a potential difference of one volt.\nThe relationship between capacitance, charge, and potential difference is linear. For example, if the potential difference across a capacitor is halved, the quantity of charge stored by that capacitor will also be halved.\nFor most applications, the farad is an impractically large unit of capacitance. Most electrical and electronic applications are covered by the following SI prefixes:\n\n1 mF (millifarad, one thousandth (10\u22123) of a farad) = 0.001 F = 1000 \u03bcF = 1000000000 pF\n1 \u03bcF (microfarad, one millionth (10\u22126) of a farad) = 0.000 001 F = 1000 nF = 1000000 pF\n1 nF (nanofarad, one billionth (10\u22129) of a farad) = 0.000 000 001 F = 0.001 \u03bcF = 1000 pF\n1 pF (picofarad, one trillionth (10\u221212) of a farad) = 0.000 000 000 001 F = 0.001 nF\n\n\n=== Equalities ===\nA farad is a derived unit based on four of the seven base units of the International System of Units: kilogram (kg), metre (m), second (s), and ampere (A).\nExpressed in combinations of SI units, the farad is:\n\n  \n    \n      \n        \n          F\n        \n        =\n        \n          \n            \n              \n                \n                  \n                    s\n                  \n                  \n                    4\n                  \n                \n                \n                  \u22c5\n                \n                \n                  \n                    A\n                  \n                  \n                    2\n                  \n                \n              \n              \n                \n                  \n                    m\n                  \n                  \n                    2\n                  \n                \n                \n                  \u22c5\n                \n                \n                  kg\n                \n              \n            \n          \n        \n        =\n        \n          \n            \n              \n                \n                  \n                    s\n                  \n                  \n                    2\n                  \n                \n                \n                  \u22c5\n                \n                \n                  \n                    C\n                  \n                  \n                    2\n                  \n                \n              \n              \n                \n                  \n                    m\n                  \n                  \n                    2\n                  \n                \n                \n                  \u22c5\n                \n                \n                  kg\n                \n              \n            \n          \n        \n        =\n        \n          \n            \n              C\n              V\n            \n          \n        \n        =\n        \n          \n            \n              \n                \n                  A\n                \n                \n                  \u22c5\n                \n                \n                  s\n                \n              \n              V\n            \n          \n        \n        =\n        \n          \n            \n              \n                \n                  W\n                \n                \n                  \u22c5\n                \n                \n                  s\n                \n              \n              \n                \n                  V\n                \n                \n                  2\n                \n              \n            \n          \n        \n        =\n        \n          \n            \n              J\n              \n                \n                  V\n                \n                \n                  2\n                \n              \n            \n          \n        \n        =\n        \n          \n            \n              \n                \n                  N\n                \n                \n                  \u22c5\n                \n                \n                  m\n                \n              \n              \n                \n                  V\n                \n                \n                  2\n                \n              \n            \n          \n        \n        =\n        \n          \n            \n              \n                \n                  C\n                \n                \n                  2\n                \n              \n              J\n            \n          \n        \n        =\n        \n          \n            \n              \n                \n                  C\n                \n                \n                  2\n                \n              \n              \n                \n                  N\n                \n                \n                  \u22c5\n                \n                \n                  m\n                \n              \n            \n          \n        \n        =\n        \n          \n            \n              s\n              \u03a9\n            \n          \n        \n        =\n        \n          \n            \n              1\n              \n                \u03a9\n                \n                  \u22c5\n                \n                \n                  Hz\n                \n              \n            \n          \n        \n        =\n        \n          \n            \n              S\n              Hz\n            \n          \n        \n        =\n        \n          \n            \n              \n                \n                  s\n                \n                \n                  2\n                \n              \n              H\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle {\\text{F}}={\\dfrac {{\\text{s}}^{4}{\\cdot }{\\text{A}}^{2}}{{\\text{m}}^{2}{\\cdot }{\\text{kg}}}}={\\dfrac {{\\text{s}}^{2}{\\cdot }{\\text{C}}^{2}}{{\\text{m}}^{2}{\\cdot }{\\text{kg}}}}={\\dfrac {\\text{C}}{\\text{V}}}={\\dfrac {{\\text{A}}{\\cdot }{\\text{s}}}{\\text{V}}}={\\dfrac {{\\text{W}}{\\cdot }{\\text{s}}}{{\\text{V}}^{2}}}={\\dfrac {\\text{J}}{{\\text{V}}^{2}}}={\\dfrac {{\\text{N}}{\\cdot }{\\text{m}}}{{\\text{V}}^{2}}}={\\dfrac {{\\text{C}}^{2}}{\\text{J}}}={\\dfrac {{\\text{C}}^{2}}{{\\text{N}}{\\cdot }{\\text{m}}}}={\\dfrac {\\text{s}}{\\Omega }}={\\dfrac {1}{\\Omega {\\cdot }{\\text{Hz}}}}={\\dfrac {\\text{S}}{\\text{Hz}}}={\\dfrac {{\\text{s}}^{2}}{\\text{H}}},}\n  \n\nwhere F = farad, s = second, C = coulomb, V = volt, W = watt, J = joule, N = newton, \u03a9 = ohm, Hz = Hertz, S = siemens, H = henry, A = ampere.\n\n\n== History ==\nThe term \"farad\" was originally coined by Latimer Clark and Charles Bright in 1861, in honor of Michael Faraday, for a unit of quantity of charge, and by 1873, the farad had become a unit of capacitance. In 1881, at the International Congress of Electricians in Paris, the name farad was officially used for the unit of electrical capacitance.\n\n\n== Explanation ==\n\nA capacitor generally consists of two conducting surfaces, frequently referred to as plates, separated by an insulating layer usually referred to as a dielectric. The original capacitor was the Leyden jar developed in the 18th century. It is the accumulation of electric charge on the plates that results in capacitance. Modern capacitors are constructed using a range of manufacturing techniques and materials to provide the extraordinarily wide range of capacitance values used in electronics applications from femtofarads to farads, with maximum-voltage ratings ranging from a few volts to several kilovolts.\nValues of capacitors are usually specified in terms of SI prefixes of farads (F),  microfarads (\u03bcF), nanofarads (nF) and picofarads (pF). The millifarad (mF) is rarely used in practice; a capacitance of 4.7 mF (0.0047 F), for example, is instead written as 4700 \u03bcF. The nanofarad (nF) is uncommon in North America. The size of commercially available capacitors ranges from around 0.1 pF to 5000F (5 kF) supercapacitors. Parasitic capacitance in high-performance integrated circuits can be measured in femtofarads (1 fF = 0.001 pF = 10\u221215 F), while high-performance test equipment can detect changes in capacitance on the order of tens of attofarads (1 aF = 10\u221218 F).\nA value of 0.1 pF is about the smallest available in capacitors for general use in electronic design, since smaller ones would be dominated by the parasitic capacitances of other components, wiring or printed circuit boards. Capacitance values of 1 pF or lower can be achieved by twisting two short lengths of insulated wire together.\nThe capacitance of the Earth's ionosphere with respect to the ground is calculated to be about 1 F.\n\n\n=== Informal and deprecated terminology ===\nThe picofarad (pF) is sometimes colloquially pronounced as \"puff\" or \"pic\", as in \"a ten-puff capacitor\". Similarly, \"mic\" (pronounced \"mike\") is sometimes used informally to signify microfarads.\nNonstandard abbreviations were and are often used. Farad has been abbreviated \"f\", \"fd\", and \"Fd\". For the prefix \"micro-\", when the Greek small letter \"\u03bc\" or the legacy micro sign \"\u03bc\" is not available (as on typewriters) or inconvenient to enter, it is often substituted with the similar-appearing \"u\" or \"U\", with little risk of confusion. It was also substituted with the similar-sounding \"M\" or \"m\", which can be confusing because M officially stands for 1,000,000, and m preferably stands for 1/1000. In texts prior to 1960, and on capacitor packages until more recently, \"microfarad(s)\" was abbreviated \"mf\" or \"MFD\" rather than the modern \"\u03bcF\". A 1940 Radio Shack catalog listed every capacitor's rating in \"Mfd.\", from 0.000005 Mfd. (5 pF) to 50 Mfd. (50 \u03bcF).\n\"Micromicrofarad\" or \"micro-microfarad\" is an obsolete unit found in some older texts and labels, contains a nonstandard metric double prefix. It is exactly equivalent to a picofarad (pF). It is abbreviated \u03bc\u03bcF, uuF, or (confusingly) \"mmf\", \"MMF\", or \"MMFD\".\nSummary of obsolete or deprecated capacitance units or abbreviations: (upper/lower case variations are not shown)\n\n\u03bcF (microfarad) = mf, mfd, uf\npF (picofarad) = mmf, mmfd, pfd, \u03bc\u03bcF\nU+3332 \u3332 SQUARE HUARADDO is a square version of \u30d5\u30a1\u30e9\u30c3\u30c9 (faraddo, the Japanese word for \"farad\") intended for Japanese vertical text. \nIt is included in Unicode for compatibility with earlier character sets.\n\n\n=== Related concepts ===\nThe reciprocal of capacitance is called electrical elastance, the (non-standard, non-SI) unit of which is the daraf.\n\n\n== CGS units ==\nThe abfarad (abbreviated abF) is an obsolete CGS unit of capacitance, which corresponds to 109 farads (1 gigafarad, GF).\nThe statfarad (abbreviated statF) is a rarely used CGS unit equivalent to the capacitance of a capacitor with a charge of 1 statcoulomb across a potential difference of 1 statvolt. It is 1/(10\u22125 c2) farad, approximately 1.1126 picofarads. More commonly, the centimeter (cm) is used, which is equal to the statfarad.\n\n\n== Notes ==\n\n\n== External links ==\nFarad unit conversion tool",
        "unit": "farad",
        "url": "https://en.wikipedia.org/wiki/Farad"
    },
    {
        "_id": "Pascal_(unit)",
        "clean": "Pascal (unit)",
        "text": "The pascal (symbol: Pa) is the unit of pressure in the International System of Units (SI). It is also used to quantify internal pressure, stress, Young's modulus, and ultimate tensile strength. The unit, named after Blaise Pascal, is an SI coherent derived unit defined as one newton per square metre (N/m2). It is also equivalent to 10 barye (10 Ba) in the CGS system. Common multiple units of the pascal are the hectopascal (1 hPa = 100 Pa), which is equal to one millibar, and the kilopascal (1 kPa = 1000 Pa), which is equal to one centibar.\nThe unit of measurement called standard atmosphere (atm) is defined as 101325 Pa.\nMeteorological observations typically report atmospheric pressure in hectopascals per the recommendation of the World Meteorological Organization, thus a standard atmosphere (atm) or typical sea-level air pressure is about 1013 hPa. Reports in the United States typically use inches of mercury or millibars (hectopascals). In Canada, these reports are given in kilopascals.\n\n\n== Etymology ==\nThe unit is named after Blaise Pascal, noted for his contributions to hydrodynamics and hydrostatics, and experiments with a barometer. The name pascal was adopted for the SI unit newton per square metre (N/m2) by the 14th General Conference on Weights and Measures in 1971.\n\n\n== Definition ==\nThe pascal can be expressed using SI derived units, or alternatively solely SI base units, as:\n\n  \n    \n      \n        \n          \n            1\n             \n            P\n            a\n            =\n            1\n             \n            N\n            \n              /\n            \n            \n              m\n              \n                2\n              \n            \n            =\n            1\n             \n            k\n            g\n            \n              /\n            \n            (\n            m\n            \n              \u22c5\n            \n            \n              s\n              \n                2\n              \n            \n            )\n            =\n            1\n             \n            J\n            \n              /\n            \n            \n              m\n              \n                3\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\rm {1~Pa=1~N/m^{2}=1~kg/(m{\\cdot }s^{2})=1~J/m^{3}}}}\n  \n\nwhere N is the newton, m is the metre, kg is the kilogram, s is the second, and J is the joule.\nOne pascal is the pressure exerted by a force of one newton perpendicularly upon an area of one square metre.\n\n\n== Standard units ==\nThe unit of measurement called an atmosphere or a standard atmosphere (atm) is 101325 Pa (101.325 kPa). This value is often used as a reference pressure and specified as such in some national and international standards, such as the International Organization for Standardization's ISO 2787 (pneumatic tools and compressors), ISO 2533 (aerospace) and ISO 5024 (petroleum). In contrast, International Union of Pure and Applied Chemistry (IUPAC) recommends the use of 100 kPa as a standard pressure when reporting the properties of substances.\nUnicode has dedicated code-points U+33A9 \u33a9 SQUARE PA and U+33AA \u33aa SQUARE KPA in the CJK Compatibility block, but these exist only for backward-compatibility with some older ideographic character-sets and are therefore deprecated.\n\n\n== Uses ==\nThe pascal (Pa) or kilopascal (kPa) as a unit of pressure measurement is widely used throughout the world and has largely replaced the pounds per square inch (psi) unit, except in some countries that still use the imperial measurement system or the US customary system, including the United States.\nGeophysicists use the gigapascal (GPa) in measuring or calculating tectonic stresses and pressures within the Earth.\nMedical elastography measures tissue stiffness non-invasively with ultrasound or magnetic resonance imaging, and often displays the Young's modulus or shear modulus of tissue in kilopascals.\nIn materials science and engineering, the pascal measures the stiffness, tensile strength and compressive strength of materials. In engineering the megapascal (MPa) is the preferred unit for these uses, because the pascal represents a very small quantity.\n\nThe pascal is also equivalent to the SI unit of energy density, the joule per cubic metre. This applies not only to the thermodynamics of pressurised gases, but also to the energy density of electric, magnetic, and gravitational fields.\nThe pascal is used to measure sound pressure. Loudness is the subjective experience of sound pressure and is measured as a sound pressure level (SPL) on a logarithmic scale of the sound pressure relative to some reference pressure. For sound in air, a pressure of 20 \u03bcPa is considered to be at the threshold of hearing for humans and is a common reference pressure, so that its SPL is zero.\nThe airtightness of buildings is measured at 50 Pa.\nIn medicine, blood pressure is measured in millimeters of mercury (mmHg, very close to one Torr).  The normal adult blood pressure is less than 120 mmHg systolic BP (SBP) and less than 80 mmHg diastolic BP (DBP). Convert mmHg to SI units as follows: 1 mmHg = 0.13332 kPa. Hence normal blood pressure in SI units is less than 16.0 kPa SBP and less than 10.7 kPa DBP. These values are similar to the pressure of water column of average human height; so pressure has to be measured on arm roughly at the level of the heart.\n\n\n=== Hectopascal and millibar units ===\n\nThe units of atmospheric pressure commonly used in meteorology were formerly the bar (100,000 Pa), which is close to the average air pressure on Earth, and the millibar. Since the introduction of SI units, meteorologists generally measure pressures in hectopascals (hPa) unit, equal to 100 pascals or 1 millibar. Exceptions include Canada, which uses kilopascals (kPa). In many other fields of science, prefixes that are a power of 1000 are preferred, which excludes the hectopascal from use.\nMany countries also use millibars. In practically all other fields, the kilopascal is used instead.\n\n\n== Multiples and submultiples ==\nDecimal multiples and submultiples are formed using standard SI units.\n\n\n== See also ==\nAtmospheric pressure which gives the usage of the hbar and the mbar\nCentimetre of water\nMeteorology\nMetric prefix\nOrders of magnitude (pressure)\nPascal's law\nPressure measurement\n\n\n== References ==",
        "unit": "pascal",
        "url": "https://en.wikipedia.org/wiki/Pascal_(unit)"
    },
    {
        "_id": "Linear_density",
        "clean": "Linear density",
        "text": "Linear density is the measure of a quantity of any characteristic value per unit of length.  Linear mass density (titer in textile engineering, the amount of mass per unit length) and linear charge density (the amount of electric charge per unit length) are two common examples used in science and engineering.\nThe term linear density or linear mass density is most often used when describing the characteristics of one-dimensional objects, although linear density can also be used to describe the density of a three-dimensional quantity along one particular dimension.  Just as density is most often used to mean mass density, the term linear density likewise often refers to linear mass density.  However, this is only one example of a linear density, as any quantity can be measured in terms of its value along one dimension.\n\n\n== Linear mass density ==\nConsider a long, thin rod of mass \n  \n    \n      \n        M\n      \n    \n    {\\displaystyle M}\n  \n and length \n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  \n.  To calculate the average linear mass density, \n  \n    \n      \n        \n          \n            \n              \n                \u03bb\n                \u00af\n              \n            \n          \n          \n            m\n          \n        \n      \n    \n    {\\displaystyle {\\bar {\\lambda }}_{m}}\n  \n, of this one dimensional object, we can simply divide the total mass, \n  \n    \n      \n        M\n      \n    \n    {\\displaystyle M}\n  \n, by the total length, \n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  \n:\n\n  \n    \n      \n        \n          \n            \n              \n                \u03bb\n                \u00af\n              \n            \n          \n          \n            m\n          \n        \n        =\n        \n          \n            M\n            L\n          \n        \n      \n    \n    {\\displaystyle {\\bar {\\lambda }}_{m}={\\frac {M}{L}}}\n  \n\nIf we describe the rod as having a varying mass (one that varies as a function of position along the length of the rod, \n  \n    \n      \n        l\n      \n    \n    {\\displaystyle l}\n  \n), we can write:\n\n  \n    \n      \n        m\n        =\n        m\n        (\n        l\n        )\n      \n    \n    {\\displaystyle m=m(l)}\n  \n\nEach infinitesimal unit of mass, \n  \n    \n      \n        d\n        m\n      \n    \n    {\\displaystyle dm}\n  \n, is equal to the product of its linear mass density, \n  \n    \n      \n        \n          \u03bb\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle \\lambda _{m}}\n  \n, and the infinitesimal unit of length, \n  \n    \n      \n        d\n        l\n      \n    \n    {\\displaystyle dl}\n  \n:\n\n  \n    \n      \n        d\n        m\n        =\n        \n          \u03bb\n          \n            m\n          \n        \n        d\n        l\n      \n    \n    {\\displaystyle dm=\\lambda _{m}dl}\n  \n\nThe linear mass density can then be understood as the derivative of the mass function with respect to the one dimension of the rod (the position along its length, \n  \n    \n      \n        l\n      \n    \n    {\\displaystyle l}\n  \n)\n\n  \n    \n      \n        \n          \u03bb\n          \n            m\n          \n        \n        =\n        \n          \n            \n              d\n              m\n            \n            \n              d\n              l\n            \n          \n        \n      \n    \n    {\\displaystyle \\lambda _{m}={\\frac {dm}{dl}}}\n  \n\nThe SI unit of linear mass density is the kilogram per meter (kg/m).\nLinear density of fibers and yarns can be measured by many methods. The simplest one is to measure a length of material and weigh it. However, this requires a large sample and masks the variability of linear density along the thread, and is difficult to apply if the fibers are crimped or otherwise cannot lay flat relaxed. If the density of the material is known, the fibers are measured individually and have a simple shape, a more accurate method is direct imaging of the fiber with a scanning electron microscope to measure the diameter and calculation of the linear density. Finally, linear density is directly measured with a vibroscope. The sample is tensioned between two hard points, mechanical vibration is induced and the fundamental frequency is measured.\n\n\n== Linear charge density ==\n\nConsider a long, thin wire of charge \n  \n    \n      \n        Q\n      \n    \n    {\\displaystyle Q}\n  \n and length \n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  \n.  To calculate the average linear charge density, \n  \n    \n      \n        \n          \n            \n              \n                \u03bb\n                \u00af\n              \n            \n          \n          \n            q\n          \n        \n      \n    \n    {\\displaystyle {\\bar {\\lambda }}_{q}}\n  \n, of this one dimensional object, we can simply divide the total charge, \n  \n    \n      \n        Q\n      \n    \n    {\\displaystyle Q}\n  \n, by the total length, \n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  \n:\n\n  \n    \n      \n        \n          \n            \n              \n                \u03bb\n                \u00af\n              \n            \n          \n          \n            q\n          \n        \n        =\n        \n          \n            Q\n            L\n          \n        \n      \n    \n    {\\displaystyle {\\bar {\\lambda }}_{q}={\\frac {Q}{L}}}\n  \n\nIf we describe the wire as having a varying charge (one that varies as a function of position along the length of the wire, \n  \n    \n      \n        l\n      \n    \n    {\\displaystyle l}\n  \n), we can write:\n\n  \n    \n      \n        q\n        =\n        q\n        (\n        l\n        )\n      \n    \n    {\\displaystyle q=q(l)}\n  \n\nEach infinitesimal unit of charge, \n  \n    \n      \n        d\n        q\n      \n    \n    {\\displaystyle dq}\n  \n, is equal to the product of its linear charge density, \n  \n    \n      \n        \n          \u03bb\n          \n            q\n          \n        \n      \n    \n    {\\displaystyle \\lambda _{q}}\n  \n, and the infinitesimal unit of length, \n  \n    \n      \n        d\n        l\n      \n    \n    {\\displaystyle dl}\n  \n:\n\n  \n    \n      \n        d\n        q\n        =\n        \n          \u03bb\n          \n            q\n          \n        \n        d\n        l\n      \n    \n    {\\displaystyle dq=\\lambda _{q}dl}\n  \n\nThe linear charge density can then be understood as the derivative of the charge function with respect to the one dimension of the wire (the position along its length, \n  \n    \n      \n        l\n      \n    \n    {\\displaystyle l}\n  \n)\n\n  \n    \n      \n        \n          \u03bb\n          \n            q\n          \n        \n        =\n        \n          \n            \n              d\n              q\n            \n            \n              d\n              l\n            \n          \n        \n      \n    \n    {\\displaystyle \\lambda _{q}={\\frac {dq}{dl}}}\n  \n\nNotice that these steps were exactly the same ones we took before to find \n  \n    \n      \n        \n          \u03bb\n          \n            m\n          \n        \n        =\n        \n          \n            \n              d\n              m\n            \n            \n              d\n              l\n            \n          \n        \n      \n    \n    {\\textstyle \\lambda _{m}={\\frac {dm}{dl}}}\n  \n.\nThe SI unit of linear charge density is the coulomb per meter (C/m).\n\n\n== Other applications ==\nIn drawing or printing, the term linear density also refers to how densely or heavily a line is drawn.\nThe most famous abstraction of linear density is the probability density function of a single random variable.\n\n\n== Units ==\n\nCommon units include:\n\nkilogram per meter\nounce (mass) per foot\nounce (mass) per inch\npound (mass) per yard: used in the North American railway industry for the linear density of rails\npound (mass) per foot\npound (mass) per inch\ntex, a unit of measure for the linear density of fibers, defined as the mass in grams per 1,000 meters\ndenier, a unit of measure for the linear density of fibers, defined as the mass in grams per 9,000 meters\ndecitex (dtex), the SI unit for the linear density of fibers, defined as the mass in grams per 10,000 meters\n\n\n== See also ==\nDensity\nArea density\nColumnar density\nPaper density\nLinear number density\n\n\n== References ==",
        "unit": "linear current density",
        "url": "https://en.wikipedia.org/wiki/Linear_density"
    },
    {
        "_id": "Megabit",
        "clean": "Megabit",
        "text": "The bit is the most basic unit of information in computing and digital communication. The name is a portmanteau of binary digit. The bit represents a logical state with one of two possible values. These values are most commonly represented as either \"1\" or \"0\", but other representations such as true/false, yes/no, on/off, or +/\u2212 are also widely used.\nThe relation between these values and the physical states of the underlying storage or device is a matter of convention, and different assignments may be used even within the same device or program. It may be physically implemented with a two-state device.\nA contiguous group of binary digits is commonly called a bit string, a bit vector, or a single-dimensional (or multi-dimensional) bit array.\nA group of eight bits is called one byte, but historically the size of the byte is not strictly defined. Frequently, half, full, double and quadruple words consist of a number of bytes which is a low power of two. A string of four bits is usually a nibble.\nIn information theory, one bit is the information entropy of a random binary variable that is 0 or 1 with equal probability, or the information that is gained when the value of such a variable becomes known. As a unit of information, the bit is also known as a shannon, named after Claude E. Shannon.\nThe symbol for the binary digit is either \"bit\", per the IEC 80000-13:2008 standard, or the lowercase character \"b\", per the IEEE 1541-2002 standard. Use of the latter may create confusion with the capital \"B\" which is the international standard symbol for the byte.\n\n\n== History ==\nThe encoding of data by discrete bits was used in the punched cards invented by Basile Bouchon and Jean-Baptiste Falcon (1732), developed by Joseph Marie Jacquard (1804), and later adopted by Semyon Korsakov, Charles Babbage, Herman Hollerith, and early computer manufacturers like IBM. A variant of that idea was the perforated paper tape. In all those systems, the medium (card or tape) conceptually carried an array of hole positions; each position could be either punched through or not, thus carrying one bit of information. The encoding of text by bits was also used in Morse code (1844) and early digital communications machines such as teletypes and stock ticker machines (1870).\nRalph Hartley suggested the use of a logarithmic measure of information in 1928. Claude E. Shannon first used the word \"bit\" in his seminal 1948 paper \"A Mathematical Theory of Communication\". He attributed its origin to John W. Tukey, who had written a Bell Labs memo on 9 January 1947 in which he contracted \"binary information digit\" to simply \"bit\".\n\n\n== Physical representation ==\nA bit can be stored by a digital device or other physical system that exists in either of two possible distinct states. These may be the two stable states of a flip-flop, two positions of an electrical switch, two distinct voltage or current levels allowed by a circuit, two distinct levels of light intensity, two directions of magnetization or polarization, the orientation of reversible double stranded DNA, etc.\nBits can be implemented in several forms. In most modern computing devices, a bit is usually represented by an electrical voltage or current pulse, or by the electrical state of a flip-flop circuit.\nFor devices using positive logic, a digit value of 1 (or a logical value of true) is represented by a more positive voltage relative to the representation of 0. Different logic families require different voltages, and variations are allowed to account for component aging and noise immunity. For example, in transistor\u2013transistor logic (TTL) and compatible circuits, digit values 0 and 1 at the output of a device are represented by no higher than 0.4 V and no lower than 2.6 V, respectively; while TTL inputs are specified to recognize 0.8 V or below as 0 and 2.2 V or above as 1.\n\n\n=== Transmission and processing ===\nBits are transmitted one at a time in serial transmission, and by a multiple number of bits in parallel transmission. A bitwise operation optionally processes bits one at a time. Data transfer rates are usually measured in decimal SI multiples of the unit bit per second (bit/s), such as kbit/s.\n\n\n=== Storage ===\nIn the earliest non-electronic information processing devices, such as Jacquard's loom or Babbage's Analytical Engine, a bit was often stored as the position of a mechanical lever or gear, or the presence or absence of a hole at a specific point of a paper card or tape. The first electrical devices for discrete logic (such as elevator and traffic light control circuits, telephone switches, and Konrad Zuse's computer) represented bits as the states of electrical relays which could be either \"open\" or \"closed\". When relays were replaced by vacuum tubes, starting in the 1940s, computer builders experimented with a variety of storage methods, such as pressure pulses traveling down a mercury delay line, charges stored on the inside surface of a cathode-ray tube, or opaque spots printed on glass discs by photolithographic techniques.\nIn the 1950s and 1960s, these methods were largely supplanted by magnetic storage devices such as magnetic-core memory, magnetic tapes, drums, and disks, where a bit was represented by the polarity of magnetization of a certain area of a ferromagnetic film, or by a change in polarity from one direction to the other. The same principle was later used in the magnetic bubble memory developed in the 1980s, and is still found in various magnetic strip items such as metro tickets and some credit cards.\nIn modern semiconductor memory, such as dynamic random-access memory, the two values of a bit may be represented by two levels of electric charge stored in a capacitor. In certain types of programmable logic arrays and read-only memory, a bit may be represented by the presence or absence of a conducting path at a certain point of a circuit. In optical discs, a bit is encoded as the presence or absence of a microscopic pit on a reflective surface. In one-dimensional bar codes, bits are encoded as the thickness of alternating black and white lines.\n\n\n== Unit and symbol ==\nThe bit is not defined in the International System of Units (SI). However, the International Electrotechnical Commission issued standard IEC 60027, which specifies that the symbol for binary digit should be 'bit', and this should be used in all multiples, such as 'kbit', for kilobit. However, the lower-case letter 'b' is widely used as well and was recommended by the IEEE 1541 Standard (2002). In contrast, the upper case letter 'B' is the standard and customary symbol for byte.\n\n\n=== Multiple bits ===\n\nMultiple bits may be expressed and represented in several ways. For convenience of representing commonly reoccurring groups of bits in information technology, several units of information have traditionally been used. The most common is the unit byte, coined by Werner Buchholz in June 1956, which historically was used to represent the group of bits used to encode a single character of text (until UTF-8 multibyte encoding took over) in a computer and for this reason it was used as the basic addressable element in many computer architectures. The trend in hardware design converged on the most common implementation of using eight bits per byte, as it is widely used today. However, because of the ambiguity of relying on the underlying hardware design, the unit octet was defined to explicitly denote a sequence of eight bits.\nComputers usually manipulate bits in groups of a fixed size, conventionally named \"words\". Like the byte, the number of bits in a word also varies with the hardware design, and is typically between 8 and 80 bits, or even more in some specialized computers. In the early 21st century, retail personal or server computers have a word size of 32 or 64 bits.\nThe International System of Units defines a series of decimal prefixes for multiples of standardized units which are commonly also used with the bit and the byte. The prefixes kilo (103) through yotta (1024) increment by multiples of one thousand, and the corresponding units are the kilobit (kbit) through the yottabit (Ybit).\n\n\n== Information capacity and information compression ==\n\nWhen the information capacity of a storage system or a communication channel is presented in bits or bits per second, this often refers to binary digits, which is a computer hardware capacity to store binary data (0 or 1, up or down, current or not, etc.). Information capacity of a storage system is only an upper bound to the quantity of information stored therein. If the two possible values of one bit of storage are not equally likely, that bit of storage contains less than one bit of information. If the value is completely predictable, then the reading of that value provides no information at all (zero entropic bits, because no resolution of uncertainty occurs and therefore no information is available). If a computer file that uses n bits of storage contains only m < n bits of information, then that information can in principle be encoded in about m bits, at least on the average. This principle is the basis of data compression technology. Using an analogy, the hardware binary digits refer to the amount of storage space available (like the number of buckets available to store things), and the information content the filling, which comes in different levels of granularity (fine or coarse, that is, compressed or uncompressed information). When the granularity is finer\u2014when information is more compressed\u2014the same bucket can hold more.\nFor example, it is estimated that the combined technological capacity of the world to store information provides 1,300 exabytes of hardware digits. However, when this storage space is filled and the corresponding content is optimally compressed, this only represents 295 exabytes of information. When optimally compressed, the resulting carrying capacity approaches Shannon information or information entropy.\n\n\n== Bit-based computing ==\nCertain bitwise computer processor instructions (such as bit set) operate at the level of manipulating bits rather than manipulating data interpreted as an aggregate of bits.\nIn the 1980s, when bitmapped computer displays became popular, some computers provided specialized bit block transfer instructions to set or copy the bits that corresponded to a given rectangular area on the screen.\nIn most computers and programming languages, when a bit within a group of bits, such as a byte or word, is referred to, it is usually specified by a number from 0 upwards corresponding to its position within the byte or word. However, 0 can refer to either the most or least significant bit depending on the context.\n\n\n== Other information units ==\n\nSimilar to torque and energy in physics; information-theoretic information and data storage size have the same dimensionality of units of measurement, but there is in general no meaning to adding, subtracting or otherwise combining the units mathematically, although one may act as a bound on the other.\nUnits of information used in information theory include the shannon (Sh), the natural unit of information (nat) and the hartley (Hart). One shannon is the maximum amount of information needed to specify the state of one bit of storage. These are related by 1 Sh \u2248 0.693 nat \u2248 0.301 Hart.\nSome authors also define a binit as an arbitrary information unit equivalent to some fixed but unspecified number of bits.\n\n\n== See also ==\nBinary numeral system\nBit rate and baud rate\nBitstream\nByte\nEntropy (information theory)\nFuzzy bit\nInteger (computer science)\nNibble\nPrimitive data type\nP-bit (probabilistic bit)\nQubit (quantum bit)\nShannon (unit)\nTernary numeral system\nTrit (Trinary digit)\n\n\n== References ==\n\n\n== External links ==\n\nBit Calculator \u2013 a tool providing conversions between bit, byte, kilobit, kilobyte, megabit, megabyte, gigabit, gigabyte\nBitXByteConverter Archived 2016-04-06 at the Wayback Machine \u2013 a tool for computing file sizes, storage capacity, and digital information in various units",
        "unit": "megabit",
        "url": "https://en.wikipedia.org/wiki/Megabit"
    },
    {
        "_id": "Candela_per_square_metre",
        "clean": "Candela per square metre",
        "text": "The candela per square metre (symbol: cd/m2) is the unit of luminance in the International System of Units (SI). The unit is based on the candela, the SI unit of luminous intensity, and the square metre, the SI unit of area. \nThe nit (symbol: nt) is a non-SI name also used for this unit (1 nt = 1 cd/m2). The term nit is believed to come from the Latin word nit\u0113re, \"to shine\".\nAs a measure of light emitted per unit area, this unit is frequently used to specify the brightness of a display device. The sRGB spec for monitors targets 80 cd/m2. Typically, monitors calibrated for SDR broadcast or studio color grading should have a brightness of 100 cd/m2. Most consumer desktop liquid crystal displays have luminances of 200 to 300 cd/m2. HDR displays range from around 400 to 2500 cd/m2.\n\n\n== Comparison with other units of luminance ==\nOne candela per square metre is equal to:\n\n10\u22124 stilbs (the CGS unit of luminance)\n\u03c0\u00d710\u22124 lamberts\n\u03c0 apostilbs\n0.292 foot-lamberts\n\u03c0\u00d7103 skots\n\u03c0\u00d7107 brils\n1 nit\n\n\n== See also ==\nOrders of magnitude (luminance)\nPhotometry (optics)\n\n\n== References ==\n\n\n== External links ==\nIEC 61966-2-1:1999 Multimedia systems and equipment \u2013 Colour measurement and management \u2013 Part 2-1: Colour management \u2013 Default RGB colour space \u2013 sRGB\nIEC International System of Units zone",
        "unit": "candela per square metre",
        "url": "https://en.wikipedia.org/wiki/Candela_per_square_metre"
    },
    {
        "_id": "Short_ton",
        "clean": "Short ton",
        "text": "The short ton (abbreviation tn) is a measurement unit equal to 2,000 pounds (907.18 kg). It is commonly used in the United States, where it is known simply as a ton; however, the term is ambiguous, the single word \"ton\" being variously used for short, long, and metric tons.\nThe various tons are defined as units of mass. They are sometimes used as units of  weight, the force exerted by a mass at standard gravity (e.g., short ton-force). One short ton exerts a weight at one standard gravity of 2,000 pound-force (lbf).\n\n\n== United States ==\n\nIn the United States, a short ton is usually known simply as a \"ton\", without distinguishing it from the tonne (1,000 kilograms or 2,204.62 pounds), known there as the \"metric ton\", or the long ton also known as the \"imperial ton\" (2,240 pounds or 1,016.05 kilograms). There are, however, some U.S. applications where unspecified tons normally mean long tons (for example, naval ships) or metric tons (world grain production figures).\nBoth the long and short ton are defined as 20 hundredweights, but a hundredweight is 100 pounds (45.36 kg) in the US system (short or net hundredweight) and 112 pounds (50.80 kg) in the imperial system (long or gross hundredweight).\nA short ton\u2013force is 2,000 pounds-force (8,896.44 N).\n\n\n== See also ==\nTonnage, volume measurement used in maritime shipping, originally based on 100 cubic feet (2.83168 m3).\n\n\n== References ==",
        "unit": "short ton",
        "url": "https://en.wikipedia.org/wiki/Short_ton"
    },
    {
        "_id": "Barn_(unit)",
        "clean": "Barn (unit)",
        "text": "A barn (symbol: b) is a metric unit of area equal to 10\u221228 m2 (100 fm2). Originally used in nuclear physics for expressing the cross sectional area of nuclei and nuclear reactions, today it is also used in all fields of high-energy physics to express the cross sections of any scattering process, and is best understood as a measure of the probability of interaction between small particles. A barn is approximately the cross-sectional area of a uranium nucleus. The barn is also the unit of area used in nuclear quadrupole resonance and nuclear magnetic resonance to quantify the interaction of a nucleus with an electric field gradient. While the barn never was an SI unit, the SI standards body acknowledged it in the 8th SI Brochure (superseded in 2019) due to its use in particle physics.\n\n\n== Etymology ==\nDuring Manhattan Project research on the atomic bomb during World War II, American physicists Marshall Holloway and Charles P. Baker were working at Purdue University on a project using a particle accelerator to measure the cross sections of certain nuclear reactions. According to an account of theirs from a couple years later, they were dining in a cafeteria in December 1942 and discussing their work. They \"lamented\" that there was no name for the unit of cross section and challenged themselves to develop one. They initially tried to find the name of \"some great man closely associated with the field\" that they could name the unit after, but struggled to find one that was appropriate. They considered \"Oppenheimer\" too long (in retrospect, they considered an \"Oppy\" to perhaps have been allowable), and considered \"Bethe\" to be too easily confused with the commonly-used Greek letter beta. They then considered naming it after John Manley, another scientist associated with their work, but considered \"Manley\" too long and \"John\" too closely associated with toilets. But this latter association, combined with the \"rural background\" of one of the scientists, suggested to them the term \"barn\", which also worked because the unit was \"really as big as a barn.\" According to the authors, the first published use of the term was in a (secret) Los Alamos report from late June 1943, on which the two originators were co-authors.\n\n\n== Commonly used prefixed versions ==\nThe unit symbol for the barn (b) is also the IEEE standard symbol for bit. In other words, 1 Mb can mean one megabarn or one megabit.\n\n\n== Conversions ==\nCalculated cross sections are often given in terms of inverse squared gigaelectronvolts (GeV\u22122), via the conversion \u01272c2/GeV2 = 0.3894 mb = 38940 am2.\nIn natural units (where \u0127 = c = 1), this simplifies to GeV\u22122 = 0.3894 mb = 38940 am2.\n\n\n=== SI units with prefix ===\nIn SI, one can use units such as square femtometers (fm2). The most common SI prefixed unit for the barn is the femtobarn, which is equal to a tenth of a square zeptometer. Many scientific papers discussing high-energy physics mention quantities of fractions of femtobarn level.\n\n\n== Inverse femtobarn ==\nThe inverse femtobarn (fb\u22121) is the unit typically used to measure the number of particle collision events per femtobarn of target cross-section, and is the conventional unit for time-integrated luminosity. Thus if a detector has accumulated 100 fb\u22121 of integrated luminosity, one expects to find 100 events per femtobarn of cross-section within these data.\nConsider a particle accelerator where two streams of particles, with cross-sectional areas measured in femtobarns, are directed to collide over a period of time. The total number of collisions will be directly proportional to the luminosity of the collisions measured over this time. Therefore, the collision count can be calculated by multiplying the integrated luminosity by the sum of the cross-section for those collision processes. This count is then expressed as inverse femtobarns for the time period (e.g., 100 fb\u22121 in nine months). Inverse femtobarns are often quoted as an indication of particle collider productivity.\nFermilab produced 10 fb\u22121 in the first decade of the 21st century.  Fermilab's Tevatron took about 4 years to reach 1 fb\u22121 in 2005, while two of CERN's LHC experiments, ATLAS and CMS, reached over 5 fb\u22121 of proton\u2013proton data in 2011 alone. In April 2012 the LHC achieved the collision energy of 8 TeV with a luminosity peak of 6760 inverse microbarns per second; by May 2012 the LHC delivered 1 inverse femtobarn of data per week to each detector collaboration. A record of over 23 fb\u22121 was achieved during 2012. As of November 2016, the LHC had achieved 40 fb\u22121 over that year, significantly exceeding the stated goal of 25 fb\u22121. In total, the second run of the LHC has delivered around 150 fb\u22121 to both ATLAS and CMS in 2015\u20132018.\n\n\n=== Usage example ===\nAs a simplified example, if a beamline runs for 8 hours (28 800 seconds) at an instantaneous luminosity of 300\u00d71030 cm\u22122\u22c5s\u22121 = 300 \u03bcb\u22121\u22c5s\u22121, then it will gather data totaling an integrated luminosity of 8640000 \u03bcb\u22121 = 8.64 pb\u22121 = 0.00864 fb\u22121 during this period. If this is multiplied by the cross-section, then a dimensionless number is obtained equal to the number of expected scattering events.\n\n\n== See also ==\n\"Shake\", a unit of time created by the same people at the same time as the barn\nOrders of magnitude (area)\nList of unusual units of measurement\nList of humorous units of measurement\n\n\n== References ==\n\n\n== External links ==\nIUPAC citation for this usage of \"barn\"",
        "unit": "kilobarn",
        "url": "https://en.wikipedia.org/wiki/Barn_(unit)"
    },
    {
        "_id": "Petayear",
        "clean": "Petayear",
        "text": "A year is the time taken for astronomical objects to complete one orbit. For example, a year on Earth is the time taken for Earth to revolve around the Sun. Generally, a year is taken to mean a calendar year, but the word is also used for periods loosely associated with the calendar or astronomical year, such as the seasonal year, the fiscal year, the academic year, etc. The term can also be used in reference to any long period or cycle, such as the Great Year.\nDue to the Earth's axial tilt, the course of a year sees the passing of the seasons, marked by changes in weather, the hours of daylight, and, consequently, vegetation and soil fertility. In temperate and subpolar regions around the planet, four seasons are generally recognized: spring, summer, autumn, and winter. In tropical and subtropical regions, several geographical sectors do not present defined seasons; but in the seasonal tropics, the annual wet and dry seasons are recognized and tracked.\n\n\n== Calendar year ==\nA calendar year is an approximation of the number of days of the Earth's orbital period, as counted in a given calendar. The Gregorian calendar, or modern calendar, presents its calendar year to be either a common year of 365 days or a leap year of 366 days, as do the Julian calendars. For the Gregorian calendar, the average length of the calendar year (the mean year) across the complete leap cycle of 400 years is 365.2425 days (97 out of 400 years are leap years).\n\n\n== Abbreviation ==\nIn English, the unit of time for year is commonly abbreviated as \"y\" or \"yr\". The symbol \"a\" (for Latin: annus, year) is sometimes used in scientific literature, though its exact duration may be inconsistent.\n\n\n== Etymology ==\nEnglish year (via West Saxon \u0121\u0113ar (/j\u025bar/), Anglian \u0121\u0113r) continues Proto-Germanic *j\u01e3ran (*j\u0113\u2081ran). Cognates are German Jahr, Old High German j\u0101r, Old Norse \u00e1r and Gothic jer, from the Proto-Indo-European noun *yeh\u2081r-om \"year, season\". Cognates also descended from the same Proto-Indo-European noun (with variation in suffix ablaut) are Avestan y\u0101r\u01dd \"year\", Greek \u1f65\u03c1\u03b1 (h\u1e53ra) \"year, season, period of time\" (whence \"hour\"), Old Church Slavonic jar\u016d, and Latin hornus \"of this year\".\nLatin annus (a 2nd declension masculine noun; annum is the accusative singular; ann\u012b is genitive singular and nominative plural; ann\u014d the dative and ablative singular) is from a PIE noun *h\u2082et-no-, which also yielded Gothic a\u00fen \"year\" (only the dative plural a\u00fenam is attested).\nAlthough most languages treat the word as thematic *yeh\u2081r-o-, there is evidence for an original derivation with an *-r/n suffix, *yeh\u2081-ro-. Both Indo-European words for year, *yeh\u2081-ro- and *h\u2082et-no-, would then be derived from verbal roots meaning \"to go, move\", *h\u2081ey- and *h\u2082et-, respectively (compare Vedic Sanskrit \u00e9ti \"goes\", atasi \"thou goest, wanderest\"). A number of English words are derived from Latin annus, such as annual, annuity, anniversary, etc.; per annum means \"each year\", ann\u014d Domin\u012b means \"in the year of the Lord\".\nThe Greek word for \"year\", \u1f14\u03c4\u03bf\u03c2, is cognate with Latin vetus \"old\", from the PIE word *wetos- \"year\", also preserved in this meaning in Sanskrit vat-sa-ras \"year\" and vat-sa- \"yearling (calf)\", the latter also reflected in Latin vitulus \"bull calf\", English wether \"ram\" (Old English we\u00f0er, Gothic wi\u00ferus \"lamb\").\nIn some languages, it is common to count years by referencing to one season, as in \"summers\", or \"winters\", or \"harvests\". Examples include Chinese \u5e74 \"year\", originally \u79c2, an ideographic compound of a person carrying a bundle of wheat denoting \"harvest\". Slavic besides god\u016d \"time period; year\" uses l\u011bto \"summer; year\".\n\n\n== Intercalation ==\nAstronomical years do not have an integer number of days or lunar months. Any calendar that follows an astronomical year must have a system of intercalation such as leap years.\n\n\n=== Julian calendar ===\nIn the Julian calendar, the average (mean) length of a year is 365.25 days. In a non-leap year, there are 365 days, in a leap year there are 366 days. A leap year occurs every fourth year during which a leap day is intercalated into the month of February. The name \"Leap Day\" is applied to the added day.\nIn astronomy, the Julian year is a unit of time defined as 365.25 days, each of exactly 86,400 seconds (SI base unit), totaling exactly 31,557,600 seconds in the Julian astronomical year.\n\n\n==== Revised Julian calendar ====\nThe Revised Julian calendar, proposed in 1923 and used in some Eastern Orthodox Churches, has 218 leap years every 900 years, for the average (mean) year length of 365.2422222 days, close to the length of the mean tropical year, 365.24219 days (relative error of 9\u00b710). In the year 2800 CE, the Gregorian and Revised Julian calendars will begin to differ by one calendar day.\n\n\n=== Gregorian calendar ===\nThe Gregorian calendar aims to ensure that the northward equinox falls on or shortly before March 21 and hence it follows the northward equinox year, or tropical year. Because 97 out of 400 years are leap years, the mean length of the Gregorian calendar year is 365.2425 days; with a relative error below one ppm (8\u00b710) relative to the current length of the mean tropical year (365.242189 days) and even closer to the current March equinox year of 365.242374 days that it aims to match. \n\n\n=== Other calendars ===\n\nHistorically, lunisolar calendars intercalated entire leap months on an observational basis. Lunisolar calendars have mostly fallen out of use except for liturgical reasons (Hebrew calendar, various Hindu calendars).\nA modern adaptation of the historical Jalali calendar, known as the Solar Hijri calendar (1925), is a purely solar calendar with an irregular pattern of leap days based on observation (or astronomical computation), aiming to place new year (Nowruz) on the day of vernal equinox (for the time zone of Tehran), as opposed to using an algorithmic system of leap years.\n\n\n== Year numbering ==\nA calendar era assigns a cardinal number to each sequential year, using a reference event in the past (called the epoch) as the beginning of the era.\nThe Gregorian calendar era is the world's most widely used civil calendar. Its epoch is a 6th century estimate of the date of birth of Jesus of Nazareth. Two notations are used to indicate year numbering in the Gregorian calendar: the Christian \"Anno Domini\" (meaning \"in the year of the Lord\"), abbreviated AD; and \"Common Era\", abbreviated CE, preferred by many of other faiths and none.  Year numbers are based on inclusive counting, so that there is no \"year zero\".  Years before the epoch are abbreviated BC for Before Christ or BCE for Before the Common Era. In Astronomical year numbering, positive numbers indicate years AD/CE, the number 0 designates 1 BC/BCE, \u22121 designates 2 BC/BCE, and so on.\nOther eras include that of Ancient Rome, Ab Urbe Condita (\"from the foundation of the city), abbreviated AUC; Anno Mundi (\"year of the world\"), used for the Hebrew calendar and abbreviated AM; and the Japanese imperial eras. The Islamic Hijri year, (year of the Hijrah, Anno Hegirae abbreviated AH), is a lunar calendar of twelve lunar months and thus is shorter than a solar year.\n\n\n== Pragmatic divisions ==\nFinancial and scientific calculations often use a 365-day calendar to simplify daily rates.\n\n\n=== Fiscal year ===\n\nA fiscal year or financial year is a 12-month period used for calculating annual financial statements in businesses and other organizations. In many jurisdictions, regulations regarding accounting require such reports once per twelve months, but do not require that the twelve months constitute a calendar year.\nFor example, in Canada and India the fiscal year runs from April 1; in the United Kingdom it runs from April 1 for purposes of corporation tax and government financial statements, but from April 6 for purposes of personal taxation and payment of state benefits; in Australia it runs from July 1; while in the United States the fiscal year of the federal government runs from October 1.\n\n\n=== Academic year ===\n\nAn academic year is the annual period during which a student attends an educational institution. The academic year may be divided into academic terms, such as semesters or quarters. The school year in many countries starts in August or September and ends in May, June or July. In Israel the academic year begins around October or November, aligned with the second month of the Hebrew calendar.\nSome schools in the UK, Canada and the United States divide the academic year into three roughly equal-length terms (called trimesters or quarters in the United States), roughly coinciding with autumn, winter, and spring. At some, a shortened summer session, sometimes considered part of the regular academic year, is attended by students on a voluntary or elective basis. Other schools break the year into two main semesters, a first (typically August through December) and a second semester (January through May). Each of these main semesters may be split in half by mid-term exams, and each of the halves is referred to as a quarter (or term in some countries). There may also be a voluntary summer session or a short January session.\nSome other schools, including some in the United States, have four marking periods. Some schools in the United States, notably Boston Latin School, may divide the year into five or more marking periods. Some state in defense of this that there is perhaps a positive correlation between report frequency and academic achievement.\nThere are typically 180 days of teaching each year in schools in the US, excluding weekends and breaks, while there are 190 days for pupils in state schools in Canada, New Zealand and the United Kingdom, and 200 for pupils in Australia.\nIn India the academic year normally starts from June 1 and ends on May 31. Though schools start closing from mid-March, the actual academic closure is on May 31 and in Nepal it starts from July 15.\nSchools and universities in Australia typically have academic years that roughly align with the calendar year (i.e., starting in February or March and ending in October to December), as the southern hemisphere experiences summer from December to February.\n\n\n== Astronomical years ==\n\n\n=== Julian year ===\n\nThe Julian year, as used in astronomy and other sciences, is a time unit defined as exactly 365.25 days of 86,400 SI seconds each (\"ephemeris days\"). This is the normal meaning of the unit \"year\" used in various scientific contexts. The Julian century of 36525 ephemeris days and the Julian millennium of 365250 ephemeris days are used in astronomical calculations. Fundamentally, expressing a time interval in Julian years is a way to precisely specify an amount of time (not how many \"real\" years), for long time intervals where stating the number of ephemeris days would be unwieldy and unintuitive. By convention, the Julian year is used in the computation of the distance covered by a light-year.\nIn the Unified Code for Units of Measure (but not according to the International Union of Pure and Applied Physics or the International Union of Geological Sciences, see below), the symbol a (without subscript) always refers to the Julian year, aj, of exactly 31557600 seconds.\n\n365.25 d \u00d7 86400 s = 1 a = 1 aj = 31.5576 Ms\nThe SI multiplier prefixes may be applied to it to form \"ka\", \"Ma\", etc.\n\n\n=== Sidereal, tropical, and anomalistic years ===\n\nEach of these three years can be loosely called an astronomical year.\nThe sidereal year is the time taken for the Earth to complete one revolution of its orbit, as measured against a fixed frame of reference (such as the fixed stars, Latin sidera, singular sidus). Its average duration is 365.256363004 days (365 d 6 h 9 min 9.76 s) (at the epoch J2000.0 = January 1, 2000, 12:00:00 TT).\nToday the mean tropical year is defined as the period of time for the mean ecliptic longitude of the Sun to increase by 360 degrees. Since the Sun's ecliptic longitude is measured with respect to the equinox, the tropical year comprises a complete cycle of the seasons and is the basis of solar calendars such as the internationally used Gregorian calendar. The modern definition of mean tropical year differs from the actual time between passages of, e.g., the northward equinox, by a minute or two, for several reasons explained below. Because of the Earth's axial precession, this year is about 20 minutes shorter than the sidereal year. The mean tropical year is approximately 365 days, 5 hours, 48 minutes, 45 seconds, using the modern definition ( = 365.24219 d \u00d7 86\u2009400 s). The length of the tropical year varies a bit over thousands of years because the rate of axial precession is not constant.\nThe anomalistic year is the time taken for the Earth to complete one revolution with respect to its apsides. The orbit of the Earth is elliptical; the extreme points, called apsides, are the perihelion, where the Earth is closest to the Sun, and the aphelion, where the Earth is farthest from the Sun. The anomalistic year is usually defined as the time between perihelion passages. Its average duration is 365.259636 days (365 d 6 h 13 min 52.6 s) (at the epoch J2011.0).\n\n\n=== Draconic year ===\n\nThe draconic year, draconitic year, eclipse year, or ecliptic year is the time taken for the Sun (as seen from the Earth) to complete one revolution with respect to the same lunar node (a point where the Moon's orbit intersects the ecliptic). The year is associated with eclipses: these occur only when both the Sun and the Moon are near these nodes; so eclipses occur within about a month of every half eclipse year. Hence there are two eclipse seasons every eclipse year. The average duration of the eclipse year is\n\n346.620075883 days (346 d 14 h 52 min 54 s) (at the epoch J2000.0).\nThis term is sometimes erroneously used for the draconic or nodal period of lunar precession, that is the period of a complete revolution of the Moon's ascending node around the ecliptic: 18.612815932 Julian years (6798.331019 days; at the epoch J2000.0).\n\n\n=== Full moon cycle ===\nThe full moon cycle is the time for the Sun (as seen from the Earth) to complete one revolution with respect to the perigee of the Moon's orbit. This period is associated with the apparent size of the full moon, and also with the varying duration of the synodic month. The duration of one full moon cycle is:\n\n411.78443029 days (411 days 18 hours 49 minutes 35 seconds) (at the epoch J2000.0).\n\n\n=== Lunar year ===\nThe lunar year comprises twelve full cycles of the phases of the Moon, as seen from Earth. It has a duration of approximately 354.37 days. Muslims use this for celebrating their Eids and for marking the start of the fasting month of Ramadan. A Muslim calendar year is based on the lunar cycle. The Jewish calendar is also essentially lunar, except that an intercalary lunar month is added once every two or three years, in order to keep the calendar synchronized with the solar cycle as well. Thus, a lunar year on the Jewish (Hebrew) calendar consists of either twelve or thirteen lunar months.\n\n\n=== Vague year ===\nThe vague year, from annus vagus or wandering year, is an integral approximation to the year equaling 365 days, which wanders in relation to more exact years. Typically the vague year is divided into 12 schematic months of 30 days each plus 5 epagomenal days. The vague year was used in the calendars of Ethiopia, Ancient Egypt, Iran, Armenia and in Mesoamerica among the Aztecs and Maya. It is still used by many Zoroastrian communities.\n\n\n=== Heliacal year ===\nA heliacal year is the interval between the heliacal risings of a star. It differs from the sidereal year for stars away from the ecliptic due mainly to the precession of the equinoxes.\n\n\n==== Sothic year ====\nThe Sothic year is the heliacal year, the interval between heliacal risings, of the star Sirius. It is currently less than the sidereal year and its duration is very close to the Julian year of 365.25 days.\n\n\n=== Gaussian year ===\nThe Gaussian year is the sidereal year for a planet of negligible mass (relative to the Sun) and unperturbed by other planets that is governed by the Gaussian gravitational constant. Such a planet would be slightly closer to the Sun than Earth's mean distance. Its length is:\n\n365.2568983 days (365 d 6 h 9 min 56 s).\n\n\n=== Besselian year ===\nThe Besselian year is a tropical year that starts when the (fictitious) mean Sun reaches an ecliptic longitude of 280\u00b0. This is currently on or close to January 1. It is named after the 19th-century German astronomer and mathematician Friedrich Bessel. The following equation can be used to compute the current Besselian epoch (in years):\n\nB = 1900.0 + (Julian dateTT \u2212 2415020.31352) / 365.242198781\nThe TT subscript indicates that for this formula, the Julian date should use the Terrestrial Time scale, or its predecessor, ephemeris time.\n\n\n=== Variation in the length of the year and the day ===\n\nThe exact length of an astronomical year changes over time.\n\nThe positions of the equinox and solstice points with respect to the apsides of Earth's orbit change: the equinoxes and solstices move westward relative to the stars because of precession, and the apsides move in the other direction because of the long-term effects of gravitational pull by the other planets. Since the speed of the Earth varies according to its position in its orbit as measured from its perihelion, Earth's speed when in a solstice or equinox point changes over time: if such a point moves toward perihelion, the interval between two passages decreases a little from year to year; if the point moves towards aphelion, that period increases a little from year to year. So a \"tropical year\" measured from one passage of the northward (\"vernal\") equinox to the next, differs from the one measured between passages of the southward (\"autumnal\") equinox. The average over the full orbit does not change because of this, so the length of the average tropical year does not change because of this second-order effect.\nEach planet's movement is perturbed by the gravity of every other planet. This leads to short-term fluctuations in its speed, and therefore its period from year to year. Moreover, it causes long-term changes in its orbit, and therefore also long-term changes in these periods.\nTidal drag between the Earth and the Moon and Sun increases the length of the day and of the month (by transferring angular momentum from the rotation of the Earth to the revolution of the Moon); since the apparent mean solar day is the unit with which we measure the length of the year in civil life, the length of the year appears to decrease. The rotation rate of the Earth is also changed by factors such as post-glacial rebound and sea level rise.\nNumerical value of year variation\nMean year lengths in this section are calculated for 2000, and differences in year lengths, compared to 2000, are given for past and future years. In the tables a day is 86,400 SI seconds long.\n\n\n=== Summary ===\nSome of the year lengths in this table are in average solar days, which are slowly getting longer (at a rate that cannot be exactly predicted in advance) and are now around 86,400.002 SI seconds.\n\nAn average Gregorian year may be said to be 365.2425 days (52.1775 weeks, and if an hour is defined as one twenty-fourth of a day, 8765.82 hours, 525949.2 minutes or 31556952 seconds). Note however that in absolute time the average Gregorian year is not adequately defined unless the period of the averaging (start and end dates) is stated, because each period of 400 years is longer (by more than 1000 seconds) than the preceding one as the rotation of the Earth slows. In this calendar, a common year is 365 days (8760 hours, 525600 minutes or 31536000 seconds), and a leap year is 366 days (8784 hours, 527040 minutes or 31622400 seconds). The 400-year civil cycle of the Gregorian calendar has 146097 days and hence exactly 20871 weeks.\n\n\n== Greater astronomical years ==\n\n\n=== Equinoctial cycle ===\nThe Great Year, or equinoctial cycle, corresponds to a complete revolution of the equinoxes around the ecliptic. Its length is about 25,700 years.\n\n\n=== Galactic year ===\nThe Galactic year is the time it takes Earth's Solar System to revolve once around the Galactic Center. It comprises roughly 230 million Earth years.\n\n\n== Seasonal year ==\n\nA seasonal year is the time between successive recurrences of a seasonal event such as the flooding of a river, the migration of a species of bird, the flowering of a species of plant, the first frost, or the first scheduled game of a certain sport. All of these events can have wide variations of more than a month from year to year.\n\n\n== Symbols and abbreviations ==\nA common symbol for the year as a unit of time is \"a\", taken from the Latin word annus.\nFor example, the U.S. National Institute of Standards and Technology (NIST) Guide for the Use of the International System of Units (SI) supports the symbol \"a\" as the unit of time for a year.\nIn English, the abbreviations \"y\" or \"yr\" are more commonly used in non-scientific literature. In some Earth sciences branches (geology and paleontology), \"kyr, myr, byr\" (thousands, millions, and billions of years, respectively) and similar abbreviations are used to denote intervals of time remote from the present. In astronomy the abbreviations kyr, Myr and Gyr are in common use for kiloyears, megayears and gigayears.\nThe Unified Code for Units of Measure (UCUM) disambiguates the varying symbologies of ISO 1000, ISO 2955 and ANSI X3.50 by using:\n\nat = 365.24219 days for the mean tropical year;\naj = 365.25 days for the mean Julian year;\nag = 365.2425 days for the mean Gregorian year;\nIn the UCUM, the symbol \"a\", without any qualifier, equals 1 aj.\nThe UCUM also minimizes confusion with are, a unit of area, by using the abbreviation \"ar\".\nSince 1989, the International Astronomical Union (IAU) recognizes the symbol \"a\" rather than \"yr\" for a year, notes the different kinds of year, and recommends adopting the Julian year of 365.25 days, unless otherwise specified (IAU Style Manual).\nSince 1987, the International Union of Pure and Applied Physics (IUPAP) notes \"a\" as the general symbol for the time unit year (IUPAP Red Book).\nSince 1993, the International Union of Pure and Applied Chemistry (IUPAC) Green Book also uses the same symbol \"a\", notes the difference between Gregorian year and Julian year, and adopts the former (a=365.2425 days), also noted in the IUPAC Gold Book.\nIn 2011, the IUPAC and the International Union of Geological Sciences jointly recommended defining the \"annus\", with symbol \"a\", as the length of the tropical year in the year 2000:\n\na = 31556925.445 seconds (approximately 365.24219265 ephemeris days)\nThis differs from the above definition of 365.25 days by about 20 parts per million. The joint document says that definitions such as the Julian year \"bear an inherent, pre-programmed obsolescence because of the variability of Earth's orbital movement\", but then proposes using the length of the tropical year as of 2000 AD (specified down to the millisecond), which suffers from the same problem. (The tropical year oscillates with time by more than a minute.)\nThe notation has proved controversial as it conflicts with an earlier convention among geoscientists to use \"a\" specifically for \"years ago\" (e.g. 1 Ma for 1 million years ago), and \"y\" or \"yr\" for a one-year time period.\nHowever, this historical practice does not comply with the NIST Guide, considering the unacceptability of mixing information concerning the physical quantity being measured (in this case, time intervals or points in time) with the units and also the unacceptability of using abbreviations for units.\nFurthermore, according to the UK Metric Association (UKMA), language-independent symbols are more universally understood (UKMA Style guide).\n\n\n=== SI prefix multipliers ===\n\nFor the following, there are alternative forms that elide the consecutive vowels, such as kilannus, megannus, etc. The exponents and exponential notations are typically used for calculating and in displaying calculations, and for conserving space, as in tables of data.\n\n\n=== Abbreviations for \"years ago\" ===\n\nIn geology and paleontology, a distinction sometimes is made between abbreviation \"yr\" for years and \"ya\" for years ago, combined with prefixes for thousand, million, or billion. In archaeology, dealing with more recent periods, normally expressed dates, e.g. \"10,000 BC\", may be used as a more traditional form than Before Present (\"BP\").\nThese abbreviations include:\n\nUse of \"mya\" and \"bya\" is deprecated in modern geophysics, the recommended usage being \"Ma\" and \"Ga\" for dates Before Present, but \"m.y.\" for the durations of epochs. This ad hoc distinction between \"absolute\" time and time intervals is somewhat controversial amongst members of the Geological Society of America.\n\n\n== See also ==\n\n\n== References ==\n\n\n=== Notes ===\n\n\n== Further reading ==\nFraser, Julius Thomas (1987). Time, the Familiar Stranger (illustrated ed.). Amherst: University of Massachusetts Press. Bibcode:1988tfs..book.....F. ISBN 978-0-87023-576-4. OCLC 15790499.\nWhitrow, Gerald James (2003). What is Time?. Oxford: Oxford University Press. ISBN 978-0-19-860781-6. OCLC 265440481.",
        "unit": "petayear",
        "url": "https://en.wikipedia.org/wiki/Petayear"
    },
    {
        "_id": "tco2e",
        "clean": "tco2e",
        "text": "Global warming potential (GWP) is an index to measure how much infrared thermal radiation a greenhouse gas would absorb over a given time frame after it has been added to the atmosphere (or emitted to the atmosphere). The GWP makes different greenhouse gases comparable with regard to their \"effectiveness in causing radiative forcing\".:\u200a2232\u200a It is expressed as a multiple of the radiation that would be absorbed by the same mass of added carbon dioxide (CO2), which is taken as a reference gas. Therefore, the GWP has a value of 1 for CO2. For other gases it depends on how strongly the gas absorbs infrared thermal radiation, how quickly the gas leaves the atmosphere, and the time frame being considered.\nFor example, methane has a GWP over 20 years (GWP-20) of 81.2 meaning that, for example, a leak of a tonne of methane is equivalent to emitting 81.2 tonnes of carbon dioxide measured over 20 years. As methane has a much shorter atmospheric lifetime than carbon dioxide, its GWP is much less over longer time periods, with a GWP-100 of 27.9 and a GWP-500 of 7.95.:\u200a7SM-24\u200a\nThe carbon dioxide equivalent (CO2e or CO2eq or CO2-e or CO2-eq) can be calculated from the GWP. For any gas, it is the mass of CO2 that would warm the earth as much as the mass of that gas. Thus it provides a common scale for measuring the climate effects of different gases. It is calculated as GWP times mass of the other gas.\n\n\n== Definition ==\n\nThe global warming potential (GWP) is defined as an \"index measuring the radiative forcing following an emission of a unit mass of a given substance, accumulated over a chosen time horizon, relative to that of the reference substance, carbon dioxide (CO2). The GWP thus represents the combined effect of the differing times these substances remain in the atmosphere and their effectiveness in causing radiative forcing.\":\u200a2232\u200a\nIn turn, radiative forcing is a scientific concept used to quantify and compare the external drivers of change to Earth's energy balance.:\u200a1\u20134\u200a Radiative forcing is the change in energy flux in the atmosphere caused by natural or anthropogenic factors of climate change as measured in watts per meter squared.\n\n\n=== GWP in policymaking ===\nAs governments develop policies to combat emissions from high-GWP sources, policymakers have chosen to use the 100-year GWP scale as the standard in international agreements. The Kigali Amendment to the Montreal Protocol sets the global phase-down of hydrofluorocarbons (HFCs), a group of high-GWP compounds. It requires countries to use a set of GWP100 values equal to those published in the IPCC's Fourth Assessment Report (AR4). This allows policymakers to have one standard for comparison instead of changing GWP values in new assessment reports. One exception to the GWP100 standard exists: New York state\u2019s Climate Leadership and Community Protection Act requires the use of GWP20, despite being a different standard from all other countries participating in phase downs of HFCs.\n\n\n== Calculated values ==\n\n\n=== Current values (IPCC Sixth Assessment Report from 2021) ===\nThe global warming potential (GWP) depends on both the efficiency of the molecule as a greenhouse gas and its atmospheric lifetime. GWP is measured relative to the same mass of CO2 and evaluated for a specific timescale. Thus, if a gas has a high (positive) radiative forcing but also a short lifetime, it will have a large GWP on a 20-year scale but a small one on a 100-year scale. Conversely, if a molecule has a longer atmospheric lifetime than CO2 its GWP will increase when the timescale is considered. Carbon dioxide is defined to have a GWP of 1 over all time periods.\nMethane has an atmospheric lifetime of 12 \u00b1 2 years.:\u200aTable 7.15\u200a The 2021 IPCC report lists the GWP as 83 over a time scale of 20 years, 30 over 100 years and 10 over 500 years.:\u200aTable 7.15\u200a The decrease in GWP at longer times is because methane decomposes to water and CO2 through chemical reactions in the atmosphere. Similarly the third most important GHG, nitrous oxide (N2O), is a common gas emitted through the denitrification part of the nitrogen cycle. It has a lifetime of 109 years and an even higher GWP level running at 273 over 20 and 100 years.\nExamples of the atmospheric lifetime and GWP relative to CO2 for several greenhouse gases are given in the following table:\n\nEstimates of GWP values over 20, 100 and 500 years are periodically compiled and revised in reports from the Intergovernmental Panel on Climate Change. The most recent report is the IPCC Sixth Assessment Report (Working Group I) from 2023.\nThe IPCC lists many other substances not shown here.  Some have high GWP but only a low concentration in the atmosphere.\nThe values given in the table assume the same mass of compound is analyzed; different ratios will result from the conversion of one substance to another. For instance, burning methane to carbon dioxide would reduce the global warming impact, but by a smaller factor than 25:1 because the mass of methane burned is less than the mass of carbon dioxide released (ratio 1:2.74). For a starting amount of 1 tonne of methane, which has a GWP of 25, after combustion there would be 2.74 tonnes of CO2, each tonne of which has a GWP of 1. This is a net reduction of 22.26 tonnes of GWP, reducing the global warming effect by a ratio of 25:2.74 (approximately 9 times).\n\n\n=== Earlier values from 2007 ===\nThe values provided in the table below are from 2007 when they were published in the IPCC Fourth Assessment Report. These values are still used (as of 2020) for some comparisons.\n\n\n=== Importance of time horizon ===\nA substance's GWP depends on the number of years (denoted by a subscript) over which the potential is calculated. A gas which is quickly removed from the atmosphere may initially have a large effect, but for longer time periods, as it has been removed, it becomes less important. Thus methane has a potential of 25 over 100 years (GWP100 = 25) but 86 over 20 years (GWP20 = 86); conversely sulfur hexafluoride has a GWP of 22,800 over 100 years but 16,300 over 20 years (IPCC Third Assessment Report). The GWP value depends on how the gas concentration decays over time in the atmosphere. This is often not precisely known and hence the values should not be considered exact. For this reason when quoting a GWP it is important to give a reference to the calculation.\nThe GWP for a mixture of gases can be obtained from the mass-fraction-weighted average of the GWPs of the individual gases.\nCommonly, a time horizon of 100 years is used by regulators.\n\n\n=== Water vapour ===\n\nWater vapour does contribute to anthropogenic global warming, but as the GWP is defined, it is negligible for H2O: an estimate gives a 100-year GWP between -0.001 and 0.0005.\nH2O can function as a greenhouse gas because it has a profound infrared absorption spectrum with more and broader absorption bands than CO2. Its concentration in the atmosphere is limited by air temperature, so that radiative forcing by water vapour increases with global warming (positive feedback). But the GWP definition excludes indirect effects. GWP definition is also based on emissions, and anthropogenic emissions of water vapour (cooling towers, irrigation) are removed via precipitation within weeks, so its GWP is negligible.\n\n\n== Calculation methods ==\n\nWhen calculating the GWP of a greenhouse gas, the value depends on the following factors:\n\nthe absorption of infrared radiation by the given gas\nthe time horizon of interest (integration period)\nthe atmospheric lifetime of the gas\nA high GWP correlates with a large infrared absorption and a long atmospheric lifetime. The dependence of GWP on the wavelength of absorption is more complicated. Even if a gas absorbs radiation efficiently at a certain wavelength, this may not affect its GWP much, if the atmosphere already absorbs most radiation at that wavelength. A gas has the most effect if it absorbs in a \"window\" of wavelengths where the atmosphere is fairly transparent. The dependence of GWP as a function of wavelength has been found empirically and published as a graph.\nBecause the GWP of a greenhouse gas depends directly on its infrared spectrum, the use of infrared spectroscopy to study greenhouse gases is centrally important in the effort to understand the impact of human activities on global climate change.\nJust as radiative forcing provides a simplified means of comparing the various factors that are believed to influence the climate system to one another, global warming potentials (GWPs) are one type of simplified index based upon radiative properties that can be used to estimate the potential future impacts of emissions of different gases upon the climate system in a relative sense. GWP is based on a number of factors, including the radiative efficiency (infrared-absorbing ability) of each gas relative to that of carbon dioxide, as well as the decay rate of each gas (the amount removed from the atmosphere over a given number of years) relative to that of carbon dioxide.\nThe radiative forcing capacity (RF) is the amount of energy per unit area, per unit time, absorbed by the greenhouse gas, that would otherwise be lost to space. It can be expressed by the formula:\n\n  \n    \n      \n        \n          \n            R\n            F\n          \n        \n        =\n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            100\n          \n        \n        \n          \n            abs\n          \n          \n            i\n          \n        \n        \u22c5\n        \n          F\n          \n            i\n          \n        \n        \n          /\n        \n        \n          (\n          \n            \n              l\n            \n            \u22c5\n            \n              d\n            \n          \n          )\n        \n      \n    \n    {\\displaystyle {\\mathit {RF}}=\\sum _{i=1}^{100}{\\text{abs}}_{i}\\cdot F_{i}/\\left({\\text{l}}\\cdot {\\text{d}}\\right)}\n  \n\nwhere the subscript i represents a wavenumber interval of 10 inverse centimeters. Absi represents the integrated infrared absorbance of the sample in that interval, and Fi represents the RF for that interval.\nThe Intergovernmental Panel on Climate Change (IPCC) provides the generally accepted values for GWP, which changed slightly between 1996 and 2001, except for methane, which had its GWP almost doubled. An exact definition of how GWP is calculated is to be found in the IPCC's 2001 Third Assessment Report. The GWP is defined as the ratio of the time-integrated radiative forcing from the instantaneous release of 1 kg of a trace substance relative to that of 1 kg of a reference gas:\n\n  \n    \n      \n        \n          \n            G\n            W\n            P\n          \n        \n        \n          (\n          x\n          )\n        \n        =\n        \n          \n            \n              a\n              \n                x\n              \n            \n            \n              a\n              \n                r\n              \n            \n          \n        \n        \n          \n            \n              \n                \u222b\n                \n                  0\n                \n                \n                  \n                    T\n                    H\n                  \n                \n              \n              [\n              x\n              ]\n              (\n              t\n              )\n              \n              d\n              t\n            \n            \n              \n                \u222b\n                \n                  0\n                \n                \n                  \n                    T\n                    H\n                  \n                \n              \n              [\n              r\n              ]\n              (\n              t\n              )\n              \n              d\n              t\n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathit {GWP}}\\left(x\\right)={\\frac {a_{x}}{a_{r}}}{\\frac {\\int _{0}^{\\mathit {TH}}[x](t)\\,dt}{\\int _{0}^{\\mathit {TH}}[r](t)\\,dt}}}\n  \n\nwhere TH is the time horizon over which the calculation is considered; ax is the radiative efficiency due to a unit increase in atmospheric abundance of the substance (i.e., Wm\u22122 kg\u22121) and [x](t) is the time-dependent decay in abundance of the substance following an instantaneous release of it at time t=0. The denominator contains the corresponding quantities for the reference gas (i.e. CO2). The radiative efficiencies ax and ar are not necessarily constant over time. While the absorption of infrared radiation by many greenhouse gases varies linearly with their abundance, a few important ones display non-linear behaviour for current and likely future abundances (e.g., CO2, CH4, and N2O). For those gases, the relative radiative forcing will depend upon abundance and hence upon the future scenario adopted.\nSince all GWP calculations are a comparison to CO2 which is non-linear, all GWP values are affected. Assuming otherwise as is done above will lead to lower GWPs for other gases than a more detailed approach would. Clarifying this, while increasing CO2 has less and less effect on radiative absorption as ppm concentrations rise, more powerful greenhouse gases like methane and nitrous oxide have different thermal absorption frequencies to CO2 that are not filled up (saturated) as much as CO2, so rising ppms of these gases are far more significant.\n\n\n== Applications ==\n\n\n=== Carbon dioxide equivalent ===\nCarbon dioxide equivalent (CO2e or CO2eq or CO2-e) of a quantity of gas is calculated from its GWP. For any gas, it is the mass of CO2 which would warm the earth as much as the mass of that gas. Thus it provides a common scale for measuring the climate effects of different gases. It is calculated as GWP multiplied by mass of the other gas. For example, if a gas has GWP of 100, two tonnes of the gas have CO2e of 200 tonnes, and 9 tonnes of the gas has CO2e of 900 tonnes.\nOn a global scale, the warming effects of one or more greenhouse gases in the atmosphere can also be expressed as an equivalent atmospheric concentration of CO2. CO2e can then be the atmospheric concentration of CO2 which would warm the earth as much as a particular concentration of some other gas or of all gases and aerosols in the atmosphere. For example, CO2e of 500 parts per million would reflect a mix of atmospheric gases which warm the earth as much as 500 parts per million of CO2 would warm it. Calculation of the equivalent atmospheric concentration of CO2 of an atmospheric greenhouse gas or aerosol is more complex and involves the atmospheric concentrations of those gases, their GWPs, and the ratios of their molar masses to the molar mass of CO2.\nCO2e calculations depend on the time-scale chosen, typically 100 years or 20 years, since gases decay in the atmosphere or are absorbed naturally, at different rates.\nThe following units are commonly used:\n\nBy the UN climate change panel (IPCC): billion metric tonnes = n\u00d7109 tonnes of CO2 equivalent (GtCO2eq)\nIn industry: million metric tonnes of carbon dioxide equivalents (MMTCDE) and MMT CO2eq.\nFor vehicles: grams of carbon dioxide equivalent per mile (gCO2e/mile) or per kilometer (gCO2e/km)\nFor example, the table above shows GWP for methane over 20 years at 86 and nitrous oxide at 289, so emissions of 1 million tonnes of methane or nitrous oxide are equivalent to emissions of 86 or 289 million tonnes of carbon dioxide, respectively.\n\n\n=== Use in Kyoto Protocol and for reporting to UNFCCC ===\nUnder the Kyoto Protocol, in 1997 the Conference of the Parties standardized international reporting, by deciding (see decision number 2/CP.3) that the values of GWP calculated for the IPCC Second Assessment Report were to be used for converting the various greenhouse gas emissions into comparable CO2 equivalents.\nAfter some intermediate updates, in 2013 this standard was updated by the Warsaw meeting of the UN Framework Convention on Climate Change (UNFCCC, decision number 24/CP.19) to require using a new set of 100-year GWP values. They published these values in Annex III, and they took them from the IPCC Fourth Assessment Report, which had been published in 2007. Those 2007 estimates are still used for international comparisons through 2020, although the latest research on warming effects has found other values, as shown in the tables above.\nThough recent reports reflect more scientific accuracy, countries and companies continue to use the IPCC Second Assessment Report (SAR) and IPCC Fourth Assessment Report values for reasons of comparison in their emission reports. The IPCC Fifth Assessment Report has skipped the 500-year values but introduced GWP estimations including the climate-carbon feedback (f) with a large amount of uncertainty.\n\n\n== Other metrics to compare greenhouse gases ==\nThe Global Temperature change Potential (GTP) is another way to compare gases. While GWP estimates infrared thermal radiation absorbed, GTP estimates the resulting rise in average surface temperature of the world, over the next 20, 50 or 100 years, caused by a greenhouse gas, relative to the temperature rise which the same mass of CO2 would cause. Calculation of GTP requires modelling how the world, especially the oceans, will absorb heat. GTP is published in the same IPCC tables with GWP.\nAnother metric called GWP* (pronounced \"GWP star\") has been proposed to take better account of short-lived climate pollutants (SLCPs) such as methane. A permanent increase in the rate of emission of an SLCP has a similar effect to that of a one-time emission of an amount of carbon dioxide, because both raise the radiative forcing permanently or (in the case of carbon dioxide) practically permanently (since the CO2 stays in the air for a long time). GWP* therefore assigns an increase in emission rate of an SLCP a supposedly equivalent amount (tonnes) of CO2. However GWP* has been criticised both for its suitability as a metric and for inherent design features which can perpetuate injustices and inequity. Developing countries whose emissions of SLCPs are increasing are \"penalized\", while developed countries such as Australia or New Zealand which have steady emissions of SLCPs are not penalized in this way, though they may be penalized for their emissions of CO2.\n\n\n== See also ==\n\nCarbon accounting\nCarbon footprint\nEmission intensity\n\n\n== References ==\n\n\n=== Sources ===\nSchimel, D.; Alves, D.; Enting, I.; Heimann, M.; et al. (1995). \"Chapter 2: Radiative Forcing of Climate Change\". Climate Change 1995: The Science of Climate Change. Contribution of Working Group I to the Second Assessment Report of the Intergovernmental Panel on Climate Change (IPCC SAR WG1). pp. 65\u2013132.\nForster, P.; Ramaswamy, V.; Artaxo, P.; Berntsen, T.; et al. (2007). \"Chapter 2: Changes in Atmospheric Constituents and Radiative Forcing\" (PDF). Climate Change 2013: The Physical Science Basis. Contribution of Working Group I to the Fourth Assessment Report of the Intergovernmental Panel on Climate Change. pp. 129\u2013234.\nMyhre, G.; Shindell, D.; Br\u00e9on, F.-M.; Collins, W.; et al. (2013). \"Chapter 8: Anthropogenic and Natural Radiative Forcing\" (PDF). Climate Change 2013: The Physical Science Basis. Contribution of Working Group I to the Fifth Assessment Report of the Intergovernmental Panel on Climate Change. pp. 659\u2013740.\n\n\n== External links ==\nList of Global Warming Potentials and Atmospheric Lifetimes from the U.S. EPA\nGWP and the different meanings of CO2e explained",
        "unit": "tco2e",
        "url": "https://en.wikipedia.org/wiki/tco2e"
    },
    {
        "_id": "Megabyte",
        "clean": "Megabyte",
        "text": "The megabyte is a multiple of the unit byte for digital information. Its recommended unit symbol is MB.  The unit prefix mega is a multiplier of 1000000 (106) in the International System of Units (SI). Therefore, one megabyte is one million bytes of information. This definition has been incorporated into the International System of Quantities.\nIn the computer and information technology fields, other definitions have been used that arose for historical reasons of convenience. A common usage has been to designate one megabyte as 1048576bytes (220 B), a quantity that conveniently expresses the binary architecture of digital computer memory. Standards bodies have deprecated this binary usage of the mega- prefix in favor of a new set of binary prefixes, by means of which the quantity 220 B is named mebibyte (symbol MiB).\n\n\n== Definitions ==\nThe unit megabyte is commonly used for 10002 (one million) bytes or 10242 bytes. The interpretation of using base 1024 originated as technical jargon for the byte multiples that needed to be expressed by the powers of 2 but lacked a convenient name. As 1024 (210) approximates 1000 (103), roughly corresponding to the SI prefix kilo-, it was a convenient term to denote the binary multiple. In 1999, the International Electrotechnical Commission (IEC) published standards for binary prefixes requiring the use of megabyte to denote 10002 bytes, and mebibyte to denote 10242 bytes.  By the end of 2009, the IEC Standard had been adopted by the IEEE, EU, ISO and NIST. Nevertheless, the term megabyte continues to be widely used with different meanings.\n\nBase 10\n1 MB = 1000000 bytes (= 10002 B = 106 B) is the definition following the rules of the International System of Units (SI), and the International Electrotechnical Commission (IEC). This definition is used in computer networking contexts and most storage media, particularly hard drives, flash-based storage, and DVDs, and is also consistent with the other uses of the SI prefix in computing, such as CPU clock speeds or measures of performance. The Mac OS X 10.6 file manager is a notable example of this usage in software. Since Snow Leopard, file sizes are reported in decimal units.\nIn this convention, one thousand megabytes (1000 MB) is equal to one gigabyte (1 GB), where 1 GB is one billion bytes.\n\nBase 2\n\n1 MB = 1048576 bytes (= 10242 B = 220 B) is the definition used by Microsoft Windows in reference to computer memory, such as random-access memory (RAM). This definition is synonymous with the unambiguous binary unit mebibyte. In this convention, one thousand and twenty-four megabytes (1024 MB) is equal to one gigabyte (1 GB), where 1 GB is 10243 bytes (i.e., 1 GiB).\nMixed\n1 MB = 1024000 bytes (= 1000\u00d71024 B) is the definition used to describe the formatted capacity of the 1.44 MB 3.5-inch HD floppy disk, which actually has a capacity of 1474560bytes.\nRandomly addressable semiconductor memory doubles in size for each address lane added to an integrated circuit package, which favors counts that are powers of two. The capacity of a disk drive is the product of the sector size, number of sectors per track, number of tracks per side, and the number of disk platters in the drive. Changes in any of these factors would not usually double the size.\n\n\n== Examples of use ==\n\nDepending on compression methods and file format, a megabyte of data can roughly be:\n\na 1 megapixel bitmap image (e.g. ~1152 \u00d7 864) with 256 colors (8 bits/pixel color depth) stored without any compression.\n6 seconds of 44.1 kHz/16 bit uncompressed CD audio.\n1 minute of 128 kbit/s MP3 lossy compressed audio.\na typical English book volume in plain text format (500 pages \u00d7 2000 characters per page).\nThe novel The Picture of Dorian Gray, by Oscar Wilde, hosted on Project Gutenberg as an uncompressed plain text file, is 0.429 MB. Great Expectations is 0.994 MB, and Moby Dick is 1.192 MB. \nThe human genome consists of DNA representing 800 MB of data. The parts that differentiate one person from another can be compressed to 4 MB.\n\n\n== See also ==\nTimeline of binary prefixes\nGigabyte \u00a7 Consumer confusion\n\n\n== References ==\n\n\n== External links ==\nHistorical Notes About The Cost Of Hard Drive Storage Space\nthe megabyte (established definition in Networking and Storage industries; from whatis.com)\nInternational Electrotechnical Commission definitions\nIEC prefixes and symbols for binary multiples Archived 2004-06-15 at the Wayback Machine",
        "unit": "megabyte",
        "url": "https://en.wikipedia.org/wiki/Megabyte"
    },
    {
        "_id": "Kilobit",
        "clean": "Kilobit",
        "text": "The bit is the most basic unit of information in computing and digital communication. The name is a portmanteau of binary digit. The bit represents a logical state with one of two possible values. These values are most commonly represented as either \"1\" or \"0\", but other representations such as true/false, yes/no, on/off, or +/\u2212 are also widely used.\nThe relation between these values and the physical states of the underlying storage or device is a matter of convention, and different assignments may be used even within the same device or program. It may be physically implemented with a two-state device.\nA contiguous group of binary digits is commonly called a bit string, a bit vector, or a single-dimensional (or multi-dimensional) bit array.\nA group of eight bits is called one byte, but historically the size of the byte is not strictly defined. Frequently, half, full, double and quadruple words consist of a number of bytes which is a low power of two. A string of four bits is usually a nibble.\nIn information theory, one bit is the information entropy of a random binary variable that is 0 or 1 with equal probability, or the information that is gained when the value of such a variable becomes known. As a unit of information, the bit is also known as a shannon, named after Claude E. Shannon.\nThe symbol for the binary digit is either \"bit\", per the IEC 80000-13:2008 standard, or the lowercase character \"b\", per the IEEE 1541-2002 standard. Use of the latter may create confusion with the capital \"B\" which is the international standard symbol for the byte.\n\n\n== History ==\nThe encoding of data by discrete bits was used in the punched cards invented by Basile Bouchon and Jean-Baptiste Falcon (1732), developed by Joseph Marie Jacquard (1804), and later adopted by Semyon Korsakov, Charles Babbage, Herman Hollerith, and early computer manufacturers like IBM. A variant of that idea was the perforated paper tape. In all those systems, the medium (card or tape) conceptually carried an array of hole positions; each position could be either punched through or not, thus carrying one bit of information. The encoding of text by bits was also used in Morse code (1844) and early digital communications machines such as teletypes and stock ticker machines (1870).\nRalph Hartley suggested the use of a logarithmic measure of information in 1928. Claude E. Shannon first used the word \"bit\" in his seminal 1948 paper \"A Mathematical Theory of Communication\". He attributed its origin to John W. Tukey, who had written a Bell Labs memo on 9 January 1947 in which he contracted \"binary information digit\" to simply \"bit\".\n\n\n== Physical representation ==\nA bit can be stored by a digital device or other physical system that exists in either of two possible distinct states. These may be the two stable states of a flip-flop, two positions of an electrical switch, two distinct voltage or current levels allowed by a circuit, two distinct levels of light intensity, two directions of magnetization or polarization, the orientation of reversible double stranded DNA, etc.\nBits can be implemented in several forms. In most modern computing devices, a bit is usually represented by an electrical voltage or current pulse, or by the electrical state of a flip-flop circuit.\nFor devices using positive logic, a digit value of 1 (or a logical value of true) is represented by a more positive voltage relative to the representation of 0. Different logic families require different voltages, and variations are allowed to account for component aging and noise immunity. For example, in transistor\u2013transistor logic (TTL) and compatible circuits, digit values 0 and 1 at the output of a device are represented by no higher than 0.4 V and no lower than 2.6 V, respectively; while TTL inputs are specified to recognize 0.8 V or below as 0 and 2.2 V or above as 1.\n\n\n=== Transmission and processing ===\nBits are transmitted one at a time in serial transmission, and by a multiple number of bits in parallel transmission. A bitwise operation optionally processes bits one at a time. Data transfer rates are usually measured in decimal SI multiples of the unit bit per second (bit/s), such as kbit/s.\n\n\n=== Storage ===\nIn the earliest non-electronic information processing devices, such as Jacquard's loom or Babbage's Analytical Engine, a bit was often stored as the position of a mechanical lever or gear, or the presence or absence of a hole at a specific point of a paper card or tape. The first electrical devices for discrete logic (such as elevator and traffic light control circuits, telephone switches, and Konrad Zuse's computer) represented bits as the states of electrical relays which could be either \"open\" or \"closed\". When relays were replaced by vacuum tubes, starting in the 1940s, computer builders experimented with a variety of storage methods, such as pressure pulses traveling down a mercury delay line, charges stored on the inside surface of a cathode-ray tube, or opaque spots printed on glass discs by photolithographic techniques.\nIn the 1950s and 1960s, these methods were largely supplanted by magnetic storage devices such as magnetic-core memory, magnetic tapes, drums, and disks, where a bit was represented by the polarity of magnetization of a certain area of a ferromagnetic film, or by a change in polarity from one direction to the other. The same principle was later used in the magnetic bubble memory developed in the 1980s, and is still found in various magnetic strip items such as metro tickets and some credit cards.\nIn modern semiconductor memory, such as dynamic random-access memory, the two values of a bit may be represented by two levels of electric charge stored in a capacitor. In certain types of programmable logic arrays and read-only memory, a bit may be represented by the presence or absence of a conducting path at a certain point of a circuit. In optical discs, a bit is encoded as the presence or absence of a microscopic pit on a reflective surface. In one-dimensional bar codes, bits are encoded as the thickness of alternating black and white lines.\n\n\n== Unit and symbol ==\nThe bit is not defined in the International System of Units (SI). However, the International Electrotechnical Commission issued standard IEC 60027, which specifies that the symbol for binary digit should be 'bit', and this should be used in all multiples, such as 'kbit', for kilobit. However, the lower-case letter 'b' is widely used as well and was recommended by the IEEE 1541 Standard (2002). In contrast, the upper case letter 'B' is the standard and customary symbol for byte.\n\n\n=== Multiple bits ===\n\nMultiple bits may be expressed and represented in several ways. For convenience of representing commonly reoccurring groups of bits in information technology, several units of information have traditionally been used. The most common is the unit byte, coined by Werner Buchholz in June 1956, which historically was used to represent the group of bits used to encode a single character of text (until UTF-8 multibyte encoding took over) in a computer and for this reason it was used as the basic addressable element in many computer architectures. The trend in hardware design converged on the most common implementation of using eight bits per byte, as it is widely used today. However, because of the ambiguity of relying on the underlying hardware design, the unit octet was defined to explicitly denote a sequence of eight bits.\nComputers usually manipulate bits in groups of a fixed size, conventionally named \"words\". Like the byte, the number of bits in a word also varies with the hardware design, and is typically between 8 and 80 bits, or even more in some specialized computers. In the early 21st century, retail personal or server computers have a word size of 32 or 64 bits.\nThe International System of Units defines a series of decimal prefixes for multiples of standardized units which are commonly also used with the bit and the byte. The prefixes kilo (103) through yotta (1024) increment by multiples of one thousand, and the corresponding units are the kilobit (kbit) through the yottabit (Ybit).\n\n\n== Information capacity and information compression ==\n\nWhen the information capacity of a storage system or a communication channel is presented in bits or bits per second, this often refers to binary digits, which is a computer hardware capacity to store binary data (0 or 1, up or down, current or not, etc.). Information capacity of a storage system is only an upper bound to the quantity of information stored therein. If the two possible values of one bit of storage are not equally likely, that bit of storage contains less than one bit of information. If the value is completely predictable, then the reading of that value provides no information at all (zero entropic bits, because no resolution of uncertainty occurs and therefore no information is available). If a computer file that uses n bits of storage contains only m < n bits of information, then that information can in principle be encoded in about m bits, at least on the average. This principle is the basis of data compression technology. Using an analogy, the hardware binary digits refer to the amount of storage space available (like the number of buckets available to store things), and the information content the filling, which comes in different levels of granularity (fine or coarse, that is, compressed or uncompressed information). When the granularity is finer\u2014when information is more compressed\u2014the same bucket can hold more.\nFor example, it is estimated that the combined technological capacity of the world to store information provides 1,300 exabytes of hardware digits. However, when this storage space is filled and the corresponding content is optimally compressed, this only represents 295 exabytes of information. When optimally compressed, the resulting carrying capacity approaches Shannon information or information entropy.\n\n\n== Bit-based computing ==\nCertain bitwise computer processor instructions (such as bit set) operate at the level of manipulating bits rather than manipulating data interpreted as an aggregate of bits.\nIn the 1980s, when bitmapped computer displays became popular, some computers provided specialized bit block transfer instructions to set or copy the bits that corresponded to a given rectangular area on the screen.\nIn most computers and programming languages, when a bit within a group of bits, such as a byte or word, is referred to, it is usually specified by a number from 0 upwards corresponding to its position within the byte or word. However, 0 can refer to either the most or least significant bit depending on the context.\n\n\n== Other information units ==\n\nSimilar to torque and energy in physics; information-theoretic information and data storage size have the same dimensionality of units of measurement, but there is in general no meaning to adding, subtracting or otherwise combining the units mathematically, although one may act as a bound on the other.\nUnits of information used in information theory include the shannon (Sh), the natural unit of information (nat) and the hartley (Hart). One shannon is the maximum amount of information needed to specify the state of one bit of storage. These are related by 1 Sh \u2248 0.693 nat \u2248 0.301 Hart.\nSome authors also define a binit as an arbitrary information unit equivalent to some fixed but unspecified number of bits.\n\n\n== See also ==\nBinary numeral system\nBit rate and baud rate\nBitstream\nByte\nEntropy (information theory)\nFuzzy bit\nInteger (computer science)\nNibble\nPrimitive data type\nP-bit (probabilistic bit)\nQubit (quantum bit)\nShannon (unit)\nTernary numeral system\nTrit (Trinary digit)\n\n\n== References ==\n\n\n== External links ==\n\nBit Calculator \u2013 a tool providing conversions between bit, byte, kilobit, kilobyte, megabit, megabyte, gigabit, gigabyte\nBitXByteConverter Archived 2016-04-06 at the Wayback Machine \u2013 a tool for computing file sizes, storage capacity, and digital information in various units",
        "unit": "kilobit",
        "url": "https://en.wikipedia.org/wiki/Kilobit"
    },
    {
        "_id": "Knot_(unit)",
        "clean": "Knot (unit)",
        "text": "The knot () is a unit of speed equal to one nautical mile per hour, exactly 1.852 km/h (approximately 1.151 mph or 0.514 m/s). The ISO standard symbol for the knot is kn. The same symbol is preferred by the Institute of Electrical and Electronics Engineers (IEEE), while kt is also common, especially in aviation, where it is the form recommended by the International Civil Aviation Organization (ICAO). The knot is a non-SI unit. The knot is used in meteorology, and in maritime and air navigation. A vessel travelling at 1 knot along a meridian travels approximately one minute of geographic latitude in one hour.\n\n\n== Definitions ==\n1 international knot =\n1 nautical mile per hour (by definition),\n1852.000 metres per hour (exactly),\n0.51444 metres per second (approximately),\n1.15078 miles per hour (approximately),\n20.25372 inches per second (approximately)\n1.68781 feet per second (approximately).\nThe length of the internationally agreed nautical mile is 1852 m. The US adopted the international definition in 1954, having previously used the US nautical mile (1853.248 m). The UK adopted the international nautical mile definition in 1970, having previously used the UK Admiralty nautical mile (6080 ft or 1853.184 m).\n\n (* = approximate values)\n\n\n== Usage ==\nThe speeds of vessels relative to the fluids in which they travel (boat speeds and air speeds) can be measured in knots. If so, for consistency, the speeds of navigational fluids (ocean currents, tidal streams, river currents and wind speeds) are also measured in knots. Thus, speed over the ground (SOG; ground speed (GS) in aircraft) and rate of progress towards a distant point (\"velocity made good\", VMG) can also be given in knots. Since 1979, the International Civil Aviation Organization list the knot as permitted for temporary use in aviation, but no end date to the temporary period has been agreed as of 2024.\n\n\n== Origin ==\nUntil the mid-19th century, vessel speed at sea was measured using a chip log. This consisted of a wooden panel, attached by line to a reel, and weighted on one edge to float perpendicularly to the water surface and thus present substantial resistance to the water moving around it. The chip log was cast over the stern of the moving vessel and the line allowed to pay out. Knots tied at a distance of 47 feet 3 inches (14.4018 m) from each other, passed through a sailor's fingers, while another sailor used a 30-second sand-glass (28-second sand-glass is the currently accepted timing) to time the operation. The knot count would be reported and used in the sailing master's dead reckoning and navigation. This method gives a value for the knot of 20+1\u20444 inches per second or 1.85166 kilometres per hour. The difference from the modern definition is less than 0.02%.\nDerivation of knots spacing:\n\n  \n    \n      \n        1\n         \n        \n          \n            kn\n          \n        \n        =\n        1852\n         \n        \n          \n            m/h\n          \n        \n        =\n        0.5144\n         \n        \n          \n            m/s\n          \n        \n      \n    \n    {\\displaystyle 1~{\\textrm {kn}}=1852~{\\textrm {m/h}}=0.5144~{\\textrm {m/s}}}\n  \n, so in \n  \n    \n      \n        28\n      \n    \n    {\\displaystyle 28}\n  \n seconds that is \n  \n    \n      \n        14.40\n      \n    \n    {\\displaystyle 14.40}\n  \n metres per knot.\n\n\n== Modern use ==\n\nAlthough the unit knot does not fit within the SI system, its retention for nautical and aviation use is important because the length of a nautical mile, upon which the knot is based, is closely related to the longitude/latitude geographic coordinate system. As a result, nautical miles and knots are convenient units to use when navigating an aircraft or ship.\nOn a standard nautical chart using Mercator projection, the horizontal (East\u2013West) scale varies with latitude. On a chart of the North Atlantic, the scale varies by a factor of two from Florida to Greenland. A single graphic scale, of the sort on many maps, would therefore be useless on such a chart. Since the length of a nautical mile, for practical purposes, is equivalent to about a minute of latitude, a distance in nautical miles on a chart can easily be measured by using dividers and the latitude scales on the sides of the chart. Recent British Admiralty charts have a latitude scale down the middle to make this even easier.\nSpeed is sometimes incorrectly expressed as \"knots per hour\", which would mean \"nautical miles per hour per hour\" and thus would refer to acceleration.\n\n\n=== Aeronautical terms ===\nPrior to 1969, airworthiness standards for civil aircraft in the United States Federal Aviation Regulations specified that distances were to be in statute miles, and speeds in miles per hour. In 1969, these standards were progressively amended to specify that distances were to be in nautical miles, and speeds in knots.\nThe following abbreviations are used to distinguish between various measurements of airspeed:\n\nTAS is \"knots true airspeed\", the airspeed of an aircraft relative to undisturbed air\nKIAS is \"knots indicated airspeed\", the speed shown on an aircraft's pitot-static airspeed indicator\nCAS is \"knots calibrated airspeed\", the indicated airspeed corrected for position error and instrument error\nEAS is \"knots equivalent airspeed\", the calibrated airspeed corrected for adiabatic compressible flow for the particular altitude\nThe indicated airspeed is close to the true airspeed only at sea level in standard conditions and at low speeds. At 11000 m (36000 ft), an indicated airspeed of 300 kn may correspond to a true airspeed of 500 kn in standard conditions.\n\n\n== See also ==\n\nBeaufort scale\nHull speed, which deals with theoretical estimates of practical maximum speed of displacement hulls\nKnot count\nKnotted cord\nMetre per second\nOrders of magnitude (speed)\nRope (unit)\n\n\n== References ==",
        "unit": "knot",
        "url": "https://en.wikipedia.org/wiki/Knot_(unit)"
    },
    {
        "_id": "Frequency",
        "clean": "Frequency",
        "text": "Frequency (symbol f), most often measured in hertz (symbol: Hz), is the number of occurrences of a repeating event per unit of time. It is also occasionally referred to as temporal frequency for clarity and to distinguish it from spatial frequency.  Ordinary frequency is related to angular frequency (symbol \u03c9, with SI unit radian per second) by a factor of 2\u03c0.  The period (symbol T) is the interval of time between events, so the period is the reciprocal of the frequency: T = 1/f.\nFrequency is an important parameter used in science and engineering to specify the rate of oscillatory and vibratory phenomena, such as mechanical vibrations, audio signals (sound), radio waves, and light.\nFor example, if a heart beats at a frequency of 120 times per minute (2 hertz), the period\u2014the time interval between beats\u2014is half a second (60 seconds divided by 120).\n\n\n== Definitions and units ==\n\nFor cyclical phenomena such as oscillations, waves, or for examples of simple harmonic motion, the term frequency is defined as the number of cycles or repetitions per unit of time. The conventional symbol for frequency is f or \u03bd (the Greek letter nu) is also used. The period T is the time taken to complete one cycle of an oscillation or rotation. The frequency and the period are related by the equation\n\n  \n    \n      \n        f\n        =\n        \n          \n            1\n            T\n          \n        \n        .\n      \n    \n    {\\displaystyle f={\\frac {1}{T}}.}\n  \n\nThe term temporal frequency is used to emphasise that the frequency is characterised by the number of occurrences of a repeating event per unit time.\nThe SI unit of frequency is the hertz (Hz), named after the German physicist Heinrich Hertz by the International Electrotechnical Commission in 1930. It was adopted by the CGPM (Conf\u00e9rence g\u00e9n\u00e9rale des poids et mesures) in 1960, officially replacing the previous name, cycle per second (cps). The SI unit for the period, as for all measurements of time, is the second. A traditional unit of frequency used with rotating mechanical devices, where it is termed rotational frequency, is revolution per minute, abbreviated r/min or rpm. 60 rpm is equivalent to one hertz.\n\n\n== Period versus frequency ==\nAs a matter of convenience, longer and slower waves, such as ocean surface waves, are more typically described by wave period rather than frequency. Short and fast waves, like audio and radio, are usually described by their frequency. Some commonly used conversions are listed below:\n\n\n== Related quantities ==\n\nRotational frequency, usually denoted by the Greek letter \u03bd (nu), is defined as the instantaneous rate of change of the number of rotations, N, with respect to time: \u03bd = dN/dt; it is a type of frequency applied to rotational motion.\nAngular frequency, usually denoted by the Greek letter \u03c9 (omega), is defined as the rate of change of angular displacement (during rotation), \u03b8 (theta), or the rate of change of the phase of a sinusoidal waveform (notably in oscillations and waves), or as the rate of change of the argument to the sine function:\n\n  \n    \n      \n        y\n        (\n        t\n        )\n        =\n        sin\n        \u2061\n        \u03b8\n        (\n        t\n        )\n        =\n        sin\n        \u2061\n        (\n        \u03c9\n        t\n        )\n        =\n        sin\n        \u2061\n        (\n        2\n        \n          \u03c0\n        \n        f\n        t\n        )\n      \n    \n    {\\displaystyle y(t)=\\sin \\theta (t)=\\sin(\\omega t)=\\sin(2\\mathrm {\\pi } ft)}\n  \n \n  \n    \n      \n        \n          \n            \n              \n                d\n              \n              \u03b8\n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        =\n        \u03c9\n        =\n        2\n        \n          \u03c0\n        \n        f\n        .\n      \n    \n    {\\displaystyle {\\frac {\\mathrm {d} \\theta }{\\mathrm {d} t}}=\\omega =2\\mathrm {\\pi } f.}\n  \n\nThe unit of angular frequency is the radian per second (rad/s) but, for discrete-time signals, can also be expressed as radians per sampling interval, which is a dimensionless quantity. Angular frequency is frequency multiplied by 2\u03c0.\nSpatial frequency, denoted here by \u03be (xi), is analogous to temporal frequency, but with a spatial measurement replacing time measurement, e.g.: \n  \n    \n      \n        y\n        (\n        t\n        )\n        =\n        sin\n        \u2061\n        \u03b8\n        (\n        t\n        ,\n        x\n        )\n        =\n        sin\n        \u2061\n        (\n        \u03c9\n        t\n        +\n        k\n        x\n        )\n      \n    \n    {\\displaystyle y(t)=\\sin \\theta (t,x)=\\sin(\\omega t+kx)}\n  \n \n  \n    \n      \n        \n          \n            \n              \n                d\n              \n              \u03b8\n            \n            \n              \n                d\n              \n              x\n            \n          \n        \n        =\n        k\n        =\n        2\n        \u03c0\n        \u03be\n        .\n      \n    \n    {\\displaystyle {\\frac {\\mathrm {d} \\theta }{\\mathrm {d} x}}=k=2\\pi \\xi .}\n  \n\nSpatial period or wavelength is the spatial analog to temporal period.\n\n\n== In wave propagation ==\n\nFor periodic waves in nondispersive media (that is, media in which the wave speed is independent of frequency), frequency has an inverse relationship to the wavelength, \u03bb (lambda). Even in dispersive media, the frequency f of a sinusoidal wave is equal to the phase velocity v of the wave divided by the wavelength \u03bb of the wave:\n\n  \n    \n      \n        f\n        =\n        \n          \n            v\n            \u03bb\n          \n        \n        .\n      \n    \n    {\\displaystyle f={\\frac {v}{\\lambda }}.}\n  \n\nIn the special case of electromagnetic waves in vacuum, then v = c, where c is the speed of light in vacuum, and this expression becomes\n\n  \n    \n      \n        f\n        =\n        \n          \n            c\n            \u03bb\n          \n        \n        .\n      \n    \n    {\\displaystyle f={\\frac {c}{\\lambda }}.}\n  \n\nWhen monochromatic waves travel from one medium to another, their frequency remains the same\u2014only their wavelength and speed change.\n\n\n== Measurement ==\n\nMeasurement of frequency can be done in the following ways:\n\n\n=== Counting ===\nCalculating the frequency of a repeating event is accomplished by counting the number of times that event occurs within a specific time period, then dividing the count by the period. For example, if 71 events occur within 15 seconds the frequency is:\n\n  \n    \n      \n        f\n        =\n        \n          \n            71\n            \n              15\n              \n              \n                s\n              \n            \n          \n        \n        \u2248\n        4.73\n        \n        \n          Hz\n        \n        .\n      \n    \n    {\\displaystyle f={\\frac {71}{15\\,{\\text{s}}}}\\approx 4.73\\,{\\text{Hz}}.}\n  \n\nIf the number of counts is not very large, it is more accurate to measure the time interval for a predetermined number of occurrences, rather than the number of occurrences within a specified time. The latter method introduces a random error into the count of between zero and one count, so on average half a count. This is called gating error and causes an average error in the calculated frequency of \n  \n    \n      \n        \u0394\n        f\n        =\n        \n          \n            1\n            \n              2\n              \n                T\n                \n                  m\n                \n              \n            \n          \n        \n      \n    \n    {\\textstyle \\Delta f={\\frac {1}{2T_{\\text{m}}}}}\n  \n, or a fractional error of \n  \n    \n      \n        \n          \n            \n              \u0394\n              f\n            \n            f\n          \n        \n        =\n        \n          \n            1\n            \n              2\n              f\n              \n                T\n                \n                  m\n                \n              \n            \n          \n        \n      \n    \n    {\\textstyle {\\frac {\\Delta f}{f}}={\\frac {1}{2fT_{\\text{m}}}}}\n  \n where \n  \n    \n      \n        \n          T\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle T_{\\text{m}}}\n  \n is the timing interval and \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n  \n is the measured frequency. This error decreases with frequency, so it is generally a problem at low frequencies where the number of counts N is small.\n\n\n=== Stroboscope ===\nAn old method of measuring the frequency of rotating or vibrating objects is to use a stroboscope. This is an intense repetitively flashing light (strobe light) whose frequency can be adjusted with a calibrated timing circuit. The strobe light is pointed at the rotating object and the frequency adjusted up and down. When the frequency of the strobe equals the frequency of the rotating or vibrating object, the object completes one cycle of oscillation and returns to its original position between the flashes of light, so when illuminated by the strobe the object appears stationary. Then the frequency can be read from the calibrated readout on the stroboscope. A downside of this method is that an object rotating at an integer multiple of the strobing frequency will also appear stationary.\n\n\n=== Frequency counter ===\n\nHigher frequencies are usually measured with a frequency counter. This is an electronic instrument which measures the frequency of an applied repetitive electronic signal and displays the result in hertz on a digital display. It uses digital logic to count the number of cycles during a time interval established by a precision quartz time base. Cyclic processes that are not electrical, such as the rotation rate of a shaft, mechanical vibrations, or sound waves, can be converted to a repetitive electronic signal by transducers and the signal applied to a frequency counter. As of 2018, frequency counters can cover the range up to about 100 GHz. This represents the limit of direct counting methods; frequencies above this must be measured by indirect methods.\n\n\n=== Heterodyne methods ===\nAbove the range of frequency counters, frequencies of electromagnetic signals are often measured indirectly utilizing heterodyning (frequency conversion). A reference signal of a known frequency near the unknown frequency is mixed with the unknown frequency in a nonlinear mixing device such as a diode. This creates a heterodyne or \"beat\" signal at the difference between the two frequencies.  If the two signals are close together in frequency the heterodyne is low enough to be measured by a frequency counter. This process only measures the difference between the unknown frequency and the reference frequency. To convert higher frequencies, several stages of heterodyning can be used. Current research is extending this method to infrared and light frequencies (optical heterodyne detection).\n\n\n== Examples ==\n\n\n=== Light ===\n\nVisible light is an electromagnetic wave, consisting of oscillating electric and magnetic fields traveling through space. The frequency of the wave determines its color: 400 THz (4\u00d71014 Hz) is red light, 800 THz (8\u00d71014 Hz) is violet light, and between these (in the range 400\u2013800 THz) are all the other colors of the visible spectrum. An electromagnetic wave with a frequency less than 4\u00d71014 Hz will be invisible to the human eye; such waves are called infrared (IR) radiation. At even lower frequency, the wave is called a microwave, and at still lower frequencies it is called a radio wave. Likewise, an electromagnetic wave with a frequency higher than 8\u00d71014 Hz will also be invisible to the human eye; such waves are called ultraviolet (UV) radiation. Even higher-frequency waves are called X-rays, and higher still are gamma rays.\nAll of these waves, from the lowest-frequency radio waves to the highest-frequency gamma rays, are fundamentally the same, and they are all called electromagnetic radiation. They all travel through vacuum at the same speed (the speed of light), giving them wavelengths inversely proportional to their frequencies.\n\n  \n    \n      \n        \n          c\n          =\n          f\n          \u03bb\n          ,\n        \n      \n    \n    {\\displaystyle \\displaystyle c=f\\lambda ,}\n  \n\nwhere c is the speed of light (c in vacuum or less in other media), f is the frequency and \u03bb is the wavelength.\nIn dispersive media, such as glass, the speed depends somewhat on frequency, so the wavelength is not quite inversely proportional to frequency.\n\n\n=== Sound ===\n\nSound propagates as mechanical vibration waves of pressure and displacement, in air or other substances. In general, frequency components of a sound determine its \"color\", its timbre. When speaking about the frequency (in singular) of a sound, it means the property that most determines its pitch.\nThe frequencies an ear can hear are limited to a specific range of frequencies.  The audible frequency range for humans is typically given as being between about 20 Hz and 20,000 Hz (20 kHz), though the high frequency limit usually reduces with age. Other species have different hearing ranges. For example, some dog breeds can perceive vibrations up to 60,000 Hz.\nIn many media, such as air, the speed of sound is approximately independent of frequency, so the wavelength of the sound waves (distance between repetitions) is approximately inversely proportional to frequency.\n\n\n=== Line current ===\n\nIn Europe, Africa, Australia, southern South America, most of Asia, and Russia, the frequency of the alternating current in household electrical outlets is 50 Hz (close to the tone G), whereas in North America and northern South America, the frequency of the alternating current in household electrical outlets is 60 Hz (between the tones B\u266d and B; that is, a minor third above the European frequency). The frequency of the 'hum' in an audio recording can show in which of these general regions the recording was made.\n\n\n== Aperiodic frequency ==\nAperiodic frequency is the rate of incidence or occurrence of non-cyclic phenomena, including random processes such as radioactive decay. It is expressed with the unit reciprocal second (s\u22121) or, in the case of radioactivity, with the unit becquerel.\nIt is defined as a rate, f = N/\u0394t, involving the number of entities counted or the number of events happened (N) during a given time duration (\u0394t); it is a physical quantity of type temporal rate.\n\n\n== See also ==\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Sources ==\nDavies, A. (1997). Handbook of Condition Monitoring: Techniques and Methodology. New York: Springer. ISBN 978-0-412-61320-3.\nSerway, Raymond A.; Faughn, Jerry S. (1989). College Physics. London: Thomson/Brooks-Cole. ISBN 978-05344-0-814-5.\nYoung, Ian R. (1999). Wind Generated Ocean Waves. Elsevere Ocean Engineering. Vol. 2. Oxford: Elsevier. ISBN 978-0-08-043317-2.\n\n\n== Further reading ==\nGiancoli, D.C. (1988). Physics for Scientists and Engineers (2nd ed.). Prentice Hall. ISBN 978-0-13-669201-0.\n\n\n== External links ==\n\nKeyboard frequencies = naming of notes \u2013 The English and American system versus the German system\nA frequency generator with sound, useful for hearing tests",
        "unit": "frequency",
        "url": "https://en.wikipedia.org/wiki/Frequency"
    },
    {
        "_id": "Coulomb",
        "clean": "Coulomb",
        "text": "The coulomb (symbol: C) is the unit of electric charge in the International System of Units (SI). It is equal to the electric charge delivered by a 1 ampere current in 1 second and is defined in terms of the elementary charge e, at about 6.241509\u00d71018 e.\n\n\n== Definition ==\nThe SI defines the coulomb by taking the value of the elementary charge e to be 1.602176634\u00d710\u221219 C, but was previously defined in terms of the force between two wires. The coulomb was originally defined, using the latter definition of the ampere, as 1 A \u00d7 1 s. \nThe 2019 redefinition of the ampere and other SI base units fixed the numerical value of the elementary charge when expressed in coulombs and therefore fixed the value of the coulomb when expressed as a multiple of the fundamental charge. \nOne coulomb is approximately 6241509074460762607.776 e (and is thus not an integer multiple of the elementary charge), where the number is the reciprocal of 1.602176634\u00d710\u221219 C. The coulomb is exactly\n\n  \n    \n      \n        1\n         \n        \n          C\n        \n        =\n        \n          \n            1\n            \n              1.602\n              \n              176\n              \n              634\n              \u00d7\n              \n                10\n                \n                  \u2212\n                  19\n                \n              \n            \n          \n        \n         \n        e\n        .\n      \n    \n    {\\displaystyle 1~\\mathrm {C} ={\\frac {1}{1.602\\,176\\,634\\times 10^{-19}}}~e.}\n  \n\n\n== SI prefixes ==\n\nLike other SI units, the coulomb can be modified by adding a prefix that multiplies it by a power of 10.\n\n\n== Conversions ==\nThe magnitude of the electrical charge of one mole of elementary charges (approximately 6.022\u00d71023, the Avogadro number) is known as a faraday unit of charge (closely related to the Faraday constant). One faraday equals 9.648533212...\u00d7104 coulombs. In terms of the Avogadro constant (NA), one coulomb is equal to approximately 1.036\u00d710\u22125 mol \u00d7 NA elementary charges.\nEvery farad of capacitance can hold one coulomb per volt across the capacitor.\nOne ampere hour equals 3600 C, hence 1 mA\u22c5h = 3.6 C.\nOne statcoulomb (statC), the obsolete CGS electrostatic unit of charge (esu), is approximately 3.3356\u00d710\u221210 C or about one-third of a nanocoulomb.\n\n\n== In everyday terms ==\nThe charges in static electricity from rubbing materials together are typically a few microcoulombs.\nThe amount of charge that travels through a lightning bolt is typically around 15 C, although for large bolts this can be up to 350 C.\nThe amount of charge that travels through a typical alkaline AA battery from being fully charged to discharged is about 5 kC = 5000 C \u2248 1400 mA\u22c5h.\nA typical smartphone battery can hold 10800 C \u2248 3000 mA\u22c5h.\n\n\n== Name and history ==\n\nThe coulomb is named after Charles-Augustin de Coulomb. As with every SI unit named for a person, its symbol starts with an upper case letter (C), but when written in full, it follows the rules for capitalisation of a common noun; i.e., coulomb becomes capitalised at the beginning of a sentence and in titles but is otherwise in lower case.\nBy 1878, the British Association for the Advancement of Science had defined the volt, ohm, and farad, but not the coulomb.  In 1881, the International Electrical Congress, now the International Electrotechnical Commission (IEC), approved the volt as the unit for electromotive force, the ampere as the unit for electric current, and the coulomb as the unit of electric charge.  \nAt that time, the volt was defined as the potential difference [i.e., what is nowadays called the \"voltage (difference)\"] across a conductor when a current of one ampere dissipates one watt of power.\nThe coulomb (later \"absolute coulomb\" or \"abcoulomb\" for disambiguation) was part of the EMU system of units. The \"international coulomb\" based on laboratory specifications for its measurement was introduced by the IEC in 1908. The entire set of \"reproducible units\" was abandoned in 1948 and the \"international coulomb\" became the modern coulomb.\n\n\n== See also ==\nAbcoulomb, a cgs unit of charge\nAmp\u00e8re's circuital law\nCoulomb's law\nElectrostatics\nElementary charge\nFaraday constant, the number of coulombs per mole of elementary charges\n\n\n== Notes and references ==",
        "unit": "coulomb",
        "url": "https://en.wikipedia.org/wiki/Coulomb"
    },
    {
        "_id": "Minute_and_second_of_arc",
        "clean": "Minute and second of arc",
        "text": "A minute of arc, arcminute (arcmin), arc minute, or minute arc, denoted by the symbol \u2032, is a unit of angular measurement equal to \u20601/60\u2060 of one degree. Since one degree is \u20601/360\u2060 of a turn, or complete rotation, one arcminute is \u20601/21600\u2060 of a turn. The nautical mile (nmi) was originally defined as the arc length of a minute of latitude on a spherical Earth, so the actual Earth's circumference is very near 21600 nmi.  A minute of arc is \u2060\u03c0/10800\u2060 of a radian.\nA second of arc, arcsecond (arcsec), or arc second, denoted by the symbol \u2033, is \u20601/60\u2060 of an arcminute, \u20601/3600\u2060 of a degree, \u20601/1296000\u2060 of a turn, and \u2060\u03c0/648000\u2060 (about \u20601/206264.8\u2060) of a radian.\nThese units originated in Babylonian astronomy as sexagesimal (base 60) subdivisions of the degree; they are used in fields that involve very small angles, such as astronomy, optometry, ophthalmology, optics, navigation, land surveying, and marksmanship.\nTo express even smaller angles, standard SI prefixes can be employed; the milliarcsecond (mas) and microarcsecond (\u03bcas), for instance, are commonly used in astronomy. For a three-dimensional area such as on a sphere, square arcminutes or seconds may be used.\n\n\n== Symbols and abbreviations ==\nThe prime symbol \u2032 (U+2032) designates the arcminute, though a single quote ' (U+0027) is commonly used where only ASCII characters are permitted. One arcminute is thus written as 1\u2032. It is also abbreviated as arcmin or amin.\nSimilarly, double prime \u2033 (U+2033) designates the arcsecond, though a double quote \" (U+0022) is commonly used where only ASCII characters are permitted. One arcsecond is thus written as 1\u2033. It is also abbreviated as arcsec or asec.\n\nIn celestial navigation, seconds of arc are rarely used in calculations, the preference usually being for degrees, minutes, and decimals of a minute, for example, written as 42\u00b0 25.32\u2032 or 42\u00b0 25.322\u2032. This notation has been carried over into marine GPS and aviation GPS receivers, which normally display latitude and longitude in the latter format by default.\n\n\n== Common examples ==\nThe average apparent diameter of the full Moon is about 31 arcminutes, or 0.52\u00b0.\nOne arcminute is the approximate distance two contours can be separated by, and still be distinguished by, a person with 20/20 vision.\nOne arcsecond is the approximate angle subtended by a U.S. dime coin (18 mm) at a distance of 4 kilometres (about 2.5 mi). An arcsecond is also the angle subtended by\n\nan object of diameter 725.27 km at a distance of one astronomical unit,\nan object of diameter 45866916 km at one light-year,\nan object of diameter one astronomical unit (149597870.7 km) at a distance of one parsec, per the definition of the latter.\nOne milliarcsecond is about the size of a half dollar, seen from a distance equal to that between the Washington Monument and the Eiffel Tower.\nOne microarcsecond is about the size of a period at the end of a sentence in the Apollo mission manuals left on the Moon as seen from Earth.\nOne nanoarcsecond is about the size of a penny on Neptune's moon Triton as observed from Earth.\nAlso notable examples of size in arcseconds are:\n\nHubble Space Telescope has calculational resolution of 0.05 arcseconds and actual resolution of almost 0.1 arcseconds, which is close to the diffraction limit.\nAt crescent phase, Venus measures between 60.2 and 66 seconds of arc.\n\n\n== History ==\nThe concepts of degrees, minutes, and seconds\u2014as they relate to the measure of both angles and time\u2014derive from Babylonian astronomy and time-keeping. Influenced by the Sumerians, the ancient Babylonians divided the Sun's perceived motion across the sky over the course of one full day into 360 degrees. Each degree was subdivided into 60 minutes and each minute into 60 seconds. Thus, one Babylonian degree was equal to four minutes in modern terminology, one Babylonian minute to four modern seconds, and one Babylonian second to \u20601/15\u2060 (approximately 0.067) of a modern second.\n\n\n== Uses ==\n\n\n=== Astronomy ===\n\nSince antiquity, the arcminute and arcsecond have been used in astronomy: in the ecliptic coordinate system as latitude (\u03b2) and longitude (\u03bb); in the horizon system as altitude (Alt) and azimuth (Az); and in the equatorial coordinate system as declination (\u03b4). All are measured in degrees, arcminutes, and arcseconds. The principal exception is right ascension (RA) in equatorial coordinates, which is measured in time units of hours, minutes, and seconds.\nContrary to what one might assume, minutes and seconds of arc do not directly relate to minutes and seconds of time, in either the rotational frame of the Earth around its own axis (day), or the Earth's rotational frame around the Sun (year). The Earth's rotational rate around its own axis is 15 minutes of arc per minute of time (360 degrees / 24 hours in day); the Earth's rotational rate around the Sun (not entirely constant) is roughly 24 minutes of time per minute of arc (from 24 hours in day), which tracks the annual progression of the Zodiac. Both of these factor in what astronomical objects you can see from surface telescopes (time of year) and when you can best see them (time of day), but neither are in unit correspondence. For simplicity, the explanations given assume a degree/day in the Earth's annual rotation around the Sun, which is off by roughly 1%. The same ratios hold for seconds, due to the consistent factor of 60 on both sides.  \nThe arcsecond is also often used to describe small astronomical angles such as the angular diameters of planets (e.g. the angular diameter of Venus which varies between 10\u2033 and 60\u2033); the proper motion of stars; the separation of components of binary star systems; and parallax, the small change of position of a star or Solar System body as the Earth revolves about the Sun. These small angles may also be written in milliarcseconds (mas), or thousandths of an arcsecond. The unit of distance called the parsec, abbreviated from the parallax angle of one arc second, was developed for such parallax measurements. The distance from the Sun to a celestial object is the reciprocal of the angle, measured in arcseconds, of the object's apparent movement caused by parallax.\nThe European Space Agency's astrometric satellite Gaia, launched in 2013, can approximate star positions to 7 microarcseconds (\u03bcas).\nApart from the Sun, the star with the largest angular diameter from Earth is R Doradus, a red giant with a diameter of 0.05\u2033. Because of the effects of atmospheric blurring, ground-based telescopes will smear the image of a star to an angular diameter of about 0.5\u2033; in poor conditions this increases to 1.5\u2033 or even more. The dwarf planet Pluto has proven difficult to resolve because its angular diameter is about 0.1\u2033. Techniques exist for improving seeing on the ground. Adaptive optics, for example, can produce images around 0.05\u2033 on a 10 m class telescope.\nSpace telescopes are not affected by the Earth's atmosphere but are diffraction limited. For example, the Hubble Space Telescope can reach an angular size of stars down to about 0.1\u2033.\n\n\n=== Cartography ===\nMinutes (\u2032) and seconds (\u2033) of arc are also used in cartography and navigation. At sea level one minute of arc along the equator equals exactly one geographical mile (not to be confused with international mile or statute mile) along the Earth's equator or approximately one nautical mile (1,852 metres; 1.151 miles). A second of arc, one sixtieth of this amount, is roughly 30 metres (98 feet). The exact distance varies along meridian arcs or any other great circle arcs because the figure of the Earth is slightly oblate (bulges a third of a percent at the equator).\nPositions are traditionally given using degrees, minutes, and seconds of arcs for latitude, the arc north or south of the equator, and for longitude, the arc east or west of the Prime Meridian. Any position on or above the Earth's reference ellipsoid can be precisely given with this method. However, when it is inconvenient to use  base-60 for minutes and seconds, positions are frequently expressed as decimal fractional degrees to an equal amount of precision. Degrees given to three decimal places (\u20601/1000\u2060 of a degree) have about \u20601/4\u2060 the precision of degrees-minutes-seconds (\u20601/3600\u2060 of a degree) and specify locations within about 120 metres (390 feet).  For navigational purposes positions are given in degrees and decimal minutes, for instance The Needles lighthouse is at 50\u00ba 39.734\u2019N 001\u00ba 35.500\u2019W.\n\n\n=== Property cadastral surveying ===\nRelated to cartography, property boundary surveying using the metes and bounds system and cadastral surveying relies on fractions of a degree to describe property lines' angles in reference to cardinal directions. A boundary \"mete\" is described with a beginning reference point, the cardinal direction North or South followed by an angle less than 90 degrees and a second cardinal direction, and a linear distance. The boundary runs the specified linear distance from the beginning point, the direction of the distance being determined by rotating the first cardinal direction the specified angle toward the second cardinal direction. For example, North 65\u00b0 39\u2032 18\u2033 West 85.69 feet would describe a line running from the starting point 85.69 feet in a direction 65\u00b0 39\u2032 18\u2033 (or 65.655\u00b0) away from north toward the west.\n\n\n=== Firearms ===\n\nThe arcminute is commonly found in the firearms industry and literature, particularly concerning the precision of rifles, though the industry refers to it as minute of angle (MOA).  It is especially popular as a unit of measurement with shooters familiar with the imperial measurement system because 1 MOA subtends a circle with a diameter of 1.047 inches (which is often rounded to just 1 inch) at 100 yards (2.66 cm at 91 m or 2.908 cm at 100 m), a traditional distance on American target ranges.  The subtension is linear with the distance, for example, at 500 yards, 1 MOA subtends 5.235 inches, and at 1000 yards 1 MOA subtends 10.47 inches.\nSince many modern telescopic sights are adjustable in half (\u20601/2\u2060), quarter (\u20601/4\u2060) or eighth (\u20601/8\u2060) MOA increments, also known as clicks, zeroing and adjustments are made by counting 2, 4 and 8 clicks per MOA respectively.\nFor example, if the point of impact is 3 inches high and 1.5 inches left of the point of aim at 100 yards (which for instance could be measured by using a spotting scope with a calibrated reticle, or a  target delineated for such purposes), the scope needs to be adjusted 3 MOA down, and 1.5 MOA right. Such adjustments are trivial when the scope's adjustment dials have a MOA scale printed on them, and even figuring the right number of clicks is relatively easy on scopes that click in fractions of MOA. This makes zeroing and adjustments much easier:\n\nTo adjust a 1\u20442 MOA scope 3 MOA down and 1.5 MOA right, the scope needs to be adjusted 3 \u00d7 2 = 6 clicks down and 1.5 x 2 = 3 clicks right\nTo adjust a 1\u20444 MOA scope 3 MOA down and 1.5 MOA right, the scope needs to be adjusted 3 x 4 = 12 clicks down and 1.5 \u00d7 4 = 6 clicks right\nTo adjust a 1\u20448 MOA scope 3 MOA down and 1.5 MOA right, the scope needs to be adjusted 3 x 8 = 24 clicks down and 1.5 \u00d7 8 = 12 clicks right\n\nAnother common system of measurement in firearm scopes is the milliradian (mrad). Zeroing an mrad based scope is easy for users familiar with base ten systems. The most common adjustment value in mrad based scopes is \u20601/10\u2060 mrad (which approximates 1\u20443 MOA).\n\nTo adjust a \u20601/10\u2060 mrad scope 0.9 mrad down and 0.4 mrad right, the scope needs to be adjusted 9 clicks down and 4 clicks right (which equals approximately 3 and 1.5 MOA respectively).\nOne thing to be aware of is that some MOA scopes, including some higher-end models, are calibrated such that an adjustment of 1 MOA on the scope knobs corresponds to exactly 1 inch of impact adjustment on a target at 100 yards, rather than the mathematically correct 1.047 inches. This is commonly known as the Shooter's MOA (SMOA) or Inches Per Hundred Yards (IPHY). While the difference between one true MOA and one SMOA is less than half of an inch even at 1000 yards, this error compounds significantly on longer range shots that may require adjustment upwards of 20\u201330 MOA to compensate for the bullet drop. If a shot requires an adjustment of 20 MOA or more, the difference between true MOA and SMOA will add up to 1 inch or more. In competitive target shooting, this might mean the difference between a hit and a miss.\nThe physical group size equivalent to m minutes of arc can be calculated as follows: group size = tan(\u2060m/60\u2060) \u00d7 distance. In the example previously given, for 1 minute of arc, and substituting 3,600 inches for 100 yards, 3,600 tan(\u20601/60\u2060) \u2248 1.047 inches. In metric units 1 MOA at 100 metres \u2248 2.908 centimetres.\nSometimes, a precision-oriented firearm's performance will be measured in MOA.  This simply means that under ideal conditions (i.e. no wind, high-grade ammo, clean barrel, and a stable mounting platform such as a vise or a benchrest used to eliminate shooter error), the gun is capable of producing a group of shots whose center points (center-to-center) fit into a circle, the average diameter of circles in several groups can be subtended by that amount of arc.  For example, a 1 MOA rifle should be capable, under ideal conditions, of repeatably shooting 1-inch groups at 100 yards.  Most higher-end rifles are warrantied by their manufacturer to shoot under a given MOA threshold (typically 1 MOA or better) with specific ammunition and no error on the shooter's part.  For example, Remington's M24 Sniper Weapon System is required to shoot 0.8 MOA or better, or be rejected from sale by quality control.\nRifle manufacturers and gun magazines often refer to this capability as sub-MOA, meaning a gun consistently shooting groups under 1 MOA.  This means that a single group of 3 to 5 shots at 100 yards, or the average of several groups, will measure less than 1 MOA between the two furthest shots in the group, i.e. all shots fall within 1 MOA.  If larger samples are taken (i.e., more shots per group) then group size typically increases, however this will ultimately average out.  If a rifle was truly a 1 MOA rifle, it would be just as likely that two consecutive shots land exactly on top of each other as that they land 1 MOA apart.  For 5-shot groups, based on 95% confidence, a rifle that normally shoots 1 MOA can be expected to shoot groups between 0.58 MOA and 1.47 MOA, although the majority of these groups will be under 1 MOA. What this means in practice is if a rifle that shoots 1-inch groups on average at 100 yards shoots a group measuring 0.7 inches followed by a group that is 1.3 inches, this is not statistically abnormal.\nThe metric system counterpart of the MOA is the milliradian (mrad or 'mil'), being equal to 1\u20441000 of the target range, laid out on a circle that has the observer as centre and the target range as radius. The number of milliradians on a full such circle therefore always is equal to 2 \u00d7 \u03c0 \u00d7 1000, regardless the target range. Therefore, 1 MOA \u2248 0.2909 mrad. This means that an object which spans 1 mrad on the reticle is at a range that is in metres equal to the object's linear size in millimetres (e.g. an object of 100 mm subtending 1 mrad is 100 metres away). So there is no conversion factor required, contrary to the MOA system.  A reticle with markings (hashes or dots) spaced with a one mrad apart (or a fraction of a mrad) are collectively called a mrad reticle. If the markings are round they are called mil-dots.\nIn the table below conversions from mrad to metric values are exact (e.g. 0.1 mrad equals exactly 10 mm at 100 metres), while conversions of minutes of arc to both metric and imperial values are approximate.\n\n1\u2032 at 100 yards is about 1.047 inches\n1\u2032 \u2248 0.291 mrad (or 29.1 mm at 100 m, approximately  30 mm at 100 m)\n1 mrad \u2248 3.44\u2032, so \u20601/10\u2060 mrad \u2248 \u20601/3\u2060\u2032\n0.1 mrad equals exactly 1 cm at 100 m, or exactly 0.36 inches at 100 yards\n\n\n=== Human vision ===\nIn humans, 20/20 vision is the ability to resolve a spatial pattern separated by a visual angle of one minute of arc, from a distance of twenty feet.\nA 20/20 letter subtends 5 minutes of arc total.\n\n\n=== Materials ===\nThe deviation from parallelism between two surfaces, for instance in optical engineering, is usually measured in arcminutes or arcseconds.\nIn addition, arcseconds are sometimes used in rocking curve (\u03c9-scan) x ray diffraction measurements of high-quality epitaxial thin films.\n\n\n=== Manufacturing ===\nSome measurement devices make use of arcminutes and arcseconds to measure angles when the object being measured is too small for direct visual inspection. For instance, a toolmaker's optical comparator will often include an option to measure in \"minutes and seconds\".\n\n\n== See also ==\nGradian\nDegree (angle) \u00a7 Subdivisions\nSexagesimal \u00a7 Modern usage\nSquare minute\nSquare second\nSteradian\nMilliradian\nNautical mile\n\n\n== References ==\n\n\n== External links ==\nMOA/ mils By Robert Simeone\nA Guide to calculate distance using MOA Scope by Steve Coffman",
        "unit": "second of arc",
        "url": "https://en.wikipedia.org/wiki/Minute_and_second_of_arc"
    },
    {
        "_id": "Density",
        "clean": "Density",
        "text": "Density (volumetric mass density or specific mass) is a substance's mass per unit of volume. The symbol most often used for density is \u03c1 (the lower case Greek letter rho), although the Latin letter D can also be used. Mathematically, density is defined as mass divided by volume: \n\n  \n    \n      \n        \u03c1\n        =\n        \n          \n            m\n            V\n          \n        \n        ,\n      \n    \n    {\\displaystyle \\rho ={\\frac {m}{V}},}\n  \n\nwhere \u03c1 is the density, m is the mass, and V is the volume. In some cases (for instance, in the United States oil and gas industry), density is loosely defined as its weight per unit volume, although this is scientifically inaccurate \u2013  this quantity is more specifically called specific weight.\nFor a pure substance the density has the same numerical value as its mass concentration.\nDifferent materials usually have different densities, and density may be relevant to buoyancy, purity and packaging. Osmium is the densest known element at standard conditions for temperature and pressure.\nTo simplify comparisons of density across different systems of units, it is sometimes replaced by the dimensionless quantity \"relative density\" or \"specific gravity\", i.e. the ratio of the density of the material to that of a standard material, usually water. Thus a relative density less than one relative to water means that the substance floats in water.\nThe density of a material varies with temperature and pressure. This variation is typically small for solids and liquids but much greater for gases. Increasing the pressure on an object decreases the volume of the object and thus increases its density. Increasing the temperature of a substance (with a few exceptions) decreases its density by increasing its volume. In most materials, heating the bottom of a fluid results in convection of the heat from the bottom to the top, due to the decrease in the density of the heated fluid, which causes it to rise relative to denser unheated material.\nThe reciprocal of the density of a substance is occasionally called its specific volume, a term sometimes used in thermodynamics. Density is an intensive property in that increasing the amount of a substance does not increase its density; rather it increases its mass.\nOther conceptually comparable quantities or ratios include specific density, relative density (specific gravity), and specific weight.\n\n\n== History ==\n\n\n=== Density, floating, and sinking ===\nThe understanding that different materials have different densities, and of a relationship between density, floating, and sinking must date to prehistoric times. Much later it was put in writing. Aristotle, for example, wrote:\n\nThere is so great a difference in density between salt and fresh water that vessels laden with cargoes of the same weight almost sink in rivers, but ride quite easily at sea and are quite seaworthy. And an ignorance of this has sometimes cost people dear who load their ships in rivers. The following is a proof that the density of a fluid is greater when a substance is mixed with it. If you make water very salt by mixing salt in with it, eggs will float on it. ... If there were any truth in the stories they tell about the lake in Palestine it would further bear out what I say. For they say if you bind a man or beast and throw him into it he floats and does not sink beneath the surface.\n\n\n=== Volume vs. density; volume of an irregular shape ===\n\nIn a well-known but probably apocryphal tale, Archimedes was given the task of determining whether King Hiero's goldsmith was embezzling gold during the manufacture of a golden wreath dedicated to the gods and replacing it with another, cheaper alloy. Archimedes knew that the irregularly shaped wreath could be crushed into a cube whose volume could be calculated easily and compared with the mass; but the king did not approve of this. Baffled, Archimedes is said to have taken an immersion bath and observed from the rise of the water upon entering that he could calculate the volume of the gold wreath through the displacement of the water. Upon this discovery, he leapt from his bath and ran naked through the streets shouting, \"Eureka! Eureka!\" (Ancient Greek: \u0395\u03cd\u03c1\u03b7\u03ba\u03b1!, lit.\u2009'I have found it'). As a result, the term eureka entered common parlance and is used today to indicate a moment of enlightenment.\nThe story first appeared in written form in Vitruvius' books of architecture, two centuries after it supposedly took place. Some scholars have doubted the accuracy of this tale, saying among other things that the method would have required precise measurements that would have been difficult to make at the time.\nNevertheless, in 1586, Galileo Galilei, in one of his first experiments, made a possible reconstruction of how the experiment could have been performed with ancient Greek resources\n\n\n== Units ==\nFrom the equation for density (\u03c1 = m/V), mass density has any unit that is mass divided by volume. As there are many units of mass and volume covering many different magnitudes there are a large number of units for mass density in use. The SI unit of kilogram per cubic metre (kg/m3) and the cgs unit of gram per cubic centimetre (g/cm3) are probably the most commonly used units for density. One g/cm3 is equal to 1000 kg/m3.  One cubic centimetre (abbreviation cc) is equal to one millilitre. In industry, other larger or smaller units of mass and or volume are often more practical and US customary units may be used. See below for a list of some of the most common units of density.\nThe litre and tonne are not part of the SI, but are acceptable for use with it, leading to the following units:\n\nkilogram per litre (kg/L)\ngram per millilitre (g/mL)\ntonne per cubic metre (t/m3)\nDensities using the following metric units all have exactly the same numerical value, one thousandth of the value in (kg/m3). Liquid water has a density of about 1 kg/dm3, making any of these SI units numerically convenient to use as most solids and liquids have densities between 0.1 and 20 kg/dm3.\n\nkilogram per cubic decimetre (kg/dm3)\ngram per cubic centimetre (g/cm3)\n1 g/cm3 = 1000 kg/m3\nmegagram (metric ton) per cubic metre (Mg/m3)\nIn US customary units density can be stated in:\n\nAvoirdupois ounce per cubic inch (1 g/cm3 \u2248 0.578036672 oz/cu in)\nAvoirdupois ounce per fluid ounce (1 g/cm3 \u2248 1.04317556 oz/US fl oz = 1.04317556 lb/US fl pint)\nAvoirdupois pound per cubic inch (1 g/cm3 \u2248 0.036127292 lb/cu in)\npound per cubic foot (1 g/cm3 \u2248 62.427961 lb/cu ft)\npound per cubic yard (1 g/cm3 \u2248 1685.5549 lb/cu yd)\npound per US liquid gallon (1 g/cm3 \u2248 8.34540445 lb/US gal)\npound per US bushel (1 g/cm3 \u2248 77.6888513 lb/bu)\nslug per cubic foot\nImperial units differing from the above (as the Imperial gallon and bushel differ from the US units) in practice are rarely used, though found in older documents. The Imperial gallon was based on the concept that an Imperial fluid ounce of water would have a mass of one Avoirdupois ounce, and indeed 1 g/cm3 \u2248 1.00224129 ounces per Imperial fluid ounce = 10.0224129 pounds per Imperial gallon. The density of precious metals could conceivably be based on Troy ounces and pounds, a possible cause of confusion.\nKnowing the volume of the unit cell of a crystalline material and its formula weight (in daltons), the density can be calculated. One dalton per cubic \u00e5ngstr\u00f6m is equal to a density of 1.660 539 066 60 g/cm3.\n\n\n== Measurement ==\nA number of techniques as well as standards exist for the measurement of density of materials. Such techniques include the use of a hydrometer (a buoyancy method for liquids), Hydrostatic balance (a buoyancy method for liquids and solids), immersed body method (a buoyancy method for liquids), pycnometer (liquids and solids), air comparison pycnometer (solids), oscillating densitometer (liquids), as well as pour and tap (solids). However, each individual method or technique measures different types of density (e.g. bulk density, skeletal density, etc.), and therefore it is necessary to have an understanding of the type of density  being measured as well as the type of material in question.\n\n\n=== Homogeneous materials ===\nThe density at all points of a homogeneous object equals its total mass divided by its total volume. The mass is normally measured with a scale or balance; the volume may be measured directly (from the geometry of the object) or by the displacement of a fluid. To determine the density of a liquid or a gas, a hydrometer, a dasymeter or a Coriolis flow meter may be used, respectively. Similarly, hydrostatic weighing uses the displacement of water due to a submerged object to determine the density of the object.\n\n\n=== Heterogeneous materials ===\nIf the body is not homogeneous, then its density varies between different regions of the object. In that case the density around any given location is determined by calculating the density of a small volume around that location. In the limit of an infinitesimal volume the density of an inhomogeneous object at a point becomes: \n  \n    \n      \n        \u03c1\n        (\n        \n          \n            \n              r\n              \u2192\n            \n          \n        \n        )\n        =\n        d\n        m\n        \n          /\n        \n        d\n        V\n      \n    \n    {\\displaystyle \\rho ({\\vec {r}})=dm/dV}\n  \n, where \n  \n    \n      \n        d\n        V\n      \n    \n    {\\displaystyle dV}\n  \n is an elementary volume at position \n  \n    \n      \n        \n          \n            \n              r\n              \u2192\n            \n          \n        \n      \n    \n    {\\displaystyle {\\vec {r}}}\n  \n. The mass of the body then can be expressed as\n\n  \n    \n      \n        m\n        =\n        \n          \u222b\n          \n            V\n          \n        \n        \u03c1\n        (\n        \n          \n            \n              r\n              \u2192\n            \n          \n        \n        )\n        \n        d\n        V\n        .\n      \n    \n    {\\displaystyle m=\\int _{V}\\rho ({\\vec {r}})\\,dV.}\n  \n\n\n=== Non-compact materials ===\n\nIn practice, bulk materials such as sugar, sand, or snow contain voids. Many materials exist in nature as flakes, pellets, or granules.\nVoids are regions which contain something other than the considered material.  Commonly the void is air, but it could also be vacuum,  liquid, solid, or a different gas or gaseous mixture.\nThe bulk volume of a material \u2014inclusive of the void space fraction\u2014 is often obtained by a simple measurement (e.g. with a calibrated measuring cup) or geometrically from known dimensions.\nMass divided by bulk volume determines bulk density.  This is not the same thing as the material volumetric mass density.\nTo determine the material volumetric mass density, one must first discount the volume of the void fraction. Sometimes this can be determined by geometrical reasoning.  For the close-packing of equal spheres the non-void fraction can be at most about 74%.  It can also be determined empirically.  Some bulk materials, however, such as sand, have a variable void fraction which depends on how the material is agitated or poured. It might be loose or compact, with more or less air space depending on handling.\nIn practice, the void fraction is not necessarily air, or even gaseous.  In the case of sand, it could be water, which can be advantageous for measurement as the void fraction for sand saturated in water\u2014once any air bubbles are thoroughly driven out\u2014is potentially more consistent than dry sand measured with an air void.\nIn the case of non-compact materials, one must also take care in determining the mass of the material sample. If the material is under pressure (commonly ambient air pressure at the earth's surface) the determination of mass from a measured sample weight might need to account for buoyancy effects due to the density of the void constituent, depending on how the measurement was conducted. In the case of dry sand, sand is so much denser than air that the buoyancy effect is commonly neglected (less than one part in one thousand).\nMass change upon displacing one void material with another while maintaining constant volume can be used to estimate the void fraction, if the difference in density of the two voids materials is reliably known.\n\n\n== Changes of density ==\n\nIn general, density can be changed by changing either the pressure or the temperature. Increasing the pressure always increases the density of a material. Increasing the temperature generally decreases the density, but there are notable exceptions to this generalization. For example, the density of water increases between its melting point at 0 \u00b0C and 4 \u00b0C; similar behavior is observed in silicon at low temperatures.\nThe effect of pressure and temperature on the densities of liquids and solids is small. The compressibility for a typical liquid or solid is 10\u22126 bar\u22121 (1 bar = 0.1 MPa) and a typical thermal expansivity is 10\u22125 K\u22121. This roughly translates into needing around ten thousand times atmospheric pressure to reduce the volume of a substance by one percent. (Although the pressures needed may be around a thousand times smaller for sandy soil and some clays.) A one percent expansion of volume typically requires a temperature increase on the order of thousands of degrees Celsius.\nIn contrast, the density of gases is strongly affected by pressure. The density of an ideal gas is\n\n  \n    \n      \n        \u03c1\n        =\n        \n          \n            \n              M\n              P\n            \n            \n              R\n              T\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle \\rho ={\\frac {MP}{RT}},}\n  \n\nwhere M is the molar mass, P is the pressure, R is the universal gas constant, and T is the absolute temperature. This means that the density of an ideal gas can be doubled by doubling the pressure, or by halving the absolute temperature.\nIn the case of volumic thermal expansion at constant pressure and small intervals of temperature the temperature dependence of density is\n\n  \n    \n      \n        \u03c1\n        =\n        \n          \n            \n              \u03c1\n              \n                \n                  T\n                  \n                    0\n                  \n                \n              \n            \n            \n              1\n              +\n              \u03b1\n              \u22c5\n              \u0394\n              T\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle \\rho ={\\frac {\\rho _{T_{0}}}{1+\\alpha \\cdot \\Delta T}},}\n  \n\nwhere \n  \n    \n      \n        \n          \u03c1\n          \n            \n              T\n              \n                0\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\rho _{T_{0}}}\n  \n is the density at a reference temperature, \n  \n    \n      \n        \u03b1\n      \n    \n    {\\displaystyle \\alpha }\n  \n is the thermal expansion coefficient of the material at temperatures close to \n  \n    \n      \n        \n          T\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle T_{0}}\n  \n.\n\n\n== Density of solutions ==\nThe density of a solution is the sum of mass (massic) concentrations of the components of that solution.\nMass (massic) concentration of each given component \n  \n    \n      \n        \n          \u03c1\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\rho _{i}}\n  \n in a solution sums to density of the solution,\n\n  \n    \n      \n        \u03c1\n        =\n        \n          \u2211\n          \n            i\n          \n        \n        \n          \u03c1\n          \n            i\n          \n        \n        .\n      \n    \n    {\\displaystyle \\rho =\\sum _{i}\\rho _{i}.}\n  \n\nExpressed as a function of the densities of pure components of the mixture and their volume participation, it allows the determination of excess molar volumes:\n\n  \n    \n      \n        \u03c1\n        =\n        \n          \u2211\n          \n            i\n          \n        \n        \n          \u03c1\n          \n            i\n          \n        \n        \n          \n            \n              V\n              \n                i\n              \n            \n            V\n          \n        \n        \n        =\n        \n          \u2211\n          \n            i\n          \n        \n        \n          \u03c1\n          \n            i\n          \n        \n        \n          \u03c6\n          \n            i\n          \n        \n        =\n        \n          \u2211\n          \n            i\n          \n        \n        \n          \u03c1\n          \n            i\n          \n        \n        \n          \n            \n              V\n              \n                i\n              \n            \n            \n              \n                \u2211\n                \n                  i\n                \n              \n              \n                V\n                \n                  i\n                \n              \n              +\n              \n                \u2211\n                \n                  i\n                \n              \n              \n                \n                  \n                    V\n                    \n                      E\n                    \n                  \n                \n                \n                  i\n                \n              \n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle \\rho =\\sum _{i}\\rho _{i}{\\frac {V_{i}}{V}}\\,=\\sum _{i}\\rho _{i}\\varphi _{i}=\\sum _{i}\\rho _{i}{\\frac {V_{i}}{\\sum _{i}V_{i}+\\sum _{i}{V^{E}}_{i}}},}\n  \n\nprovided that there is no interaction between the components.\nKnowing the relation between excess volumes and activity coefficients of the  components, one can determine the activity coefficients:\n\n  \n    \n      \n        \n          \n            \n              \n                V\n                \n                  E\n                \n              \n              \u00af\n            \n          \n          \n            i\n          \n        \n        =\n        R\n        T\n        \n          \n            \n              \u2202\n              ln\n              \u2061\n              \n                \u03b3\n                \n                  i\n                \n              \n            \n            \n              \u2202\n              P\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle {\\overline {V^{E}}}_{i}=RT{\\frac {\\partial \\ln \\gamma _{i}}{\\partial P}}.}\n  \n\n\n== List of densities ==\n\n\n=== Various materials ===\n\n\n=== Others ===\n\n\n=== Water ===\n\n\n=== Air ===\n\n\n=== Molar volumes of liquid and solid phase of elements ===\n\n\n== See also ==\n\n\n== References ==\n\n\n== External links ==\n\"Density\" . Encyclop\u00e6dia Britannica. Vol. 8 (11th ed.). 1911.\n\"Density\" . The New Student's Reference Work . 1914.\nVideo: Density Experiment with Oil and Alcohol\nVideo: Density Experiment with Whiskey and Water\nGlass Density Calculation \u2013 Calculation of the density of glass at room temperature and of glass melts at 1000 \u2013 1400\u00b0C\nList of Elements of the Periodic Table \u2013 Sorted by Density\nCalculation of saturated liquid densities for some components\nField density test Archived December 15, 2010, at the Wayback Machine\nWater \u2013 Density and specific weight\nTemperature dependence of the density of water \u2013 Conversions of density units\nA delicious density experiment Archived July 18, 2015, at the Wayback Machine\nWater density calculator Archived July 13, 2011, at the Wayback Machine Water density for a given salinity and temperature.\nLiquid density calculator Select a liquid from the list and calculate density as a function of temperature.\nGas density calculator Calculate density of a gas for as a function of temperature and pressure.\nDensities of various materials.\nDetermination of Density of Solid, instructions for performing classroom experiment.\nLam EJ, Alvarez MN, Galvez ME, Alvarez EB (2008). \"A model for calculating the density of aqueous multicomponent electrolyte solutions\". Journal of the Chilean Chemical Society. 53 (1): 1393\u20138. doi:10.4067/S0717-97072008000100015.\nRadovi\u0107 IR, Kijev\u010danin ML, Tasi\u0107 A\u017d, Djordjevi\u0107 BD, \u0160erbanovi\u0107 SP (2010). \"Derived thermodynamic properties of alcohol+ cyclohexylamine mixtures\". Journal of the Serbian Chemical Society. 75 (2): 283\u2013293. CiteSeerX 10.1.1.424.3486. doi:10.2298/JSC1002283R.",
        "unit": "density",
        "url": "https://en.wikipedia.org/wiki/Density"
    },
    {
        "_id": "Viscosity",
        "clean": "Viscosity",
        "text": "The viscosity of a fluid is a measure of its resistance to deformation at a given rate. For liquids, it corresponds to the informal concept of \"thickness\": for example, syrup has a higher viscosity than water. Viscosity is defined scientifically as a force multiplied by a time divided by an area. Thus its SI units are newton-seconds per square meter, or pascal-seconds.\nViscosity quantifies the internal frictional force between adjacent layers of fluid that are in relative motion. For instance, when a viscous fluid is forced through a tube, it flows more quickly near the tube's center line than near its walls. Experiments show that some stress (such as a pressure difference between the two ends of the tube) is needed to sustain the flow. This is because a force is required to overcome the friction between the layers of the fluid which are in relative motion. For a tube with a constant rate of flow, the strength of the compensating force is proportional to the fluid's viscosity.\nIn general, viscosity depends on a fluid's state, such as its temperature, pressure, and rate of deformation. However, the dependence on some of these properties is negligible in certain cases. For example, the viscosity of a Newtonian fluid does not vary significantly with the rate of deformation. \nZero viscosity (no resistance to shear stress) is observed only at very low temperatures in superfluids; otherwise, the second law of thermodynamics requires all fluids to have positive viscosity. A fluid that has zero viscosity (non-viscous) is called ideal or inviscid.\nFor non-Newtonian fluid's viscosity, there are pseudoplastic, plastic, and dilatant flows that are time-independent, and there are thixotropic and rheopectic flows that are time-dependent.\n\n\n== Etymology ==\nThe word \"viscosity\" is derived from the Latin viscum (\"mistletoe\"). Viscum also referred to a viscous glue derived from mistletoe berries.\n\n\n== Definitions ==\n\n\n=== Dynamic viscosity ===\n\nIn materials science and engineering, there is often interest in understanding the forces or stresses involved in the deformation of a material. For instance, if the material were a simple spring, the answer would be given by Hooke's law, which says that the force experienced by a spring is proportional to the distance displaced from equilibrium. Stresses which can be attributed to the deformation of a material from some rest state are called elastic stresses. In other materials, stresses are present which can be attributed to the deformation rate over time. These are called viscous stresses. For instance, in a fluid such as water the stresses which arise from shearing the fluid do not depend on the distance the fluid has been sheared; rather, they depend on how quickly the shearing occurs.\nViscosity is the material property which relates the viscous stresses in a material to the rate of change of a deformation (the strain rate). Although it applies to general flows, it is easy to visualize and define in a simple shearing flow, such as a planar Couette flow.\nIn the Couette flow, a fluid is trapped between two infinitely large plates, one fixed and one in parallel motion at constant speed \n  \n    \n      \n        u\n      \n    \n    {\\displaystyle u}\n  \n (see illustration to the right). If the speed of the top plate is low enough (to avoid turbulence), then in steady state the fluid particles move parallel to it, and their speed varies from \n  \n    \n      \n        0\n      \n    \n    {\\displaystyle 0}\n  \n at the bottom to \n  \n    \n      \n        u\n      \n    \n    {\\displaystyle u}\n  \n at the top. Each layer of fluid moves faster than the one just below it, and friction between them gives rise to a force resisting their relative motion. In particular, the fluid applies on the top plate a force in the direction opposite to its motion, and an equal but opposite force on the bottom plate. An external force is therefore required in order to keep the top plate moving at constant speed.\nIn many fluids, the flow velocity is observed to vary linearly from zero at the bottom to \n  \n    \n      \n        u\n      \n    \n    {\\displaystyle u}\n  \n at the top. Moreover, the magnitude of the force, \n  \n    \n      \n        F\n      \n    \n    {\\displaystyle F}\n  \n, acting on the top plate is found to be proportional to the speed \n  \n    \n      \n        u\n      \n    \n    {\\displaystyle u}\n  \n and the area \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n of each plate, and inversely proportional to their separation \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  \n: \n\n  \n    \n      \n        F\n        =\n        \u03bc\n        A\n        \n          \n            u\n            y\n          \n        \n        .\n      \n    \n    {\\displaystyle F=\\mu A{\\frac {u}{y}}.}\n  \n\nThe proportionality factor is the dynamic viscosity of the fluid, often simply referred to as the viscosity. It is denoted by the Greek letter mu (\u03bc). The dynamic viscosity has the dimensions \n  \n    \n      \n        \n          (\n          m\n          a\n          s\n          s\n          \n            /\n          \n          l\n          e\n          n\n          g\n          t\n          h\n          )\n          \n            /\n          \n          t\n          i\n          m\n          e\n        \n      \n    \n    {\\displaystyle \\mathrm {(mass/length)/time} }\n  \n, therefore resulting in the SI units and the derived units:\n\n  \n    \n      \n        [\n        \u03bc\n        ]\n        =\n        \n          \n            \n              k\n              g\n            \n            \n              m\n              \n                \u22c5\n              \n              s\n            \n          \n        \n        =\n        \n          \n            \n              N\n            \n            \n              \n                m\n                \n                  2\n                \n              \n            \n          \n        \n        \n          \u22c5\n        \n        \n          \n            s\n          \n        \n        =\n        \n          \n            P\n            a\n            \n              \u22c5\n            \n            s\n          \n        \n        =\n      \n    \n    {\\displaystyle [\\mu ]={\\frac {\\rm {kg}}{\\rm {m{\\cdot }s}}}={\\frac {\\rm {N}}{\\rm {m^{2}}}}{\\cdot }{\\rm {s}}={\\rm {Pa{\\cdot }s}}=}\n  \n pressure multiplied by time \n  \n    \n      \n        =\n      \n    \n    {\\displaystyle =}\n  \n energy per unit volume multiplied by time.\nThe aforementioned ratio \n  \n    \n      \n        u\n        \n          /\n        \n        y\n      \n    \n    {\\displaystyle u/y}\n  \n is called the rate of shear deformation or shear velocity, and is the derivative of the fluid speed in the direction parallel to the normal vector of the plates (see illustrations to the right). If the velocity does not vary linearly with \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  \n, then the appropriate generalization is:\n\n  \n    \n      \n        \u03c4\n        =\n        \u03bc\n        \n          \n            \n              \u2202\n              u\n            \n            \n              \u2202\n              y\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle \\tau =\\mu {\\frac {\\partial u}{\\partial y}},}\n  \n\nwhere \n  \n    \n      \n        \u03c4\n        =\n        F\n        \n          /\n        \n        A\n      \n    \n    {\\displaystyle \\tau =F/A}\n  \n, and \n  \n    \n      \n        \u2202\n        u\n        \n          /\n        \n        \u2202\n        y\n      \n    \n    {\\displaystyle \\partial u/\\partial y}\n  \n is the local shear velocity. This expression is referred to as Newton's law of viscosity. In shearing flows with planar symmetry, it is what defines \n  \n    \n      \n        \u03bc\n      \n    \n    {\\displaystyle \\mu }\n  \n. It is a special case of the general definition of viscosity (see below), which can be expressed in coordinate-free form.\nUse of the Greek letter mu (\n  \n    \n      \n        \u03bc\n      \n    \n    {\\displaystyle \\mu }\n  \n) for the dynamic viscosity (sometimes also called the absolute viscosity) is common among mechanical and chemical engineers, as well as mathematicians and physicists. However, the Greek letter eta (\n  \n    \n      \n        \u03b7\n      \n    \n    {\\displaystyle \\eta }\n  \n) is also used by chemists, physicists, and the IUPAC. The viscosity \n  \n    \n      \n        \u03bc\n      \n    \n    {\\displaystyle \\mu }\n  \n is sometimes also called the shear viscosity. However, at least one author discourages the use of this terminology, noting that \n  \n    \n      \n        \u03bc\n      \n    \n    {\\displaystyle \\mu }\n  \n can appear in non-shearing flows in addition to shearing flows.\n\n\n=== Kinematic viscosity ===\nIn fluid dynamics, it is sometimes more appropriate to work in terms of kinematic viscosity (sometimes also called the momentum diffusivity), defined as the ratio of the dynamic viscosity (\u03bc) over the density of the fluid (\u03c1). It is usually denoted by the Greek letter nu (\u03bd):\n\n  \n    \n      \n        \u03bd\n        =\n        \n          \n            \u03bc\n            \u03c1\n          \n        \n        ,\n      \n    \n    {\\displaystyle \\nu ={\\frac {\\mu }{\\rho }},}\n  \n\nand has the dimensions \n  \n    \n      \n        \n          (\n          l\n          e\n          n\n          g\n          t\n          h\n          \n            )\n            \n              2\n            \n          \n          \n            /\n          \n          t\n          i\n          m\n          e\n        \n      \n    \n    {\\displaystyle \\mathrm {(length)^{2}/time} }\n  \n, therefore resulting in the SI units and the derived units:\n\n  \n    \n      \n        [\n        \u03bd\n        ]\n        =\n        \n          \n            \n              m\n              \n                2\n              \n            \n            s\n          \n        \n        =\n        \n          \n            \n              \n                N\n                \n                  \u22c5\n                \n                m\n              \n              \n                k\n                g\n              \n            \n          \n          \n            \u22c5\n          \n          s\n        \n        =\n        \n          \n            \n              J\n              \n                k\n                g\n              \n            \n          \n          \n            \u22c5\n          \n          s\n        \n        =\n      \n    \n    {\\displaystyle [\\nu ]=\\mathrm {\\frac {m^{2}}{s}} =\\mathrm {{\\frac {N{\\cdot }m}{kg}}{\\cdot }s} =\\mathrm {{\\frac {J}{kg}}{\\cdot }s} =}\n  \n specific energy multiplied by time \n  \n    \n      \n        =\n      \n    \n    {\\displaystyle =}\n  \n energy per unit mass multiplied by time.\n\n\n=== General definition ===\n\nIn very general terms, the viscous stresses in a fluid are defined as those resulting from the relative velocity of different fluid particles. As such, the viscous stresses must depend on spatial gradients of the flow velocity. If the velocity gradients are small, then to a first approximation the viscous stresses depend only on the first derivatives of the velocity. (For Newtonian fluids, this is also a linear dependence.) In Cartesian coordinates, the general relationship can then be written as\n\n  \n    \n      \n        \n          \u03c4\n          \n            i\n            j\n          \n        \n        =\n        \n          \u2211\n          \n            k\n          \n        \n        \n          \u2211\n          \n            \u2113\n          \n        \n        \n          \u03bc\n          \n            i\n            j\n            k\n            \u2113\n          \n        \n        \n          \n            \n              \u2202\n              \n                v\n                \n                  k\n                \n              \n            \n            \n              \u2202\n              \n                r\n                \n                  \u2113\n                \n              \n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle \\tau _{ij}=\\sum _{k}\\sum _{\\ell }\\mu _{ijk\\ell }{\\frac {\\partial v_{k}}{\\partial r_{\\ell }}},}\n  \n\nwhere \n  \n    \n      \n        \n          \u03bc\n          \n            i\n            j\n            k\n            \u2113\n          \n        \n      \n    \n    {\\displaystyle \\mu _{ijk\\ell }}\n  \n is a viscosity tensor that maps the velocity gradient tensor \n  \n    \n      \n        \u2202\n        \n          v\n          \n            k\n          \n        \n        \n          /\n        \n        \u2202\n        \n          r\n          \n            \u2113\n          \n        \n      \n    \n    {\\displaystyle \\partial v_{k}/\\partial r_{\\ell }}\n  \n onto the viscous stress tensor \n  \n    \n      \n        \n          \u03c4\n          \n            i\n            j\n          \n        \n      \n    \n    {\\displaystyle \\tau _{ij}}\n  \n. Since the indices in this expression can vary from 1 to 3, there are 81 \"viscosity coefficients\" \n  \n    \n      \n        \n          \u03bc\n          \n            i\n            j\n            k\n            l\n          \n        \n      \n    \n    {\\displaystyle \\mu _{ijkl}}\n  \n in total. However, assuming that the viscosity rank-2 tensor is isotropic reduces these 81 coefficients to three independent parameters \n  \n    \n      \n        \u03b1\n      \n    \n    {\\displaystyle \\alpha }\n  \n, \n  \n    \n      \n        \u03b2\n      \n    \n    {\\displaystyle \\beta }\n  \n, \n  \n    \n      \n        \u03b3\n      \n    \n    {\\displaystyle \\gamma }\n  \n:\n\n  \n    \n      \n        \n          \u03bc\n          \n            i\n            j\n            k\n            \u2113\n          \n        \n        =\n        \u03b1\n        \n          \u03b4\n          \n            i\n            j\n          \n        \n        \n          \u03b4\n          \n            k\n            \u2113\n          \n        \n        +\n        \u03b2\n        \n          \u03b4\n          \n            i\n            k\n          \n        \n        \n          \u03b4\n          \n            j\n            \u2113\n          \n        \n        +\n        \u03b3\n        \n          \u03b4\n          \n            i\n            \u2113\n          \n        \n        \n          \u03b4\n          \n            j\n            k\n          \n        \n        ,\n      \n    \n    {\\displaystyle \\mu _{ijk\\ell }=\\alpha \\delta _{ij}\\delta _{k\\ell }+\\beta \\delta _{ik}\\delta _{j\\ell }+\\gamma \\delta _{i\\ell }\\delta _{jk},}\n  \n\nand furthermore, it is assumed that no viscous forces may arise when the fluid is undergoing simple rigid-body rotation, thus \n  \n    \n      \n        \u03b2\n        =\n        \u03b3\n      \n    \n    {\\displaystyle \\beta =\\gamma }\n  \n, leaving only two independent parameters. The most usual decomposition is in terms of the standard (scalar) viscosity \n  \n    \n      \n        \u03bc\n      \n    \n    {\\displaystyle \\mu }\n  \n and the bulk viscosity \n  \n    \n      \n        \u03ba\n      \n    \n    {\\displaystyle \\kappa }\n  \n such that \n  \n    \n      \n        \u03b1\n        =\n        \u03ba\n        \u2212\n        \n          \n            \n              2\n              3\n            \n          \n        \n        \u03bc\n      \n    \n    {\\displaystyle \\alpha =\\kappa -{\\tfrac {2}{3}}\\mu }\n  \n and \n  \n    \n      \n        \u03b2\n        =\n        \u03b3\n        =\n        \u03bc\n      \n    \n    {\\displaystyle \\beta =\\gamma =\\mu }\n  \n. In vector notation this appears as:\n\n  \n    \n      \n        \n          \u03c4\n        \n        =\n        \u03bc\n        \n          [\n          \n            \u2207\n            \n              v\n            \n            +\n            (\n            \u2207\n            \n              v\n            \n            \n              )\n              \n                \n                  T\n                \n              \n            \n          \n          ]\n        \n        \u2212\n        \n          (\n          \n            \n              \n                2\n                3\n              \n            \n            \u03bc\n            \u2212\n            \u03ba\n          \n          )\n        \n        (\n        \u2207\n        \u22c5\n        \n          v\n        \n        )\n        \n          \u03b4\n        \n        ,\n      \n    \n    {\\displaystyle {\\boldsymbol {\\tau }}=\\mu \\left[\\nabla \\mathbf {v} +(\\nabla \\mathbf {v} )^{\\mathrm {T} }\\right]-\\left({\\frac {2}{3}}\\mu -\\kappa \\right)(\\nabla \\cdot \\mathbf {v} )\\mathbf {\\delta } ,}\n  \n\nwhere \n  \n    \n      \n        \n          \u03b4\n        \n      \n    \n    {\\displaystyle \\mathbf {\\delta } }\n  \n is the unit tensor. This equation can be thought of as a generalized form of Newton's law of viscosity.\nThe bulk viscosity (also called volume viscosity) expresses a type of internal friction that resists the shearless compression or expansion of a fluid. Knowledge of \n  \n    \n      \n        \u03ba\n      \n    \n    {\\displaystyle \\kappa }\n  \n is frequently not necessary in fluid dynamics problems. For example, an incompressible fluid satisfies \n  \n    \n      \n        \u2207\n        \u22c5\n        \n          v\n        \n        =\n        0\n      \n    \n    {\\displaystyle \\nabla \\cdot \\mathbf {v} =0}\n  \n and so the term containing \n  \n    \n      \n        \u03ba\n      \n    \n    {\\displaystyle \\kappa }\n  \n drops out. Moreover, \n  \n    \n      \n        \u03ba\n      \n    \n    {\\displaystyle \\kappa }\n  \n is often assumed to be negligible for gases since it is \n  \n    \n      \n        0\n      \n    \n    {\\displaystyle 0}\n  \n in a monatomic ideal gas. One situation in which \n  \n    \n      \n        \u03ba\n      \n    \n    {\\displaystyle \\kappa }\n  \n can be important is the calculation of energy loss in sound and shock waves, described by Stokes' law of sound attenuation, since these phenomena involve rapid expansions and compressions.\nThe defining equations for viscosity are not fundamental laws of nature, so their usefulness, as well as methods for measuring or calculating the viscosity, must be established using separate means. A potential issue is that viscosity depends, in principle, on the full microscopic state of the fluid, which encompasses the positions and momenta of every particle in the system. Such highly detailed information is typically not available in realistic systems. However, under certain conditions most of this information can be shown to be negligible. In particular, for Newtonian fluids near equilibrium and far from boundaries (bulk state), the viscosity depends only space- and time-dependent macroscopic fields (such as temperature and density) defining local equilibrium.\nNevertheless, viscosity may still carry a non-negligible dependence on several system properties, such as temperature, pressure, and the amplitude and frequency of any external forcing. Therefore, precision measurements of viscosity are only defined\nwith respect to a specific fluid state. To standardize comparisons among experiments and theoretical models, viscosity data is sometimes extrapolated to ideal limiting cases, such as the zero shear limit, or (for gases) the zero density limit.\n\n\n== Momentum transport ==\n\nTransport theory provides an alternative interpretation of viscosity in terms of momentum transport: viscosity is the material property which characterizes momentum transport within a fluid, just as thermal conductivity characterizes heat transport, and (mass) diffusivity characterizes mass transport. This perspective is implicit in Newton's law of viscosity, \n  \n    \n      \n        \u03c4\n        =\n        \u03bc\n        (\n        \u2202\n        u\n        \n          /\n        \n        \u2202\n        y\n        )\n      \n    \n    {\\displaystyle \\tau =\\mu (\\partial u/\\partial y)}\n  \n, because the shear stress \n  \n    \n      \n        \u03c4\n      \n    \n    {\\displaystyle \\tau }\n  \n has units equivalent to a momentum flux, i.e., momentum per unit time per unit area. Thus, \n  \n    \n      \n        \u03c4\n      \n    \n    {\\displaystyle \\tau }\n  \n can be interpreted as specifying the flow of momentum in the \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  \n direction from one fluid layer to the next. Per Newton's law of viscosity, this momentum flow occurs across a velocity gradient, and the magnitude of the corresponding momentum flux is determined by the viscosity.\nThe analogy with heat and mass transfer can be made explicit. Just as heat flows from high temperature to low temperature and mass flows from high density to low density, momentum flows from high velocity to low velocity. These behaviors are all described by compact expressions, called constitutive relations, whose one-dimensional forms are given here:\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  J\n                \n              \n              \n                \n                =\n                \u2212\n                D\n                \n                  \n                    \n                      \u2202\n                      \u03c1\n                    \n                    \n                      \u2202\n                      x\n                    \n                  \n                \n              \n              \n              \n                \n                  (Fick's law of diffusion)\n                \n              \n            \n            \n              \n                \n                  q\n                \n              \n              \n                \n                =\n                \u2212\n                \n                  k\n                  \n                    t\n                  \n                \n                \n                  \n                    \n                      \u2202\n                      T\n                    \n                    \n                      \u2202\n                      x\n                    \n                  \n                \n              \n              \n              \n                \n                  (Fourier's law of heat conduction)\n                \n              \n            \n            \n              \n                \u03c4\n              \n              \n                \n                =\n                \u03bc\n                \n                  \n                    \n                      \u2202\n                      u\n                    \n                    \n                      \u2202\n                      y\n                    \n                  \n                \n              \n              \n              \n                \n                  (Newton's law of viscosity)\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\mathbf {J} &=-D{\\frac {\\partial \\rho }{\\partial x}}&&{\\text{(Fick's law of diffusion)}}\\\\[5pt]\\mathbf {q} &=-k_{t}{\\frac {\\partial T}{\\partial x}}&&{\\text{(Fourier's law of heat conduction)}}\\\\[5pt]\\tau &=\\mu {\\frac {\\partial u}{\\partial y}}&&{\\text{(Newton's law of viscosity)}}\\end{aligned}}}\n  \n\nwhere \n  \n    \n      \n        \u03c1\n      \n    \n    {\\displaystyle \\rho }\n  \n is the density, \n  \n    \n      \n        \n          J\n        \n      \n    \n    {\\displaystyle \\mathbf {J} }\n  \n and \n  \n    \n      \n        \n          q\n        \n      \n    \n    {\\displaystyle \\mathbf {q} }\n  \n are the mass and heat fluxes, and \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  \n and \n  \n    \n      \n        \n          k\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle k_{t}}\n  \n are the mass diffusivity and thermal conductivity. The fact that mass, momentum, and energy (heat) transport are among the most relevant processes in continuum mechanics is not a coincidence: these are among the few physical quantities that are conserved at the microscopic level in interparticle collisions. Thus, rather than being dictated by the fast and complex microscopic interaction timescale, their dynamics occurs on macroscopic timescales, as described by the various equations of transport theory and hydrodynamics.\n\n\n== Newtonian and non-Newtonian fluids ==\n\nNewton's law of viscosity is not a fundamental law of nature, but rather a constitutive equation (like Hooke's law, Fick's law, and Ohm's law) which serves to define the viscosity \n  \n    \n      \n        \u03bc\n      \n    \n    {\\displaystyle \\mu }\n  \n. Its form is motivated by experiments which show that for a wide range of fluids, \n  \n    \n      \n        \u03bc\n      \n    \n    {\\displaystyle \\mu }\n  \n is independent of strain rate. Such fluids are called Newtonian. Gases, water, and many common liquids can be considered Newtonian in ordinary conditions and contexts. However, there are many non-Newtonian fluids that significantly deviate from this behavior. For example:\n\nShear-thickening (dilatant) liquids, whose viscosity increases with the rate of shear strain.\nShear-thinning liquids, whose viscosity decreases with the rate of shear strain.\nThixotropic liquids, that become less viscous over time when shaken, agitated, or otherwise stressed.\nRheopectic liquids, that become more viscous over time when shaken, agitated, or otherwise stressed.\nBingham plastics that behave as a solid at low stresses but flow as a viscous fluid at high stresses.\nTrouton's ratio is the ratio of extensional viscosity to shear viscosity. For a Newtonian fluid, the Trouton ratio is 3. Shear-thinning liquids are very commonly, but misleadingly, described as thixotropic.\nViscosity may also depend on the fluid's physical state (temperature and pressure) and other, external, factors. For gases and other compressible fluids, it depends on temperature and varies very slowly with pressure. The viscosity of some fluids may depend on other factors. A magnetorheological fluid, for example, becomes thicker when subjected to a magnetic field, possibly to the point of behaving like a solid.\n\n\n== In solids ==\nThe viscous forces that arise during fluid flow are distinct from the elastic forces that occur in a solid in response to shear, compression, or extension stresses. While in the latter the stress is proportional to the amount of shear deformation, in a fluid it is proportional to the rate of deformation over time. For this reason, James Clerk Maxwell used the term fugitive elasticity for fluid viscosity.\nHowever, many liquids (including water) will briefly react like elastic solids when subjected to sudden stress. Conversely, many \"solids\" (even granite) will flow like liquids, albeit very slowly, even under arbitrarily small stress. Such materials are best described as viscoelastic\u2014that is, possessing both elasticity (reaction to deformation) and viscosity (reaction to rate of deformation).\nViscoelastic solids may exhibit both shear viscosity and bulk viscosity. The extensional viscosity is a linear combination of the shear and bulk viscosities that describes the reaction of a solid elastic material to elongation. It is widely used for characterizing polymers.\nIn geology, earth materials that exhibit viscous deformation at least three orders of magnitude greater than their elastic deformation are sometimes called rheids.\n\n\n== Measurement ==\n\nViscosity is measured with various types of viscometers and rheometers. Close temperature control of the fluid is essential to obtain accurate measurements, particularly in materials like lubricants, whose viscosity can double with a change of only 5 \u00b0C. A rheometer is used for fluids that cannot be defined by a single value of viscosity and therefore require more parameters to be set and measured than is the case for a viscometer.\nFor some fluids, the viscosity is constant over a wide range of shear rates (Newtonian fluids). The fluids without a constant viscosity (non-Newtonian fluids) cannot be described by a single number. Non-Newtonian fluids exhibit a variety of different correlations between shear stress and shear rate.\nOne of the most common instruments for measuring kinematic viscosity is the glass capillary viscometer.\nIn coating industries, viscosity may be measured with a cup in which the efflux time is measured. There are several sorts of cup\u2014such as the Zahn cup and the Ford viscosity cup\u2014with the usage of each type varying mainly according to the industry.\nAlso used in coatings, a Stormer viscometer employs load-based rotation to determine viscosity. The viscosity is reported in Krebs units (KU), which are unique to Stormer viscometers.\nVibrating viscometers can also be used to measure viscosity. Resonant, or vibrational viscometers work by creating shear waves within the liquid. In this method, the sensor is submerged in the fluid and is made to resonate at a specific frequency. As the surface of the sensor shears through the liquid, energy is lost due to its viscosity. This dissipated energy is then measured and converted into a viscosity reading. A higher viscosity causes a greater loss of energy.\nExtensional viscosity can be measured with various rheometers that apply extensional stress.\nVolume viscosity can be measured with an acoustic rheometer.\nApparent viscosity is a calculation derived from tests performed on drilling fluid used in oil or gas well development. These calculations and tests help engineers develop and maintain the properties of the drilling fluid to the specifications required.\nNanoviscosity (viscosity sensed by nanoprobes) can be measured by fluorescence correlation spectroscopy.\n\n\n== Units ==\nThe SI unit of dynamic viscosity is the newton-second per square meter (N\u00b7s/m2), also frequently expressed in the equivalent forms pascal-second (Pa\u00b7s), kilogram per meter per second (kg\u00b7m\u22121\u00b7s\u22121) and poiseuille (Pl). The CGS unit is the poise (P, or g\u00b7cm\u22121\u00b7s\u22121 = 0.1 Pa\u00b7s), named after Jean L\u00e9onard Marie Poiseuille. It is commonly expressed, particularly in ASTM standards, as centipoise (cP). The centipoise is convenient because the viscosity of water at 20 \u00b0C is about 1 cP, and one centipoise is equal to the SI millipascal second (mPa\u00b7s).\nThe SI unit of kinematic viscosity is square meter per second (m2/s), whereas the CGS unit for kinematic viscosity is the stokes (St, or cm2\u00b7s\u22121 = 0.0001 m2\u00b7s\u22121), named after Sir George Gabriel Stokes. In U.S. usage, stoke is sometimes used as the singular form. The submultiple centistokes (cSt) is often used instead, 1 cSt = 1 mm2\u00b7s\u22121 = 10\u22126 m2\u00b7s\u22121. 1 cSt is 1 cP divided by 1000 kg/m^3, close to the density of water. The kinematic viscosity of water at 20 \u00b0C is about 1 cSt.\nThe most frequently used systems of US customary, or Imperial, units are the British Gravitational (BG) and English Engineering (EE). In the BG system, dynamic viscosity has units of pound-seconds per square foot (lb\u00b7s/ft2), and in the EE system it has units of pound-force-seconds per square foot (lbf\u00b7s/ft2). The pound and pound-force are equivalent; the two systems differ only in how force and mass are defined. In the BG system the pound is a basic unit from which the unit of mass (the slug) is defined by Newton's Second Law, whereas in the EE system the units of force and mass (the pound-force and pound-mass respectively) are defined independently through the Second Law using the proportionality constant gc.\nKinematic viscosity has units of square feet per second (ft2/s) in both the BG and EE systems.\nNonstandard units include the reyn (lbf\u00b7s/in2), a British unit of dynamic viscosity. In the automotive industry the viscosity index is used to describe the change of viscosity with temperature.\nThe reciprocal of viscosity is fluidity, usually symbolized by \n  \n    \n      \n        \u03d5\n        =\n        1\n        \n          /\n        \n        \u03bc\n      \n    \n    {\\displaystyle \\phi =1/\\mu }\n  \n or \n  \n    \n      \n        F\n        =\n        1\n        \n          /\n        \n        \u03bc\n      \n    \n    {\\displaystyle F=1/\\mu }\n  \n, depending on the convention used, measured in reciprocal poise (P\u22121, or cm\u00b7s\u00b7g\u22121), sometimes called the rhe. Fluidity is seldom used in engineering practice.\nAt one time the petroleum industry relied on measuring kinematic viscosity by means of the Saybolt viscometer, and expressing kinematic viscosity in units of Saybolt universal seconds (SUS). Other abbreviations such as SSU (Saybolt seconds universal) or SUV (Saybolt universal viscosity) are sometimes used. Kinematic viscosity in centistokes can be converted from SUS according to the arithmetic and the reference table provided in ASTM D 2161.\n\n\n== Molecular origins ==\nMomentum transport in gases is mediated by discrete molecular collisions, and in liquids by attractive forces that bind molecules close together. Because of this, the dynamic viscosities of liquids are typically much larger than those of gases. In addition, viscosity tends to increase with temperature in gases and decrease with temperature in liquids.\nAbove the liquid-gas critical point, the liquid and gas phases are replaced by a single supercritical phase. In this regime, the mechanisms of momentum transport interpolate between liquid-like and gas-like behavior.\nFor example, along a supercritical isobar (constant-pressure surface), the kinematic viscosity decreases at low temperature and increases at high temperature, with a minimum in between. A rough estimate for the value\nat the minimum is\n\n  \n    \n      \n        \n          \u03bd\n          \n            min\n          \n        \n        =\n        \n          \n            1\n            \n              4\n              \u03c0\n            \n          \n        \n        \n          \n            \u210f\n            \n              \n                m\n                \n                  e\n                \n              \n              m\n            \n          \n        \n      \n    \n    {\\displaystyle \\nu _{\\text{min}}={\\frac {1}{4\\pi }}{\\frac {\\hbar }{\\sqrt {m_{\\text{e}}m}}}}\n  \n\nwhere \n  \n    \n      \n        \u210f\n      \n    \n    {\\displaystyle \\hbar }\n  \n is the Planck constant, \n  \n    \n      \n        \n          m\n          \n            e\n          \n        \n      \n    \n    {\\displaystyle m_{\\text{e}}}\n  \n is the electron mass, and \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  \n is the molecular mass.\nIn general, however, the viscosity of a system depends in detail on how the molecules constituting the system interact, and there are no simple but correct formulas for it. The simplest exact expressions are the Green\u2013Kubo relations for the linear shear viscosity or the transient time correlation function expressions derived by Evans and Morriss in 1988. Although these expressions are each exact, calculating the viscosity of a dense fluid using these relations currently requires the use of molecular dynamics computer simulations. Somewhat more progress can be made for a dilute gas, as elementary assumptions about how gas molecules move and interact lead to a basic understanding of the molecular origins of viscosity. More sophisticated treatments can be constructed by systematically coarse-graining the equations of motion of the gas molecules. An example of such a treatment is Chapman\u2013Enskog theory, which derives expressions for the viscosity of a dilute gas from the Boltzmann equation.\n\n\n=== Pure gases ===\n\nViscosity in gases arises principally from the molecular diffusion that transports momentum between layers of flow. An elementary calculation for a dilute gas at temperature \n  \n    \n      \n        T\n      \n    \n    {\\displaystyle T}\n  \n and density \n  \n    \n      \n        \u03c1\n      \n    \n    {\\displaystyle \\rho }\n  \n gives\n\n  \n    \n      \n        \u03bc\n        =\n        \u03b1\n        \u03c1\n        \u03bb\n        \n          \n            \n              \n                2\n                \n                  k\n                  \n                    B\n                  \n                \n                T\n              \n              \n                \u03c0\n                m\n              \n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle \\mu =\\alpha \\rho \\lambda {\\sqrt {\\frac {2k_{\\text{B}}T}{\\pi m}}},}\n  \n\nwhere \n  \n    \n      \n        \n          k\n          \n            B\n          \n        \n      \n    \n    {\\displaystyle k_{\\text{B}}}\n  \n is the Boltzmann constant, \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  \n the molecular mass, and \n  \n    \n      \n        \u03b1\n      \n    \n    {\\displaystyle \\alpha }\n  \n a numerical constant on the order of \n  \n    \n      \n        1\n      \n    \n    {\\displaystyle 1}\n  \n. The quantity \n  \n    \n      \n        \u03bb\n      \n    \n    {\\displaystyle \\lambda }\n  \n, the mean free path, measures the average distance a molecule travels between collisions. Even without a priori knowledge of \n  \n    \n      \n        \u03b1\n      \n    \n    {\\displaystyle \\alpha }\n  \n, this expression has nontrivial implications. In particular, since \n  \n    \n      \n        \u03bb\n      \n    \n    {\\displaystyle \\lambda }\n  \n is typically inversely proportional to density and increases with temperature, \n  \n    \n      \n        \u03bc\n      \n    \n    {\\displaystyle \\mu }\n  \n itself should increase with temperature and be independent of density at fixed temperature. In fact, both of these predictions persist in more sophisticated treatments, and accurately describe experimental observations. By contrast, liquid viscosity typically decreases with temperature.\nFor rigid elastic spheres of diameter \n  \n    \n      \n        \u03c3\n      \n    \n    {\\displaystyle \\sigma }\n  \n, \n  \n    \n      \n        \u03bb\n      \n    \n    {\\displaystyle \\lambda }\n  \n can be computed, giving\n\n  \n    \n      \n        \u03bc\n        =\n        \n          \n            \u03b1\n            \n              \u03c0\n              \n                3\n                \n                  /\n                \n                2\n              \n            \n          \n        \n        \n          \n            \n              \n                k\n                \n                  B\n                \n              \n              m\n              T\n            \n            \n              \u03c3\n              \n                2\n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\mu ={\\frac {\\alpha }{\\pi ^{3/2}}}{\\frac {\\sqrt {k_{\\text{B}}mT}}{\\sigma ^{2}}}.}\n  \n\nIn this case \n  \n    \n      \n        \u03bb\n      \n    \n    {\\displaystyle \\lambda }\n  \n is independent of temperature, so \n  \n    \n      \n        \u03bc\n        \u221d\n        \n          T\n          \n            1\n            \n              /\n            \n            2\n          \n        \n      \n    \n    {\\displaystyle \\mu \\propto T^{1/2}}\n  \n. For more complicated molecular models, however, \n  \n    \n      \n        \u03bb\n      \n    \n    {\\displaystyle \\lambda }\n  \n depends on temperature in a non-trivial way, and simple kinetic arguments as used here are inadequate. More fundamentally, the notion of a mean free path becomes imprecise for particles that interact over a finite range, which limits the usefulness of the concept for describing real-world gases.\n\n\n==== Chapman\u2013Enskog theory ====\n\nA technique developed by Sydney Chapman and David Enskog in the early 1900s allows a more refined calculation of \n  \n    \n      \n        \u03bc\n      \n    \n    {\\displaystyle \\mu }\n  \n. It is based on the Boltzmann equation, which provides a statistical description of a dilute gas in terms of intermolecular interactions. The technique allows accurate calculation of \n  \n    \n      \n        \u03bc\n      \n    \n    {\\displaystyle \\mu }\n  \n for molecular models that are more realistic than rigid elastic spheres, such as those incorporating intermolecular attractions. Doing so is necessary to reproduce the correct temperature dependence of \n  \n    \n      \n        \u03bc\n      \n    \n    {\\displaystyle \\mu }\n  \n, which experiments show increases more rapidly than the \n  \n    \n      \n        \n          T\n          \n            1\n            \n              /\n            \n            2\n          \n        \n      \n    \n    {\\displaystyle T^{1/2}}\n  \n trend predicted for rigid elastic spheres. Indeed, the Chapman\u2013Enskog analysis shows that the predicted temperature dependence can be tuned by varying the parameters in various molecular models. A simple example is the Sutherland model, which describes rigid elastic spheres with weak mutual attraction. In such a case, the attractive force can be treated perturbatively, which leads to a simple expression for \n  \n    \n      \n        \u03bc\n      \n    \n    {\\displaystyle \\mu }\n  \n:\n\n  \n    \n      \n        \u03bc\n        =\n        \n          \n            5\n            \n              16\n              \n                \u03c3\n                \n                  2\n                \n              \n            \n          \n        \n        \n          \n            (\n            \n              \n                \n                  \n                    k\n                    \n                      B\n                    \n                  \n                  m\n                  T\n                \n                \u03c0\n              \n            \n            )\n          \n          \n            \n            \n            1\n            \n              /\n            \n            2\n          \n        \n         \n        \n          \n            (\n            \n              1\n              +\n              \n                \n                  S\n                  T\n                \n              \n            \n            )\n          \n          \n            \n            \n            \u2212\n            1\n          \n        \n        ,\n      \n    \n    {\\displaystyle \\mu ={\\frac {5}{16\\sigma ^{2}}}\\left({\\frac {k_{\\text{B}}mT}{\\pi }}\\right)^{\\!\\!1/2}\\ \\left(1+{\\frac {S}{T}}\\right)^{\\!\\!-1},}\n  \n\nwhere \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n is independent of temperature, being determined only by the parameters of the intermolecular attraction. To connect with experiment, it is convenient to rewrite as\n\n  \n    \n      \n        \u03bc\n        =\n        \n          \u03bc\n          \n            0\n          \n        \n        \n          \n            (\n            \n              \n                T\n                \n                  T\n                  \n                    0\n                  \n                \n              \n            \n            )\n          \n          \n            \n            \n            3\n            \n              /\n            \n            2\n          \n        \n         \n        \n          \n            \n              \n                T\n                \n                  0\n                \n              \n              +\n              S\n            \n            \n              T\n              +\n              S\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle \\mu =\\mu _{0}\\left({\\frac {T}{T_{0}}}\\right)^{\\!\\!3/2}\\ {\\frac {T_{0}+S}{T+S}},}\n  \n\nwhere \n  \n    \n      \n        \n          \u03bc\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\mu _{0}}\n  \n is the viscosity at temperature \n  \n    \n      \n        \n          T\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle T_{0}}\n  \n. This expression is usually named Sutherland's formula. If \n  \n    \n      \n        \u03bc\n      \n    \n    {\\displaystyle \\mu }\n  \n is known from experiments at \n  \n    \n      \n        T\n        =\n        \n          T\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle T=T_{0}}\n  \n and at least one other temperature, then \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n can be calculated. Expressions for \n  \n    \n      \n        \u03bc\n      \n    \n    {\\displaystyle \\mu }\n  \n obtained in this way are qualitatively accurate for a number of simple gases. Slightly more sophisticated models, such as the Lennard-Jones potential, or the more flexible Mie potential, may provide better agreement with experiments, but only at the cost of a more opaque dependence on temperature. A further advantage of these more complex interaction potentials is that they can be used to develop accurate models for a wide variety of properties using the same potential parameters. In situations where little experimental data is available, this makes it possible to obtain model parameters from fitting to properties such as pure-fluid vapour-liquid equilibria, before using the parameters thus obtained to predict the viscosities of interest with reasonable accuracy.\nIn some systems, the assumption of spherical symmetry must be abandoned, as is the case for vapors with highly polar molecules like H2O. In these cases, the Chapman\u2013Enskog analysis is significantly more complicated.\n\n\n==== Bulk viscosity ====\n\nIn the kinetic-molecular picture, a non-zero bulk viscosity arises in gases whenever there are non-negligible relaxational timescales governing the exchange of energy between the translational energy of molecules and their internal energy, e.g. rotational and vibrational. As such, the bulk viscosity is \n  \n    \n      \n        0\n      \n    \n    {\\displaystyle 0}\n  \n for a monatomic ideal gas, in which the internal energy of molecules is negligible, but is nonzero for a gas like carbon dioxide, whose molecules possess both rotational and vibrational energy.\n\n\n=== Pure liquids ===\n\nIn contrast with gases, there is no simple yet accurate picture for the molecular origins of viscosity in liquids.\nAt the simplest level of description, the relative motion of adjacent layers in a liquid is opposed primarily by attractive molecular forces\nacting across the layer boundary. In this picture, one (correctly) expects viscosity to decrease with increasing temperature. This is because\nincreasing temperature increases the random thermal motion of the molecules, which makes it easier for them to overcome their attractive interactions.\nBuilding on this visualization, a simple theory can be constructed in analogy with the discrete structure of a solid: groups of molecules in a liquid \nare visualized as forming \"cages\" which surround and enclose single molecules. These cages can be occupied or unoccupied, and\nstronger molecular attraction corresponds to stronger cages.\nDue to random thermal motion, a molecule \"hops\" between cages at a rate which varies inversely with the strength of molecular attractions. In equilibrium these \"hops\" are not biased in any direction.\nOn the other hand, in order for two adjacent layers to move relative to each other, the \"hops\" must be biased in the direction\nof the relative motion. The force required to sustain this directed motion can be estimated for a given shear rate, leading to\n\nwhere \n  \n    \n      \n        \n          N\n          \n            A\n          \n        \n      \n    \n    {\\displaystyle N_{\\text{A}}}\n  \n is the Avogadro constant, \n  \n    \n      \n        h\n      \n    \n    {\\displaystyle h}\n  \n is the Planck constant, \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n  \n is the volume of a mole of liquid, and \n  \n    \n      \n        \n          T\n          \n            b\n          \n        \n      \n    \n    {\\displaystyle T_{\\text{b}}}\n  \n is the normal boiling point. This result has the same form as the well-known empirical relation\n\nwhere \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n and \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n are constants fit from data. On the other hand, several authors express caution with respect to this model.\nErrors as large as 30% can be encountered using equation (1), compared with fitting equation (2) to experimental data. More fundamentally, the physical assumptions underlying equation (1) have been criticized. It has also been argued that the exponential dependence in equation (1) does not necessarily describe experimental observations more accurately than simpler, non-exponential expressions.\nIn light of these shortcomings, the development of a less ad hoc model is a matter of practical interest. Foregoing simplicity in favor of precision, it is possible to write rigorous expressions for viscosity starting from the fundamental equations of motion for molecules. A classic example of this approach is Irving\u2013Kirkwood theory. On the other hand, such expressions are given as averages over multiparticle correlation functions and are therefore difficult to apply in practice.\nIn general, empirically derived expressions (based on existing viscosity measurements) appear to be the only consistently reliable means of calculating viscosity in liquids.\nLocal atomic structure changes observed in undercooled liquids on cooling below the equilibrium melting temperature either in terms of radial distribution function g(r) or structure factor S(Q) are found to be directly responsible for the liquid fragility: deviation of the temperature dependence of viscosity of the undercooled liquid from the Arrhenius equation (2) through modification of the activation energy for viscous flow. At the same time equilibrium liquids follow the Arrhenius equation.\n\n\n=== Mixtures and blends ===\n\n\n==== Gaseous mixtures ====\nThe same molecular-kinetic picture of a single component gas can also be applied to a gaseous mixture. For instance, in the Chapman\u2013Enskog approach the viscosity \n  \n    \n      \n        \n          \u03bc\n          \n            mix\n          \n        \n      \n    \n    {\\displaystyle \\mu _{\\text{mix}}}\n  \n of a binary mixture of gases can be written in terms of the individual component viscosities \n  \n    \n      \n        \n          \u03bc\n          \n            1\n            ,\n            2\n          \n        \n      \n    \n    {\\displaystyle \\mu _{1,2}}\n  \n, their respective volume fractions, and the intermolecular interactions. \nAs for the single-component gas, the dependence of \n  \n    \n      \n        \n          \u03bc\n          \n            mix\n          \n        \n      \n    \n    {\\displaystyle \\mu _{\\text{mix}}}\n  \n on the parameters of the intermolecular interactions enters through various collisional integrals which may not be expressible in closed form. To obtain usable expressions for \n  \n    \n      \n        \n          \u03bc\n          \n            mix\n          \n        \n      \n    \n    {\\displaystyle \\mu _{\\text{mix}}}\n  \n which reasonably match experimental data, the collisional integrals may be computed numerically or from correlations. In some cases, the collision integrals are regarded as fitting parameters, and are fitted directly to experimental data. This is a common approach in the development of reference equations for gas-phase viscosities. An example of such a procedure is the Sutherland approach for the single-component gas, discussed above.\nFor gas mixtures consisting of simple molecules, Revised Enskog Theory has been shown to accurately represent both the density- and temperature dependence of the viscosity over a wide range of conditions.\n\n\n==== Blends of liquids ====\nAs for pure liquids, the viscosity of a blend of liquids is difficult to predict from molecular principles. One method is to extend the molecular \"cage\" theory presented above for a pure liquid. This can be done with varying levels of sophistication. One expression resulting from such an analysis is the Lederer\u2013Roegiers equation for a binary mixture:\n\n  \n    \n      \n        ln\n        \u2061\n        \n          \u03bc\n          \n            blend\n          \n        \n        =\n        \n          \n            \n              x\n              \n                1\n              \n            \n            \n              \n                x\n                \n                  1\n                \n              \n              +\n              \u03b1\n              \n                x\n                \n                  2\n                \n              \n            \n          \n        \n        ln\n        \u2061\n        \n          \u03bc\n          \n            1\n          \n        \n        +\n        \n          \n            \n              \u03b1\n              \n                x\n                \n                  2\n                \n              \n            \n            \n              \n                x\n                \n                  1\n                \n              \n              +\n              \u03b1\n              \n                x\n                \n                  2\n                \n              \n            \n          \n        \n        ln\n        \u2061\n        \n          \u03bc\n          \n            2\n          \n        \n        ,\n      \n    \n    {\\displaystyle \\ln \\mu _{\\text{blend}}={\\frac {x_{1}}{x_{1}+\\alpha x_{2}}}\\ln \\mu _{1}+{\\frac {\\alpha x_{2}}{x_{1}+\\alpha x_{2}}}\\ln \\mu _{2},}\n  \n\nwhere \n  \n    \n      \n        \u03b1\n      \n    \n    {\\displaystyle \\alpha }\n  \n is an empirical parameter, and \n  \n    \n      \n        \n          x\n          \n            1\n            ,\n            2\n          \n        \n      \n    \n    {\\displaystyle x_{1,2}}\n  \n and \n  \n    \n      \n        \n          \u03bc\n          \n            1\n            ,\n            2\n          \n        \n      \n    \n    {\\displaystyle \\mu _{1,2}}\n  \n are the respective mole fractions and viscosities of the component liquids.\nSince blending is an important process in the lubricating and oil industries, a variety of empirical and proprietary equations exist for predicting the viscosity of a blend.\n\n\n=== Solutions and suspensions ===\n\n\n==== Aqueous solutions ====\n\nDepending on the solute and range of concentration, an aqueous electrolyte solution can have either a larger or smaller viscosity compared with pure water at the same temperature and pressure. For instance, a 20% saline (sodium chloride) solution has viscosity over 1.5 times that of pure water, whereas a 20% potassium iodide solution has viscosity about 0.91 times that of pure water.\nAn idealized model of dilute electrolytic solutions leads to the following prediction for the viscosity \n  \n    \n      \n        \n          \u03bc\n          \n            s\n          \n        \n      \n    \n    {\\displaystyle \\mu _{s}}\n  \n of a solution:\n\n  \n    \n      \n        \n          \n            \n              \u03bc\n              \n                s\n              \n            \n            \n              \u03bc\n              \n                0\n              \n            \n          \n        \n        =\n        1\n        +\n        A\n        \n          \n            c\n          \n        \n        ,\n      \n    \n    {\\displaystyle {\\frac {\\mu _{s}}{\\mu _{0}}}=1+A{\\sqrt {c}},}\n  \n\nwhere \n  \n    \n      \n        \n          \u03bc\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\mu _{0}}\n  \n is the viscosity of the solvent, \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  \n is the concentration, and \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n is a positive constant which depends on both solvent and solute properties. However, this expression is only valid for very dilute solutions, having \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  \n less than 0.1 mol/L. For higher concentrations, additional terms are necessary which account for higher-order molecular correlations:\n\n  \n    \n      \n        \n          \n            \n              \u03bc\n              \n                s\n              \n            \n            \n              \u03bc\n              \n                0\n              \n            \n          \n        \n        =\n        1\n        +\n        A\n        \n          \n            c\n          \n        \n        +\n        B\n        c\n        +\n        C\n        \n          c\n          \n            2\n          \n        \n        ,\n      \n    \n    {\\displaystyle {\\frac {\\mu _{s}}{\\mu _{0}}}=1+A{\\sqrt {c}}+Bc+Cc^{2},}\n  \n\nwhere \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n and \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n are fit from data. In particular, a negative value of \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n is able to account for the decrease in viscosity observed in some solutions. Estimated values of these constants are shown below for sodium chloride and potassium iodide at temperature 25 \u00b0C (mol = mole, L = liter).\n\n\n==== Suspensions ====\nIn a suspension of solid particles (e.g. micron-size spheres suspended in oil), an effective viscosity \n  \n    \n      \n        \n          \u03bc\n          \n            eff\n          \n        \n      \n    \n    {\\displaystyle \\mu _{\\text{eff}}}\n  \n can be defined in terms of stress and strain components which are averaged over a volume large compared with the distance between the suspended particles, but small with respect to macroscopic dimensions. Such suspensions generally exhibit non-Newtonian behavior. However, for dilute systems in steady flows, the behavior is Newtonian and expressions for \n  \n    \n      \n        \n          \u03bc\n          \n            eff\n          \n        \n      \n    \n    {\\displaystyle \\mu _{\\text{eff}}}\n  \n can be derived directly from the particle dynamics. In a very dilute system, with volume fraction \n  \n    \n      \n        \u03d5\n        \u2272\n        0.02\n      \n    \n    {\\displaystyle \\phi \\lesssim 0.02}\n  \n, interactions between the suspended particles can be ignored. In such a case one can explicitly calculate the flow field around each particle independently, and combine the results to obtain \n  \n    \n      \n        \n          \u03bc\n          \n            eff\n          \n        \n      \n    \n    {\\displaystyle \\mu _{\\text{eff}}}\n  \n. For spheres, this results in the Einstein's effective viscosity formula:\n\n  \n    \n      \n        \n          \u03bc\n          \n            eff\n          \n        \n        =\n        \n          \u03bc\n          \n            0\n          \n        \n        \n          (\n          \n            1\n            +\n            \n              \n                5\n                2\n              \n            \n            \u03d5\n          \n          )\n        \n        ,\n      \n    \n    {\\displaystyle \\mu _{\\text{eff}}=\\mu _{0}\\left(1+{\\frac {5}{2}}\\phi \\right),}\n  \n\nwhere \n  \n    \n      \n        \n          \u03bc\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\mu _{0}}\n  \n is the viscosity of the suspending liquid. The linear dependence on \n  \n    \n      \n        \u03d5\n      \n    \n    {\\displaystyle \\phi }\n  \n is a consequence of neglecting interparticle interactions. For dilute systems in general, one expects \n  \n    \n      \n        \n          \u03bc\n          \n            eff\n          \n        \n      \n    \n    {\\displaystyle \\mu _{\\text{eff}}}\n  \n to take the form\n\n  \n    \n      \n        \n          \u03bc\n          \n            eff\n          \n        \n        =\n        \n          \u03bc\n          \n            0\n          \n        \n        \n          (\n          \n            1\n            +\n            B\n            \u03d5\n          \n          )\n        \n        ,\n      \n    \n    {\\displaystyle \\mu _{\\text{eff}}=\\mu _{0}\\left(1+B\\phi \\right),}\n  \n\nwhere the coefficient \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n may depend on the particle shape (e.g. spheres, rods, disks). Experimental determination of the precise value of \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n is difficult, however: even the prediction \n  \n    \n      \n        B\n        =\n        5\n        \n          /\n        \n        2\n      \n    \n    {\\displaystyle B=5/2}\n  \n for spheres has not been conclusively validated, with various experiments finding values in the range \n  \n    \n      \n        1.5\n        \u2272\n        B\n        \u2272\n        5\n      \n    \n    {\\displaystyle 1.5\\lesssim B\\lesssim 5}\n  \n. This deficiency has been attributed to difficulty in controlling experimental conditions.\nIn denser suspensions, \n  \n    \n      \n        \n          \u03bc\n          \n            eff\n          \n        \n      \n    \n    {\\displaystyle \\mu _{\\text{eff}}}\n  \n acquires a nonlinear dependence on \n  \n    \n      \n        \u03d5\n      \n    \n    {\\displaystyle \\phi }\n  \n, which indicates the importance of interparticle interactions. Various analytical and semi-empirical schemes exist for capturing this regime. At the most basic level, a term quadratic in \n  \n    \n      \n        \u03d5\n      \n    \n    {\\displaystyle \\phi }\n  \n is added to \n  \n    \n      \n        \n          \u03bc\n          \n            eff\n          \n        \n      \n    \n    {\\displaystyle \\mu _{\\text{eff}}}\n  \n:\n\n  \n    \n      \n        \n          \u03bc\n          \n            eff\n          \n        \n        =\n        \n          \u03bc\n          \n            0\n          \n        \n        \n          (\n          \n            1\n            +\n            B\n            \u03d5\n            +\n            \n              B\n              \n                1\n              \n            \n            \n              \u03d5\n              \n                2\n              \n            \n          \n          )\n        \n        ,\n      \n    \n    {\\displaystyle \\mu _{\\text{eff}}=\\mu _{0}\\left(1+B\\phi +B_{1}\\phi ^{2}\\right),}\n  \n\nand the coefficient \n  \n    \n      \n        \n          B\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle B_{1}}\n  \n is fit from experimental data or approximated from the microscopic theory. However, some authors advise caution in applying such simple formulas since non-Newtonian behavior appears in dense suspensions (\n  \n    \n      \n        \u03d5\n        \u2273\n        0.25\n      \n    \n    {\\displaystyle \\phi \\gtrsim 0.25}\n  \n for spheres), or in suspensions of elongated or flexible particles.\nThere is a distinction between a suspension of solid particles, described above, and an emulsion. The latter is a suspension of tiny droplets, which themselves may exhibit internal circulation. The presence of internal circulation can decrease the observed effective viscosity, and different theoretical or semi-empirical models must be used.\n\n\n=== Amorphous materials ===\n\nIn the high and low temperature limits, viscous flow in amorphous materials (e.g. in glasses and melts) has the Arrhenius form:\n\n  \n    \n      \n        \u03bc\n        =\n        A\n        \n          e\n          \n            Q\n            \n              /\n            \n            (\n            R\n            T\n            )\n          \n        \n        ,\n      \n    \n    {\\displaystyle \\mu =Ae^{Q/(RT)},}\n  \n\nwhere Q is a relevant activation energy, given in terms of molecular parameters; T is temperature; R is the molar gas constant; and A is approximately a constant. The activation energy Q takes a different value depending on whether the high or low temperature limit is being considered: it changes from a high value QH at low temperatures (in the glassy state) to a low value QL at high temperatures (in the liquid state).\n\nFor intermediate temperatures, \n  \n    \n      \n        Q\n      \n    \n    {\\displaystyle Q}\n  \n varies nontrivially with temperature and the simple Arrhenius form fails. On the other hand, the two-exponential equation\n\n  \n    \n      \n        \u03bc\n        =\n        A\n        T\n        exp\n        \u2061\n        \n          (\n          \n            \n              B\n              \n                R\n                T\n              \n            \n          \n          )\n        \n        \n          [\n          \n            1\n            +\n            C\n            exp\n            \u2061\n            \n              (\n              \n                \n                  D\n                  \n                    R\n                    T\n                  \n                \n              \n              )\n            \n          \n          ]\n        \n        ,\n      \n    \n    {\\displaystyle \\mu =AT\\exp \\left({\\frac {B}{RT}}\\right)\\left[1+C\\exp \\left({\\frac {D}{RT}}\\right)\\right],}\n  \n\nwhere \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n, \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n, \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n, \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  \n are all constants, provides a good fit to experimental data over the entire range of temperatures, while at the same time reducing to the correct Arrhenius form in the low and high temperature limits. This expression, also known as Duouglas-Doremus-Ojovan model , can be motivated from various theoretical models of amorphous materials at the atomic level.\nA two-exponential equation for the viscosity can be derived within the Dyre shoving model of supercooled liquids, where the Arrhenius energy barrier is identified with the high-frequency shear modulus times a characteristic shoving volume. Upon specifying the temperature dependence of the shear modulus via thermal expansion and via the repulsive part of the intermolecular potential, another two-exponential equation is retrieved:\n\n  \n    \n      \n        \u03bc\n        =\n        exp\n        \u2061\n        \n          \n            {\n            \n              \n                \n                  \n                    \n                      V\n                      \n                        c\n                      \n                    \n                    \n                      C\n                      \n                        G\n                      \n                    \n                  \n                  \n                    \n                      k\n                      \n                        B\n                      \n                    \n                    T\n                  \n                \n              \n              exp\n              \u2061\n              \n                \n                  [\n                  \n                    (\n                    2\n                    +\n                    \u03bb\n                    )\n                    \n                      \u03b1\n                      \n                        T\n                      \n                    \n                    \n                      T\n                      \n                        g\n                      \n                    \n                    \n                      (\n                      \n                        1\n                        \u2212\n                        \n                          \n                            T\n                            \n                              T\n                              \n                                g\n                              \n                            \n                          \n                        \n                      \n                      )\n                    \n                  \n                  ]\n                \n              \n            \n            }\n          \n        \n      \n    \n    {\\displaystyle \\mu =\\exp {\\left\\{{\\frac {V_{c}C_{G}}{k_{B}T}}\\exp {\\left[(2+\\lambda )\\alpha _{T}T_{g}\\left(1-{\\frac {T}{T_{g}}}\\right)\\right]}\\right\\}}}\n  \n\nwhere \n  \n    \n      \n        \n          C\n          \n            G\n          \n        \n      \n    \n    {\\displaystyle C_{G}}\n  \n denotes the high-frequency shear modulus of the material evaluated at a temperature equal to the glass transition temperature \n  \n    \n      \n        \n          T\n          \n            g\n          \n        \n      \n    \n    {\\displaystyle T_{g}}\n  \n, \n  \n    \n      \n        \n          V\n          \n            c\n          \n        \n      \n    \n    {\\displaystyle V_{c}}\n  \n is the so-called shoving volume, i.e. it is the characteristic volume of the group of atoms involved in the shoving event by which an atom/molecule escapes from the cage of nearest-neighbours, typically on the order of the volume occupied by few atoms. Furthermore, \n  \n    \n      \n        \n          \u03b1\n          \n            T\n          \n        \n      \n    \n    {\\displaystyle \\alpha _{T}}\n  \n is the thermal expansion coefficient of the material, \n  \n    \n      \n        \u03bb\n      \n    \n    {\\displaystyle \\lambda }\n  \n is a parameter which measures the steepness of the power-law rise of the ascending flank of the first peak of the radial distribution function, and is quantitatively related to the repulsive part of the interatomic potential. Finally, \n  \n    \n      \n        \n          k\n          \n            B\n          \n        \n      \n    \n    {\\displaystyle k_{B}}\n  \n denotes the Boltzmann constant.\n\n\n=== Eddy viscosity ===\nIn the study of turbulence in fluids, a common practical strategy is to ignore the small-scale vortices (or eddies) in the motion and to calculate a large-scale motion with an effective viscosity, called the \"eddy viscosity\", which characterizes the transport and dissipation of energy in the smaller-scale flow (see large eddy simulation). In contrast to the viscosity of the fluid itself, which must be positive by the second law of thermodynamics, the eddy viscosity can be negative.\n\n\n== Prediction ==\nBecause viscosity depends continuously on temperature and pressure, it cannot be fully characterized by a finite number of experimental measurements. Predictive formulas become necessary if experimental values are not available at the temperatures and pressures of interest. This capability is important for thermophysical simulations, \nin which the temperature and pressure of a fluid can vary continuously with space and time. A similar situation is encountered for mixtures of pure fluids, where the viscosity depends continuously on the concentration ratios of the constituent fluids\nFor the simplest fluids, such as dilute monatomic gases and their mixtures, ab initio quantum mechanical computations can accurately predict viscosity in terms of fundamental atomic constants, i.e., without reference to existing viscosity measurements. For the special case of dilute helium, uncertainties in the ab initio calculated viscosity are two order of magnitudes smaller than uncertainties in experimental values.\nFor slightly more complex fluids and mixtures at moderate densities (i.e. sub-critical densities) Revised Enskog Theory can be used to predict viscosities with some accuracy. Revised Enskog Theory is predictive in the sense that predictions for viscosity can be obtained using parameters fitted to other, pure-fluid thermodynamic properties or transport properties, thus requiring no a priori experimental viscosity measurements.\nFor most fluids, high-accuracy, first-principles computations are not feasible. Rather, theoretical or empirical expressions must be fit to existing viscosity measurements. If such an expression is fit to high-fidelity data over a large range of temperatures and pressures, then it is called a \"reference correlation\" for that fluid. Reference correlations have been published for many pure fluids; a few examples are water, carbon dioxide, ammonia, benzene, and xenon. Many of these cover temperature and pressure ranges that encompass gas, liquid, and supercritical phases.\nThermophysical modeling software often relies on reference correlations for predicting viscosity at user-specified temperature and pressure.\nThese correlations may be proprietary. Examples are REFPROP (proprietary) and CoolProp\n(open-source).\nViscosity can also be computed using formulas that express it in terms of the statistics of individual particle\ntrajectories. These formulas include the Green\u2013Kubo relations for the linear shear viscosity and the transient time correlation function expressions derived by Evans and Morriss in 1988. \nThe advantage of these expressions is that they are formally exact and valid for general systems. The disadvantage is that they require detailed knowledge of particle trajectories, available only in computationally expensive simulations such as molecular dynamics. \nAn accurate model for interparticle interactions is also required, which may be difficult to obtain for complex molecules.\n\n\n== Selected substances ==\n\nObserved values of viscosity vary over several orders of magnitude, even for common substances (see the order of magnitude table below). For instance, a 70% sucrose (sugar) solution has a viscosity over 400 times that of water, and 26,000 times that of air. More dramatically, pitch has been estimated to have a viscosity 230 billion times that of water.\n\n\n=== Water ===\nThe dynamic viscosity \n  \n    \n      \n        \u03bc\n      \n    \n    {\\displaystyle \\mu }\n  \n of water is about 0.89 mPa\u00b7s at room temperature (25 \u00b0C). As a function of temperature in kelvins, the viscosity can be estimated using the semi-empirical Vogel-Fulcher-Tammann equation:\n\n  \n    \n      \n        \u03bc\n        =\n        A\n        exp\n        \u2061\n        \n          (\n          \n            \n              B\n              \n                T\n                \u2212\n                C\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\mu =A\\exp \\left({\\frac {B}{T-C}}\\right)}\n  \n\nwhere A = 0.02939 mPa\u00b7s, B = 507.88 K, and C = 149.3 K. Experimentally determined values of the viscosity are also given in the table below. The values at 20 \u00b0C are a useful reference: there, the dynamic viscosity is about 1 cP and the kinematic viscosity is about 1 cSt.\n\n\n=== Air ===\nUnder standard atmospheric conditions (25 \u00b0C and pressure of 1 bar), the dynamic viscosity of air is 18.5 \u03bcPa\u00b7s, roughly 50 times smaller than the viscosity of water at the same temperature. Except at very high pressure, the viscosity of air depends mostly on the temperature. Among the many possible approximate formulas for the temperature dependence (see Temperature dependence of viscosity), one is:\n\n  \n    \n      \n        \n          \u03b7\n          \n            air\n          \n        \n        =\n        2.791\n        \u00d7\n        \n          10\n          \n            \u2212\n            7\n          \n        \n        \u00d7\n        \n          T\n          \n            0.7355\n          \n        \n      \n    \n    {\\displaystyle \\eta _{\\text{air}}=2.791\\times 10^{-7}\\times T^{0.7355}}\n  \n\nwhich is accurate in the range \u221220 \u00b0C to 400 \u00b0C. For this formula to be valid, the temperature must be given in kelvins; \n  \n    \n      \n        \n          \u03b7\n          \n            air\n          \n        \n      \n    \n    {\\displaystyle \\eta _{\\text{air}}}\n  \n then corresponds to the viscosity in Pa\u00b7s.\n\n\n=== Other common substances ===\n\n\n=== Order of magnitude estimates ===\nThe following table illustrates the range of viscosity values observed in common substances. Unless otherwise noted, a temperature of 25 \u00b0C and a pressure of 1 atmosphere are assumed.\nThe values listed are representative estimates only, as they do not account for measurement uncertainties, variability in material definitions, or non-Newtonian behavior.\n\n\n== See also ==\n\n\n== References ==\n\n\n=== Footnotes ===\n\n\n=== Citations ===\n\n\n=== Sources ===\n\n\n== External links ==\n\nViscosity - The Feynman Lectures on Physics\nFluid properties \u2013 high accuracy calculation of viscosity for frequently encountered pure liquids and gases\nFluid Characteristics Chart \u2013 a table of viscosities and vapor pressures for various fluids\nGas Dynamics Toolbox \u2013 calculate coefficient of viscosity for mixtures of gases\nGlass Viscosity Measurement \u2013 viscosity measurement, viscosity units and fixpoints, glass viscosity calculation\nKinematic Viscosity \u2013 conversion between kinematic and dynamic viscosity\nPhysical Characteristics of Water \u2013 a table of water viscosity as a function of temperature\nCalculation of temperature-dependent dynamic viscosities for some common components\nArtificial viscosity\nViscosity of Air, Dynamic and Kinematic, Engineers Edge",
        "unit": "kinematic viscosity",
        "url": "https://en.wikipedia.org/wiki/Viscosity"
    }
]